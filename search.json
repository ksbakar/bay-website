[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Statistical Methods in Medicine & Health",
    "section": "",
    "text": "Preface\nThe application of Bayesian methods in medicine and health sciences has become increasingly vital as we strive to enhance our understanding of improve diagnostic accuracy, and personalise treatment plans. This course is designed to introduce students with a background in medicine and health sciences to the principles and practices of Bayesian statistics, providing a practical framework for applying these methods in their respective fields.",
    "crumbs": [
      "**Preface**"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Bayesian methods offer a unique approach to statistical analysis by incorporating prior knowledge and continuously updating the posterior probability as new evidence becomes available. This dynamic approach is particularly suited to the medical and health sciences, where new data is constantly emerging, and decisions often need to be made in the face of uncertainty.\nThe aim of this course is to explain Bayesian statistics and demonstrate its practical applications in medicine and health sciences. We will start with the foundational concepts, gradually building up to more advanced topics, ensuring a comprehensive understanding that is both accessible and relevant to medical professionals, researchers, and students. Each chapter includes real-world examples, case studies, and practical exercises to reinforce the concepts discussed and illustrate their application in clinical and research settings.\nThroughout the course, we will explore the following key areas:\n\nBayesian Dreams! Navigating Evidence and Inference: Understanding the basics of Bayesian philosophy and how it differs from traditional frequentist approaches. Learning how to update beliefs in light of new data using Bayes’ theorem. Exploring directed acyclic graph (DAG) in the Bayesian modelling context, with graphical representations of the probabilistic relationships between variables and parameters.\nChaotics? Prior Problems, Tools, and Computation: Exploring the role of prior information and how it influences posterior conclusions. Bayesian context of exact inference and computational techniques for approximating complex posterior distributions such as Markov chain Monte Carlo (MCMC). Generative models with prior and posterior predictive checks.\nBayeswatch! Keeping an Eye on Your Gaussian Model: Understanding causation and correlation and how it can be used in Bayesian context by drawing DAG. Nevigating Bayesian hierarchical models with continuous outcome or endpoint variable. Explore the choice for hyper-parameters of prior distributions for Gaussian model. Explore examples in clinical and health research.\nBayeswatch! Keeping an Eye on Your Non-Gaussian Model: Extending Bayesian models beyond normality assumptions, where outcome or endpoint variable is binary or counts (i.e., generalised linear models under Bayesian hierarchy), and explanation with DAG. Explore key tactics on the choice of prior distributions.\nClusterphobia? Let Bayes Handle It!: Understanding and implementing hierarchical models for complex data structures common in health sciences. Learing the use of latent process modelling in Bayesian hierarchy (i.e., similar to mixed models in frequentist settings) with both Gaussian and non-Gaussian (e.g., binomial) distributions.\nWander into the Wonder! Bayesian Secrets to the Right Sample: Bayesian sample size calculations, in particular to aid the sample size selection to design trials. Discussion of adaptations with relevant sample size calculations using Bayesian methods. Bayesian model choice (e.g., Bayes factor, deviance information criterion (DIC), Watanabe-Akaike information criterion (WAIC) and leave-one-out (LOO) cross-validation).\n\nThe field of medicine and health sciences is inherently multidisciplinary, and so too is this course. It is crafted to bridge the gap between statistical theory and medical practice, enabeling healthcare professionals to make more informed, data-driven decisions. Whether you are a student eager to learn about Bayesian statistics, or a biostatistician or clinician or a health professional looking to enhance your research skills, or a researcher aiming to apply Bayesian methods to your work, this course provides the tools and knowledge you need to succeed.\nWe hope that by the end of this journey, you will not only appreciate the power and flexibility of Bayesian methods but also feel confident in applying these techniques to improve patient outcomes and advance medical research.\nWelcome to the world of Bayesian methods in medicine and health sciences. Let’s begin.",
    "crumbs": [
      "**Introduction**"
    ]
  },
  {
    "objectID": "installation_guide.html",
    "href": "installation_guide.html",
    "title": "Software Installation Guide",
    "section": "",
    "text": "Have R and RStudio installed and working\nIt is important that everyone has R and RStudio installed and functional before any assignments are due.",
    "crumbs": [
      "**Software Installation Guide**"
    ]
  },
  {
    "objectID": "installation_guide.html#have-r-and-rstudio-installed-and-working",
    "href": "installation_guide.html#have-r-and-rstudio-installed-and-working",
    "title": "Software Installation Guide",
    "section": "",
    "text": "How to install R?\nThe latest version of R can be downloaded from the Comprehensive R Archive Network (CRAN):\n\nhttps://cran.r-project.org/\n\nTo install R on a PC click on “Download R for Windows”\n\nhttps://cran.r-project.org/bin/windows/base/\n\nand then “install R for the first time”. This will take you to a new window with the latest version of R. Click on “Download R 4.X.X for Windows” and follow the installation instructions. Default settings are OK. Note: At the time of writing, the current version of R was 4.2.2 but you can install whichever is the latest version, even if it differs from the one mentioned in the course notes.\nTo install R on a Mac click on “Download R for (Mac) OS X”\n\nhttps://cran.r-project.org/bin/macosx/\n\nand download the R binary under “Latest release”. The file will have an extension of .pkg, for example R-4.X.X.pkg. Click on this link and follow the instructions. Default settings are OK. Note: you can install whichever is the latest version, even if it differs from the one mentioned in these notes.\nOptional – Check that R has installed correctly (You can skip this step and proceed to installing RStudio. If RStudio is working then you know that base R was successfully installed.) Open R and type 1 + 2 into the command line (the command line is indicated with the “&gt;” symbol). After pressing Enter, you should get the following output:\n[1] 3\n\n\nHow to install RStudio?\nYou don’t need to download RStudio in order to use R, but RStudio provides a more user friendly interface and is commonly used by many R users. We will be using RStudio, so we strongly recommend that you install it too.\nFirst make sure you have downloaded base R and confirmed that it was successfully installed. After you have downloaded base R, you can download RStudio. RStudio can be downloaded from:\n\nhttps://posit.co/download/rstudio-desktop/#download\n\nScroll down and download the appropriate file for your operating system. Follow the prompts to install.\nTo confirm that RStudio has installed correctly, double click on the RStudio icon your computer to open the program\nIf you type 1 + 2 into the R Console and hit enter you should get 3 as the output.\nGood work! You have now successfully installed R and RStudio!",
    "crumbs": [
      "**Software Installation Guide**"
    ]
  },
  {
    "objectID": "installation_guide.html#how-to-install-compiler",
    "href": "installation_guide.html#how-to-install-compiler",
    "title": "Software Installation Guide",
    "section": "How to Install Compiler?",
    "text": "How to Install Compiler?\n\nInstall ‘Rtools’ (C++ Compiler) for Windows OS\nThis is a stand-alone *.exe file that you need to install on your PC or laptop. You do not need to manually link Rtools with R or RStudio, as it will be automatically linked after the *.exe file is installed.\nDownload Rtools from\n\nhttps://cran.r-project.org/bin/windows/Rtools/\n\nbased on the R version that you have already installed. Usually, for the latest version this should be RTools 4.4, i.e.,\n\nhttps://cran.r-project.org/bin/windows/Rtools/rtools44/rtools.html\n\nFor further detail on Rtools see:\n\nhttps://github.com/stan-dev/rstan/wiki/Configuring-C—Toolchain-for-Windows\n\n\n\nOption 1: Install ‘Rtools’ for Mac OS\nInstall ‘macrtools’ R package. This installer package was developed by James Joseph Balamuta, see details:\n\nhttps://mac.thecoatlessprofessor.com/macrtools/\n\nAccording to their webpage: “This package is designed to recreate the compiled code toolchain used to compile the official macOS R binary on CRAN by following the steps described on the r-project developer page for macOS tools (https://mac.r-project.org/tools/). The package is able to to setup the compilation toolchain on any Mac that meets the standards required to install the official CRAN R binary on macOS.”\nYou can install the development version of macrtools from GitHub using R console or using “Tools” menu.\nFirst install remotes package (if not already installed)\n\n\nCode\ninstall.packages(\"remotes\")\n\n\nThen install ‘macrtools’ packages:\n\n\nCode\nremotes::install_github(\"coatless-mac/macrtools\")\n\n\nThen using ‘macrtools’ package install rtools as:\n\n\nCode\nmacrtools::macos_rtools_install()\n\n\nThis will install: (1) Xcode CLI (2) gfortran, and (3) R Development binaries from the Recipes project.\nFor further detail on installation see:\n\n\n\n\nOption 2: Install Compiler for Mac OS\nFirst, if you have the files ~/.R/Makevars and/or ~/.Renviron, save a copy in a different location (like your desktop) and delete them.\nIf you don’t know what these files are or if you have them, go to Finder &gt; Go &gt; Home, which takes you to your home directory, and press SHIFT + CMD + . to show hidden files (i.e. files starting with a full stop .) if you can’t see them already. Now search among the hidden files and if you see a folder named .R/ and/or a file .Renviron, copy them to your desktop and delete the original copies from your home directory.\nTo install the Xcode Command Line Tools, run the following in the Terminal (open Finder &gt; Applications &gt; Terminal, type in the following command and press ENTER):\n\nxcode-select –install\n\nmacOS will download and install the Xcode CLT (it will take a while, make sure you have a stable internet connection).\nIf your Mac has an Intel chip, you need to install gfortran v8.2 (for Mojave) independent of your macOS version. You can download the installer from\n\nhttps://github.com/fxcoudert/gfortran-for-macOS/releases/tag/8.2\n\nIf your Mac has an Apple M1 chip, you will need to install gfortran v11. You can download it from here:\n\nhttps://github.com/fxcoudert/gfortran-for-macOS/releases/tag/11-arm-alpha2\n\ndownload the .pkg package listed under Assets at the bottom of the page.",
    "crumbs": [
      "**Software Installation Guide**"
    ]
  },
  {
    "objectID": "installation_guide.html#install-rstan-and-brms-packages",
    "href": "installation_guide.html#install-rstan-and-brms-packages",
    "title": "Software Installation Guide",
    "section": "Install ‘rstan’ and ‘brms’ packages",
    "text": "Install ‘rstan’ and ‘brms’ packages\n‘rstan’ package\n– After installing Rtools (Windows and/or Mac OS), open RStudio and using R console or using “Tools” menu install rstan package.\n\n\nCode\ninstall.packages(\"rstan\", dependencies = TRUE)\n\n\nFor detail on rstan installation:\n\nhttps://github.com/stan-dev/rstan/wiki/RStan-Getting-Started\n\nNow, verify the installation by running:\n\n\nCode\nexample(stan_model, package = \"rstan\", run.dontrun = TRUE)\n\n\nin your R console.\n‘brms’ package\n– Then install brms package:\n\n\nCode\ninstall.packages(\"brms\")\n\n\nGreat! Now you are ready to use Stan!",
    "crumbs": [
      "**Software Installation Guide**"
    ]
  },
  {
    "objectID": "brief_module_01.html",
    "href": "brief_module_01.html",
    "title": "Module 1: Bayesian Dreams!",
    "section": "",
    "text": "Summary\nIn this module, we will learn the key principles and tools of Bayesian reasoning, a powerful framework for thinking about uncertainty, updating beliefs, and drawing inferences from data.\nWe begin by exploring Bayesian philosophy, which offers a fundamentally different view of probability compared to classical statistics. Rather than interpreting probability as the long-run frequency of events, we’ll learn to view it as a measure of belief, a way to quantify uncertainty based on both prior knowledge and observed data. This shift in perspective lays the foundation for all that follows.\nTo structure our reasoning, we introduce a logical framework, a kind of conceptual maze that guides our decision-making under uncertainty. Bayesian methods help us navigate this maze, using clear, consistent rules that allow us to revise our understanding as we encounter new situations and data.\nWe will then compare the concepts of classical and Bayesian approaches to statistical thinking. While classical methods treat parameters as fixed and unknown, we will learn how Bayesian methods treat parameters as uncertain, described by probability distributions. This distinction allows us to build more flexible and realistic models that better reflect the complexity of real-world problems.\nAt the heart of our learning is Bayes’ Theorem, the mathematical engine of Bayesian analysis. We’ll see how it allows us to update our beliefs in light of new evidence, producing what’s known as the posterior distribution. This process represents learning in action: our initial assumptions, or priors, are revised using the data, leading to updated beliefs.\nWith Bayes’ Theorem in hand, we’ll learn Bayesian inference, where we draw conclusions from data by interpreting full probability distributions instead of relying on single-point estimates. This approach gives us a deeper understanding of uncertainty in our conclusions.\nWe will also explore Bayesian odds as a way to compare competing hypotheses or models. By examining the ratio of their posterior probabilities, we learn how to judge which explanations are more plausible, based on both prior assumptions and current data. As we grow more familiar with the framework, we’ll begin to use Bayesian language, such as terms like priors, posteriors, likelihoods, and credible intervals, to articulate our reasoning.\nTo represent the relationships between variables/parameters, we will learn to use directed acyclic graphs, or DAGs. These graphical tools help us visualise assumptions, identify dependencies, and structure our models clearly.\nFinally, we’ll explain Bayesian updating, the idea that beliefs should evolve as new evidence is observed. This dynamic process mirrors how we learn in everyday life, and it ensures that our models remain responsive and reflective of the world around us.\nBy the end of this module, we will have developed a strong foundation in Bayesian thinking, learning the theory and methods, to apply them in real-world contexts where uncertainty and decision-making go hand in hand.",
    "crumbs": [
      "**Module 1:** Bayesian Dreams!"
    ]
  },
  {
    "objectID": "M01_1.html",
    "href": "M01_1.html",
    "title": "Week 1: Navigating Evidence",
    "section": "",
    "text": "Learnings\n– LO1: Explain the difference between Bayesian and frequentist concepts of statistical inference.\n– LO5: Engage in specifying, checking and interpreting Bayesian statistical analyses in practical problems using effective communication with health and medical investigators.\nIn today’s lecture we will:\n– Understand Bayesian philosophy.\n– Describe the motivation of doing Bayesian analysis.\n– Understand the difference between Bayesian and classical statistical methods.\n– Interpret a real-life problem in Bayesian context.",
    "crumbs": [
      "Week 1: **Navigating Evidence**"
    ]
  },
  {
    "objectID": "M01_1.html#learnings",
    "href": "M01_1.html#learnings",
    "title": "Week 1: Navigating Evidence",
    "section": "",
    "text": "Outcomes\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 1: **Navigating Evidence**"
    ]
  },
  {
    "objectID": "M01_1.html#background",
    "href": "M01_1.html#background",
    "title": "Week 1: Navigating Evidence",
    "section": "Background",
    "text": "Background\n\n\n\n\n\nThomas Bayes (The IMS Bulletin, 1988, vol-17(3), pp.276-278)\n\n\n\n\nThomas Bayes, who is remembered through the term “Bayesian”, passed away in 1761 without having the chance to formally present or publish his groundbreaking findings. Two years after his death, it was Richard Price who, in 1763, introduced Bayes’ extraordinary work to the Royal Statistical Society. Bayes had been developing a probabilistic framework to tackle inverse problems. Later, Richard Price (1723–1791) and Pierre-Simon Laplace (1749–1827) played crucial roles in advancing and applying this transformative idea to practical real-world scenarios.\nTo fully understand the Bayesian framework, it is essential to become familiar with several key terms commonly used in Bayesian discussions, such as Bayesian inference, prior, posterior, and Bayesian modelling. Throughout this course, we will gradually explore and learn these fundamental concepts.\nBefore that, let’s start understanding the Bayesian philosophy.",
    "crumbs": [
      "Week 1: **Navigating Evidence**"
    ]
  },
  {
    "objectID": "M01_1.html#bayesian-philosophy",
    "href": "M01_1.html#bayesian-philosophy",
    "title": "Week 1: Navigating Evidence",
    "section": "Bayesian Philosophy",
    "text": "Bayesian Philosophy\n\n\nBayesian philosophy revolves around the concept of “degrees of belief” in scientific reasoning. But what does that actually mean? Simply put, it views probability as a measure of our confidence in an event, which updates as we gather new evidence. Unlike traditional frequentist statistics, which treat probability as an objective, fixed value, Bayesian thinking sees it as a dynamic, subjective measure that evolves with new data and insights.\nLet’s explain a bit more using examples:\n\n\n\n\n\nBayesian World (Philosophy)\n\n\n\n\nSuppose a medical practitioner is treating a patient who might have a particular illness, like type-2 diabetes. Initially, based on the patient’s age, family history, and some initial symptoms, the medical practitioner might believe there’s a 20% chance the patient has diabetes. This belief is a prior probability.\nNow, the medical practitioner orders a blood test to check the patient’s blood sugar levels. When the results come back, they show elevated sugar levels, which increase the likelihood of diabetes. The medical practitioner updates their belief based on this new evidence, which is called the posterior probability.\nSo, Bayesian philosophy evolves starting with an initial belief (prior), and then updating that belief as new data (like test results) comes in. In short, it’s a flexible way of thinking where beliefs are adjusted as information is acquired.",
    "crumbs": [
      "Week 1: **Navigating Evidence**"
    ]
  },
  {
    "objectID": "M01_1.html#the-maze",
    "href": "M01_1.html#the-maze",
    "title": "Week 1: Navigating Evidence",
    "section": "The Maze",
    "text": "The Maze\n\nBayesian analysis is a logical framework that helps update our beliefs based on continuous new information. A helpful analogy is navigating a maze with an incomplete map. Each step provides new clues, and with every clue, your understanding of the maze improves. By continuously updating your knowledge with new information, you eventually find the exit. This is how Bayesian analysis works, it continuously updates our understanding as more evidence comes in.\nTo see this in practice, let’s consider an example explained below.\n\nExample\nA medical practitioner had seen many cases of pneumonia before, but this one was tricky. A 55-year-old patient, had been admitted with high fever, cough, and shortness of breath. Based on his symptoms and an initial chest X-ray, the practitioner diagnosed him with bacterial pneumonia and started him on a standard antibiotic.\nAt this point, their belief was strong that the chosen antibiotic would work, this was their prior probability based on past experience.\nDay 2:\nAfter 24 hours, the patient wasn’t improving. Thier fever remained high, and their breathing was still labored.\nThe practitioner now had new evidence, the treatment wasn’t working as quickly as it should. Applying Bayesian reasoning, they adjusted their belief:\n\nThe probability that this was a typical bacterial pneumonia responding to first-line antibiotics decreased.\nThe probability that it was a resistant strain of bacteria or even a different type of infection increased.\n\nThey needed more information.\nDay 3:\nThe practitioner ordered a sputum culture to check for antibiotic-resistant bacteria. In the meantime, they updated the treatment, switching the patient to a broader-spectrum antibiotic.\n\nIf the new antibiotic worked, it would confirm that the initial one was ineffective, meaning resistant bacteria were likely the cause.\nIf the patient still didn’t improve, it could mean this wasn’t bacterial pneumonia at all, it might be a viral infection instead.\n\nAgain, their belief about the cause of the patient’s illness shifted based on new evidence.\nDay 4:\nThe test results came in: The patient’s infection was caused by a drug-resistant strain of bacteria. This confirmed that the initial choice of antibiotics was ineffective.\nWith this new evidence, the practitioner’s belief was now much stronger that the broader-spectrum antibiotic was the right choice.\nThe Lesson of Bayesian Thinking\nThe medical practitioner didn’t just rely on their initial belief. Instead, they continuously updated their understanding as new evidence emerged, just like someone navigating a maze learns from every wrong turn. This process demonstrates how Bayesian reasoning helps in making data-driven, logical decisions, not just by guessing, but by continuously refining our beliefs with new information, ultimately leading to the best possible decision.",
    "crumbs": [
      "Week 1: **Navigating Evidence**"
    ]
  },
  {
    "objectID": "M01_1.html#concepts-classical-vs.-bayesian",
    "href": "M01_1.html#concepts-classical-vs.-bayesian",
    "title": "Week 1: Navigating Evidence",
    "section": "Concepts: Classical vs. Bayesian",
    "text": "Concepts: Classical vs. Bayesian\n\n\nThroughout this course, we will provide relative comparisons of frequentists and Bayesian statistical methods, using examples. Let us now explain some key conceptual aspects of these two approaches.\nHistory:\n\nThe origins of Bayesian statistical inference trace back to the late 18th century, predating many modern methodologies. Its use continued into the 19th century, but after World War I, statisticians like Sir Ronald Fisher, who opposed Bayesian concepts, contributed to its decline. Fisher’s 1925 statistical handbook briefly mentioned Bayesian analysis, then known as “inverse probability”, further pushing it to the margins of mainstream statistics. However, in the latter half of the 20th century, Bayesian methods gradually regained acceptance. This resurgence was particularly driven by advancements in computational technology during the 1990s, which greatly expanded their practical applications.\nData and Parameter:\n\nData represents known elements, whereas parameters are unknown quantities inferred from data. Within the Bayesian framework, this distinction becomes less clear; a variable may be either observed or unobserved, yet it is governed by the same distribution function. Consequently, an assumption can serve as a “likelihood” or a “prior” depending on the context, without altering the model. This connection between certainty (data) and uncertainty (parameters) facilitates the management of measurement errors and missing data in modeling.\nReliable Inference:\n\nThere exists a notion that a specific number of observations is necessary for reliable statistical estimates. For example, at least 30 observations are typically required for a Gaussian distribution. This notion is rooted in traditional statistical inference, where methods are validated for large sample sizes, a concept known as asymptotic behavior.\nConversely, Bayesian estimates remain valid regardless of sample size. While larger samples are advantageous, Bayesian methods provide meaningful results even with limited data. However, they necessitate a careful selection of the “prior,” which significantly influences the final inference. An improperly chosen prior can skew conclusions. Understanding the world demands thoughtful consideration, without resorting to shortcuts.\nRole of Data:\n\nThe distinctions between Bayesian and non-Bayesian methodologies are notable, yet these differences can sometimes overshadow their underlying commonalities. In many Bayesian and non-Bayesian models, the most critical assumptions typically concern the likelihood functions and their connections with the parameters. These assumptions guide the inferences drawn from each dataset. As the sample size grows, the significance of the likelihood increases. This shared emphasis on likelihood clarifies why Bayesian and non-Bayesian inferences frequently yield similar results.\nFurthermore, a common misunderstanding regarding Bayesian data analysis and inference is the belief that they are exclusively defined by Bayes’ theorem. Any inferential method employing probability theory incorporates Bayes’ theorem. Numerous examples labeled as “Bayesian” often lack distinctive attributes and instead rely on observed data frequencies, resembling non-Bayesian methods. The distinctiveness of Bayesian techniques lies in their application of Bayes’ theorem to measure the uncertainty associated with theoretical constructs not directly observable, such as parameters and models. Both methodologies can produce robust inferences, although they are grounded in different principles and entail unique trade-offs.\n\n\n\nExample\nCertainly. Here’s the same lecture content rewritten without bold text and formatted in paragraphs for a natural flow, as would be appropriate for lecture notes or a textbook-style explanation.\n\n\n\nLecture Note: Comparing Frequentist and Bayesian Approaches in Clinical Trials\nLet us begin by considering a common scenario in clinical research. Imagine we are conducting a clinical trial to test the effectiveness of a new drug in treating a disease. We divide the participants into two groups: the treatment group, where 50 patients receive the drug, and the control group, where another 50 patients receive a placebo. After the trial, we observe that 30 out of 50 patients in the treatment group recovered, while only 10 out of 50 recovered in the control group.\nUsing a frequentist proportion test, which is typically introduced in introductory statistics courses, we can analyse this data to determine whether the observed difference in recovery rates is statistically significant. The result of the test yields a p-value of 0.0001. Since this value is far below any common significance threshold (such as 0.05), we reject the null hypothesis and conclude that there is a statistically significant difference between the recovery rates in the treatment and control groups.\nNow consider the same scenario from a Bayesian perspective. Rather than testing a null hypothesis, the Bayesian approach estimates the probability that the recovery rate in the treatment group is higher than in the control group. Based on the data, this posterior probability turns out to be approximately 1, which provides strong evidence in favor of the treatment’s effectiveness. In this case, both the frequentist and Bayesian methods lead to the same conclusion, i.e., the treatment appears to work better than the placebo. This agreement is common when we have moderate or large sample sizes and clearly different outcomes between groups.\nTo explore where the Bayesian method can offer a clear advantage over the frequentist approach, let us now consider a scenario involving limited data. Suppose we only have five patients in each group. In the treatment group, 3 out of 5 recover, while in the control group, only 1 out of 5 recovers. This smaller sample makes the analysis much more difficult using a frequentist method. If we perform a hypothesis test under the null hypothesis that the recovery rates in both groups are equal, the p-value we obtain is 0.52. This high p-value means we fail to reject the null hypothesis, and thus the frequentist analysis suggests there is no statistically significant difference between the groups.\nThe Bayesian approach, however, remains informative even in this small-sample scenario. Using a non-informative prior (which we will discuss more later in this unit), meaning we do not assume any specific knowledge about the effectiveness of the treatment or placebo in advance, we combine the prior with no past information with the observed data. This results in a posterior distribution from which we can estimate the probability that the treatment group has a higher recovery rate than the control group. In this case, that probability is approximately 0.882, which indicates strong evidence in favor of the treatment, despite the small number of observations.\nThe figure below illustrates this Bayesian result using a uniform prior.\n\n\n\n\n\nUniform Prior\n\n\n\n\nIn the above example, we did not assume any historical knowledge about the effectiveness of either the treatment or the control group. Now suppose we do have prior information. For example, from earlier studies that the control group typically has a recovery rate of around 70 percent. We can incorporate this knowledge as an informed prior in the Bayesian model. After updating this prior with the current data, the posterior probability that the treatment group has a higher recovery rate than the control group is estimated to be about 0.58. While this still leans in favor of the treatment, it is a much more cautious estimate than the previous one. The ability to incorporate such external information is a unique and valuable feature of the Bayesian framework.\nThe figure below shows the Bayesian result under this informed prior.\n\n\n\n\n\nInformed Prior\n\n\n\n\nWhile both frequentist and Bayesian approaches can be effective tools in data analysis. The key strength of the Bayesian method is its ability to incorporate prior knowledge, such as data from similar treatments or earlier studies, which can improve the quality of estimates, especially when sample sizes are small. Moreover, Bayesian analysis does not reduce inference to a single p-value. Instead, it provides a full posterior distribution, which gives a more nuanced view of the uncertainty around the treatment’s effectiveness.\nIn the next lecture, we will explore the distributional aspects of Bayesian analysis in greater detail. Before that, we will review Bayes’ Theorem and explain it using foundational concepts in probability.",
    "crumbs": [
      "Week 1: **Navigating Evidence**"
    ]
  },
  {
    "objectID": "M01_1.html#dual-factor-probabilities",
    "href": "M01_1.html#dual-factor-probabilities",
    "title": "Week 1: Navigating Evidence",
    "section": "Dual-Factor Probabilities",
    "text": "Dual-Factor Probabilities\n\nKruschke (2014)\nBayesians do not imagine repetitions of an experiment in order to define and specify a probability. Probability is merely taken as a measure of certainty in a particular belief. This implies that the probability is used as a way to quantify how certain we are about a belief or an event happening. Before diving into Bayes’ theorem, let’s first understand some key concepts in probability distributions, which I assume you have already learned in the PSI unit.\nThere are many situations in which we are interested in the conjunction of two outcomes. As a specific example for developing these ideas, consider a situation where the probabilities of various combinations of people’s eye color and hair color. The data come from a particular convenience sample (Snee, 1974), and are not meant to be representative of any larger population.\n\n\n\n\n\nDual-Factor (Snee, 1974); Kruschke (2014)\n\n\n\n\nThe above Table considers four possible eye colors, listed in its rows, and four possible hair colors, listed across its columns. In each of its main cells, the table indicates the joint probability of particular combinations of eye color and hair color. For example, the top-left cell indicates that the joint probability of brown eyes and black hair is 0.11 (i.e., 11%). Notice that not all combinations of eye color and hair color are equally likely. For example, the joint probability of blue eyes and black hair is only 0.03 (i.e., 3%).\nWe may be interested in the probabilities of the eye colors overall, collapsed across hair colors. These probabilities are indicated in the right margin of the table, and they are therefore called marginal probabilities. They are computed simply by summing the joint probabilities in each row, to produce the row sums. For example, the marginal probability of green eyes, irrespective of hair color, is 0.11. The joint values indicated in the table do not all sum exactly to the displayed marginal values because of rounding error from the original data.\nWe often want to know the probability of one outcome, given that we know another outcome is true. For example, suppose I sample a person at random from the population. Suppose I tell you that this person has blue eyes. Conditional on that information, what is the probability that the person has blond hair (or any other particular hair color)? It is intuitively clear how to compute the answer: We see from the blue-eye row of the above Table that the total (i.e., marginal) amount of blue-eyed people is 0.36, and that 0.16 of the population has blue eyes and blond hair. Therefore, of the 0.36 with blue eyes, the fraction 0.16/0.36 has blond hair. In other words, of the blue-eyed people, 45% have blond hair.We also note that of the blue-eyed people, 0.03/0.36 = 8% have black hair.",
    "crumbs": [
      "Week 1: **Navigating Evidence**"
    ]
  },
  {
    "objectID": "M01_1.html#bayes-theorem",
    "href": "M01_1.html#bayes-theorem",
    "title": "Week 1: Navigating Evidence",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\n\nLet’s now go back to the example related to type-2 diabetes, where, a medical practitioner hypothise based on patient’s background history that the patient has a chance of having diabetes.\nThus, we have two events:\n\nThe hypothesis that medical practitioner’s guess is correct \\((G=[+])\\).\nThe evidence: Blood test showing elevated sugar levels \\((E=[+])\\).\n\nNow, given this experimental evidence of elevated blood sugar, how sure are the medical practitioner that their guess about the diabetes is accurate?\n\\[\nPr(\\text{Guess is correct} | \\text{Positive evidence}) = \\text{ ?}\n\\]\nHence, using conditional probability expression we write:\n\\[\nPr(\\text{G=[+]}|\\text{E=[+]}) = \\frac{Pr(\\text{G=[+]},\\text{E=[+]})}{Pr(E=[+])}\n\\]\nwhere, \\(Pr(\\text{G=[+]},\\text{E=[+]})\\) is the joint probabiity that both \\((G=[+])\\) and \\((E=[+])\\) occur. We can rearrange and write the joint probability as:\n\\[\nPr(\\text{G=[+]},\\text{E=[+]}) = Pr(\\text{G=[+]})\\times Pr(\\text{E=[+]}|\\text{G=[+]})\n\\]\nHence, by substituting the joint probability the Bayes theorem states:\n\\[\nPr(\\text{G=[+]}|\\text{E=[+]}) = \\frac{Pr(\\text{G=[+]})\\times Pr(\\text{E=[+]}|\\text{G=[+]})}{Pr(E=[+])}\n\\]\nwhere, \\(Pr(E=[+])\\) is the marginal probability for the blood test showing high suger levels for all possible hypotheses, here, the possible hypotheses are: \\(G=[+]\\) and \\(G=[-]\\). Hence, we write \\(Pr(\\text{G=[+]}|\\text{E=[+]})\\) as:\n\\[\n\\frac{Pr(\\text{G=[+]})\\times Pr(\\text{E=[+]}|\\text{G=[+]})}{Pr(\\text{G=[+]})\\times Pr(\\text{E=[+]}|\\text{G=[+]})+Pr(\\text{G=[-]})\\times Pr(\\text{E=[+]}|\\text{G=[-]})}\n\\]\nwhere, \\(Pr(\\text{G=[+]})\\) and \\(Pr(\\text{G=[-]})\\) are the probabilities of the medical practitioner’s guess is correct and incorrect respectively, thus we write \\(Pr(\\text{G=[-]}) = 1-Pr(\\text{G=[+]})\\) or vise versa.\nWe clearly see that the degree of belief probability after including the evidence is equal to the probability of guess before incorporating the evidence and probability of the evidence with the medical practitioner’s guess.\n\nExample\nTo explain the above example we write, \\(Pr(\\text{G=[+]})=0.2\\), as the medical practitioner guessed that there is a 20% chance the patient has diabetes. The complement, the probability that the patient does not have diabetes, is \\(Pr(\\text{G=[-]})=0.8\\).\nNow, let us define the test’s accuracy:\n\nSensitivity (True Positive Rate or Hit Rate): \\(Pr(\\text{E=[+]}|\\text{G=[+]})=0.85\\), i.e., 85% of diabetics test positive.\nSpecificity (True Negative Rate): \\(Pr(\\text{E=[-]}|\\text{G=[-]})=0.90\\), i.e., 90% of non-diabetics test negative.\nFalse Positive Rate (False Alarm): \\(Pr(\\text{E=[+]}|\\text{G=[-]})=1-Pr(\\text{E=[-]}|\\text{G=[-]})=1-0.9=0.10\\), i.e., 10% of non-diabetics test positive.\n\nThe question is: Given that the test result is positive, how sure is the medical practitioner that the patient truly has diabetes?\nThis is expressed as the posterior probability , which we compute using Bayes’ theorem:\n\\[\nPr(\\text{G=[+]}|\\text{E=[+]}) = \\frac{(0.2\\times 0.85)}{(0.2\\times 0.85) + (0.8\\times 0.1)} = 0.68\n\\]\nAfter observing a positive blood sugar test, the probability that the patient has diabetes increases from 20% to 68%. This means the medical practitioner is now 68% confident in their updated belief that the patient has diabetes.",
    "crumbs": [
      "Week 1: **Navigating Evidence**"
    ]
  },
  {
    "objectID": "M01_1.html#summary",
    "href": "M01_1.html#summary",
    "title": "Week 1: Navigating Evidence",
    "section": "Summary",
    "text": "Summary\nThe key concept of this week’s lecture is that Bayesian ways of thinking are inherently more suited to solving real-life problems compared to frequentist/classical approaches, as they allow for the inclusion of prior information in decision-making, providing a clear advantage in calculating inverse probability.",
    "crumbs": [
      "Week 1: **Navigating Evidence**"
    ]
  },
  {
    "objectID": "M01_1.html#live-tutorial-and-discussion",
    "href": "M01_1.html#live-tutorial-and-discussion",
    "title": "Week 1: Navigating Evidence",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 1: **Navigating Evidence**"
    ]
  },
  {
    "objectID": "M01_1.html#tutorial-exercises",
    "href": "M01_1.html#tutorial-exercises",
    "title": "Week 1: Navigating Evidence",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.\n\n\n\n\nKruschke, J. 2014. Doing Bayesian Data Analysis: A Tutorial with r, JAGS, and Stan. Academic Press.",
    "crumbs": [
      "Week 1: **Navigating Evidence**"
    ]
  },
  {
    "objectID": "M01_2.html",
    "href": "M01_2.html",
    "title": "Week 2: Bayesian Inference",
    "section": "",
    "text": "Learnings\n– LO1: Explain the difference between Bayesian and frequentist concepts of statistical inference.\n– LO2: Demonstrate how to specify and fit simple Bayesian models with appropriate attention to the role of the prior distribution and the data model.\n– LO5: Engage in specifying, checking and interpreting Bayesian statistical analyses in practical problems using effective communication with health and medical investigators.\nBy the end of this week you should be able to:\n– Describe Bayesian inference using probability distributions.\n– Understand Bayesian learning.\n– Draw DAG for Bayesian models.\n– Formulate examples.",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#learnings",
    "href": "M01_2.html#learnings",
    "title": "Week 2: Bayesian Inference",
    "section": "",
    "text": "Outcomes\n\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#inference",
    "href": "M01_2.html#inference",
    "title": "Week 2: Bayesian Inference",
    "section": "Inference",
    "text": "Inference\n\n\n\nBayesian inference is a method of statistical inference that updates the probability of a hypothesis \\((H)\\) as more evidence or data \\((D)\\) becomes available. It is based on Bayes’ theorem that we describe in our last lecture, where prior beliefs are updated with data. Following out previous lecture, if we denote \\(Pr(H)\\) as the probability of initial belief about \\(H\\) before seeing data, we can write the updated belief about \\(H\\) given the data \\(D\\) as:\n\\[\nPr(H|D) = \\frac{Pr(D|H)\\times Pr(H)}{Pr(D)}\n\\]\nwhere, \\(Pr(D)\\) is the probability of the data under all possible hypotheses.\nIn today’s lecture we will learn to the development of Bayesian model using probability distributions that incorporates Bayes throrem.",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#bayesian-models",
    "href": "M01_2.html#bayesian-models",
    "title": "Week 2: Bayesian Inference",
    "section": "Bayesian Models",
    "text": "Bayesian Models\n\nWe have explained Bayes’ rule in our previous lecture, now we will see how Bayes rule can be structured for model development using parameter and data. In a Bayesian model, a parameter is a quantity that we assume is uncertain and assign a probability distribution to it. Unlike in frequentist statistics, where parameters are fixed but unknown, Bayesian statistics treats parameters as random variables with their own probability distributions.\nLet us denote \\(\\theta\\) as the parameter and \\(D\\) as data. Hence, we write a model using the data likelihood \\(p(D|\\theta)\\), and prior distribution \\(p(\\theta)\\) of the model parameter \\(\\theta\\) (note that we have changed the notation from \\(Pr(.)\\) to \\(p(.)\\), where \\(Pr(.)\\) refers to probability and \\(p(.)\\) refers to probability distribution):\n\\[\np(D|\\theta) \\times p(\\theta)\n\\]\nHence, Bayes rule can be used to understand the parameter values, given the data,, i.e.,\n\\[\np(\\theta|D)\n\\]\nThus, using the Dual-Factor table explained in previous lecture, we can write\n\n\n\n\n\nData-Parameter; Kruschke (2014)\n\n\n\n\nWhere, each cell of the table holds the joint probability density of the specific combination of parameter value \\(\\theta\\) and data value \\(D\\), denoted \\(p(D, \\theta)\\), and which we know can be algebraically re-expressed as \\(p(D|\\theta)\\times p(\\theta)\\).\nThus, we write the Bayes rule for data and parameter model as:\n\\[\np(\\theta|D) = \\frac{p(D|\\theta)\\times p(\\theta)}{p(D)};\n\\]\nwhere,\n\\[\\begin{align}\np(D) &= \\sum_{\\theta^*} p(D|\\theta^*) p(\\theta^*); \\quad \\text{ if discrete}; \\\\\np(D) &= \\int p(D|\\theta^*) p(\\theta^*) \\text{d}\\theta^*; \\quad \\text{ if continuous};\n\\end{align}\\]\nHere, \\(p(\\theta)\\) is the prior information about \\(\\theta\\) without observing data; \\(p(D|\\theta)\\) is the likelihood, i.e., data could be generated with model parameter \\(\\theta\\); and \\(p(D)\\) is the marginal likelihood obtained from data by averaging across all possible parameters.\nThe posterior distribution of \\(\\theta\\) is:\n\\[\np(\\theta|D) = \\text{ Credibility of }\\theta\\text{ based on data and evidence}\n\\]\nThe posterior probability distribution \\(p(\\theta|D)\\) is the main goal of Bayesian inference. The posterior distribution summarises our uncertainty over the value of a parameter. If the distribution is narrower, then this indicates that we have greater confidence in our estimates of the parameter’s value, and this can be obtained by collecting more data.\nWe will now explore with examples on obtaining posterior distributions of the model parameter \\(\\theta\\) for different data distributions (i.e., models), e.g., binomial, normal and poisson distributions.",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#model-with-binary-variable",
    "href": "M01_2.html#model-with-binary-variable",
    "title": "Week 2: Bayesian Inference",
    "section": "Model with Binary Variable",
    "text": "Model with Binary Variable\n\nSuppose we have a binary observation i.e., can take values either 0 or 1, which follows Bernoulli distribution with parameter \\(\\theta\\). We already know that for \\(n&gt;1\\) number of trials the Bernoulli distribution yields a Binomial distribution. Considering \\(Y\\) as the random variable of number of successes in \\(n\\) trials for the Binomial distribution, we can write:\n\\[\np(Y=y|\\theta) = \\begin{pmatrix}n\\\\y \\end{pmatrix} \\theta^y (1-\\theta)^{n-y}\n\\]\nThis also represents the likelihood of a Bernoulli variable.\nNow, if we consider a Beta prior distribution for \\(\\theta\\) with hyper-parameters \\(a\\) and \\(b\\) (i.e., shape parameters of Beta distribution), then we can write the probability density function of the prior distribution as:\n\\[\np(\\theta) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\n\\]\nwhere, \\(B(a,b)=\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\) is the beta function. Thus, using Bayes theorem we write\n\\[\np(\\theta|y) = \\frac{\\begin{pmatrix}n\\\\y \\end{pmatrix} \\theta^y (1-\\theta)^{n-y}\\times \\theta^{a-1}(1-\\theta)^{b-1}}{p(y)\\times B(a,b)}\n\\]\nWith some simple calculations, we can write the marginal likelihood as:\n\\[\np(y) = \\begin{pmatrix}n\\\\y \\end{pmatrix}\\frac{B(y+a,n-y+b)}{B(a,b)}\n\\]\nHence, we get the analytical form of the posterior distribution of \\(\\theta\\) as:\n\\[\np(\\theta|y) = \\frac{\\theta^{y+a-1}(1-\\theta)^{n-y+b-1}}{B(y+a,n-y+b)}\n\\]\nwhich follows a Beta distribution with parameters \\((a+y)\\) and \\((b+n-y)\\).\nExample:\nLet us explain this with the type-2 diabetes example we discussed earlier. Now, the medical practitioner is trying to estimate the probability that a patient has type-2 diabetes \\(\\theta\\) based on both prior knowledge and a new diagnostic test result.\nBefore any test, the medical practitioner relies on existing medical data. Suppose past research suggests that for a certain risk group the probability of having type-2 diabetes is 0.5. We represent this belief using a \\(Beta(a=2,b=2)\\). This prior suggests that while any probability is possible, \\(\\theta\\) is likely to be around 0.5, with room for updating.\nNow, we assume the test result corresponds to 7 positive cases out of 10 tests, meaning, \\(n=10\\) and \\(y=7\\).\nHence, we get the posterior distribution of medical practitioner’s new belief about the probability of the patient having the disease as: \\(Beta(2+7,2+3)=Beta(9,5)\\).\nWe can see from the density plots, the posterior shifts toward higher probabilities of type-2 diabetes, meaning the medical practitioner is now more confident that the patient may have type-2 diabetes.\n\n\nCode\nlibrary(ggplot2)\na_prior &lt;- 2   \nb_prior &lt;- 2   \nn &lt;- 10        \ny &lt;- 7         \na_post &lt;- a_prior + y\nb_post &lt;- b_prior + (n - y)\ntheta &lt;- seq(0, 1, length.out = 100)\nprior_density &lt;- dbeta(theta, a_prior, b_prior)\nlikelihood &lt;- dbinom(y, size = n, prob = theta)\nposterior_density &lt;- dbeta(theta, a_post, b_post)\ndata &lt;- data.frame(\n  theta = rep(theta, 3),\n  density = c(prior_density, likelihood, posterior_density),\n  Distribution = rep(c(\"Prior\", \"Likelihood\", \"Posterior\"), each = length(theta))\n)\nggplot(data, aes(x = theta, y = density, color = Distribution)) +\n  geom_line(size = 1) +\n  labs(title = \"Prior, Likelihood, and Posterior Distributions\",\n       x = expression(theta),\n       y = \"Density\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"green\",\"red\",\"blue\"))",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#bayesian-odds",
    "href": "M01_2.html#bayesian-odds",
    "title": "Week 2: Bayesian Inference",
    "section": "Bayesian Odds",
    "text": "Bayesian Odds\n\n\nWe can also think of the Bayesian approach in terms of odds, such as what odds should I assign to an event or hypothesis. The simple definition of odds can be written as:\n\\[\n\\text{Odds of an event} = \\frac{Pr(\\text{event})}{1-Pr(\\text{event})}\n\\] which represents the ratio between the occurrence of an event and its non-occurrence.\nLet’s revisit the medical practitioner example from our first lecture to clarify this concept. Imagine the practitioner assessing a patient for diabetes. Initially, they have a prior belief about the patient’s likelihood of having the condition. After conducting a blood test that reveals elevated sugar levels, this new evidence increases the probability of diabetes. The practitioner then updates their belief accordingly, refining their assessment based on the test results. We wanted to know, given this experimental evidence, how sure are the medical practitioner that their guess about the diabetes is accurate?\nThus, to reflect the medical practitioner example by Bayesian odds, we write:\n\\[\n\\text{Odds}(\\text{G=[+]}|\\text{E=[+]}) = \\text{Odds}(\\text{G}) \\times \\frac{Pr(\\text{E=[+]}|\\text{G=[+]})}{Pr(\\text{E=[+]}|\\text{G=[-]})}\n\\]\nwhere, \\(\\text{Odds}(\\text{G=[+]}|\\text{E=[+]})\\) is the odds of the guess is correct given the evidence, and \\(\\text{Odds}(\\text{G})\\) is the odds of the guess that we define:\n\\[\n\\text{Odds}(\\text{G}) = \\frac{Pr(\\text{G=[+]})}{Pr(\\text{G=[-]})}\n\\]\nand \\(\\frac{Pr(\\text{E=[+]}|\\text{G=[+]})}{Pr(\\text{E=[+]}|\\text{G=[-]})}\\) is the ratio of evidence under the guess \\(\\text{G}\\).\nThis reflects\n\\[\n\\text{posterior or updated odds} = \\text{prior or initial odds}\\times \\text{relative explanatory power}\n\\]\nThis explains that evidence (i.e., data/information) always changes the outcome, which could be probability, probability distributions or odds or any other outcome of interest.\nExample\nLet us explain this again with the type-2 diabetes example, where based on the patient’s age, family history, and some initial symptoms, the medical practitioner guessed that there’s a 20% chance the patient has diabetes. This belief is a prior probability. The odds from of this probability is:\n\\[\n\\text{Odds}(\\text{G}) = \\frac{0.2}{0.8} = 0.25\n\\]\nSo, the prior odds of the patient having diabetes are 1:4 (one in four) guessed by the medical practitioner.\nThe blood test result shows elevated sugar levels. To assess how much this result affects our belief, we use the ratio of evidence under the guess \\(\\text{G}\\) (also known as the likelihood ratio).\nSuppose the medical practitioner has historically observed from boold test that 85% of diabetic patients have elevated sugar levels, while only 10% of non-diabetic patients do (e.g., due to other factors). Hence, we write\n\\[\n\\frac{Pr(\\text{E=[+]}|\\text{G=[+]})}{Pr(\\text{E=[+]}|\\text{G=[-]})} = \\frac{0.85}{0.10} = 8.5\n\\]\nThis means the test result (elevated sugar) is 8.5 times more likely in someone with diabetes than in someone without it.\nNow, the posterior odds\n\\[\n\\text{Odds}(\\text{G=[+]}|\\text{E=[+]}) = 0.25 \\times 8.5 = 2.125\n\\]\nSo, the updated odds is 2.125:1, i.e., the patient is 2.125 times more likely to have diabetes than non-diabetic individuals after considering the test result.\nUsing the well known relationship between odds and probability, we can also get back the posterior probability from the posterior odds as:\n\\[\nPr(\\text{G=[+]}|\\text{E=[+]}) = \\frac{\\text{Odds}(\\text{G=[+]}|\\text{E=[+]})}{1+\\text{Odds}(\\text{G=[+]}|\\text{E=[+]})} = \\frac{2.125}{1+2.125} = 0.68\n\\]\nIn summary we write, before the test, the medical practitioner believed there was a 20% chance of diabetes. After seeing the elevated blood sugar result, the probability increased to 68% (compare the example in lecture 1), because the test result is 8.5 times more likely in diabetic than non-diabetic individuals. The practitioner may now recommend further tests (e.g., an HbA1c test) before confirming the diagnosis.",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#bayesian-language",
    "href": "M01_2.html#bayesian-language",
    "title": "Week 2: Bayesian Inference",
    "section": "Bayesian Language",
    "text": "Bayesian Language\n\n\n\nWe will learn a common language for illustrating and denoting the Bayesian models. This will help us to develop and write complex Bayesian models in a simpler way that we will learn later in this course.\nLet’s explain this using a Bernoulli model with parameter \\(\\theta\\). Let \\(x\\) be the random variable that follows Bernoulli distribution. If we have \\(n\\) number of independent observations \\((x_1,\\ldots,x_n)'\\), then denoting \\(y=\\sum_i^n x_i\\) we write the likelihood function as:\n\\[\n\\prod_i^n \\theta^{x_i} (1-\\theta)^{1-x_i} = \\theta^{\\sum_i^n x_i} (1-\\theta)^{\\sum_i^n (1-x_i)} = \\theta^y (1-\\theta)^{n-y}\n\\]\nwhich we can write in the form of a Binomial distribution:\n\\[\np(y|\\theta) \\propto   \\theta^y (1-\\theta)^{n-y}\n\\]\nNote that we replaced “=” with “\\(\\propto\\)” as the term \\(\\begin{pmatrix}n\\\\y \\end{pmatrix}\\) is a constant, which does not depend on the parameter \\(\\theta\\). Now, considering prior conjugacy, we assume that \\(\\theta\\) follows a Beta distribution with shape parameters \\(a\\) and \\(b\\) and we write \\(\\theta \\sim \\text{Beta}(a,b)\\) and define\n\\[\np(\\theta) \\propto \\theta^{a-1}(1-\\theta)^{b-1}\n\\]\nThus, the posterior distribution of \\(\\theta\\) can be written as \\(\\theta|y \\sim \\text{Beta}(a+y,b+n-y)\\) and defined as:\n\\[\np(\\theta|y) \\propto \\theta^y (1-\\theta)^{n-y}\\theta^{a-1}(1-\\theta)^{b-1}\n\\]\n\\[\np(\\theta|y) \\propto \\theta^{y+a-1} (1-\\theta)^{n-y+b-1}\n\\]\nWe have again included the “\\(\\propto\\)” term, as we will learn in our next two lectures that the marginal distribution of the data, i.e., \\(p(y)\\) does not depend on the model parameter and can therefore be omitted when obtaining the posterior distribution.",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#directed-acyclic-graph-dag",
    "href": "M01_2.html#directed-acyclic-graph-dag",
    "title": "Week 2: Bayesian Inference",
    "section": "Directed Acyclic Graph (DAG)",
    "text": "Directed Acyclic Graph (DAG)\n\nThe directed acyclic graph (DAG) is a graphical representation that illustrates the relationships and dependencies between different variables and parameters in a model. In a DAG, nodes represent variables/parameters, and directed edges indicate dependency relationships, pointing from one node to another. The acyclic nature ensures that there are no closed loops, maintaining a clear directionality of influence from parent nodes to child nodes.\nBayesian models\nIn the context of Bayesian modelling, DAGs are crucial as they visually provides the conditional dependencies between variables and model parameters, which enables a structured and intuitive approach to understand uncertainty and dependencies from the model.\nUsing the model with binary observations, we write a DAG as:\n\n\n\n\n\n\nThis is a simple graphical model, where \\(y\\) depends on \\(\\theta\\), with \\(\\theta\\) being a logical function of hyper-parameters \\(a\\) and \\(b\\).",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#more-examples",
    "href": "M01_2.html#more-examples",
    "title": "Week 2: Bayesian Inference",
    "section": "More Examples",
    "text": "More Examples\n\nDecision Making\n\nHere, we will explore how Bayesian method can provide more insights compared to the frequentist approach for decision making with same information.\nLet us assume that we have a population of patients, and in this population, the prevalence of a certain disease is either 10% or 20%. As a researcher you need to determine whether the true prevalence of the disease is 10% or 20%.\nYou are being asked to make a crucial decision, and if you make the correct determination, your research funding is renewed.\nHence, you conduct diagnostic tests on randomly selected patients from the population. Each test costs $200, and you must purchase tests in increments of $1,000 (5 tests at a time). You have a total budget of $4,000, meaning you can test 5, 10, 15, or 20 persons.\nThe cost of making an incorrect decision is high, as it could lead to misinformed healthcare policies or ineffective treatment strategies. However, conducting tests is also expensive, so you don’t want to spend more than necessary.\nSuppose you observe one person with the disease in a sample size of 5, two persons in a sample size of 10, three persons in a sample size of 15, and four persons in a sample size of 20.\nFequentists Method\nBased on the scenario we write the null \\((H_0)\\) and alternative \\((H_1)\\) hypotheses as:\n\\(H_0\\): \\(10\\)% prevalence \\(H_1\\): \\(&gt;10\\)% prevalence\np-value based on 5 samples:\n\\[\nPr(y\\ge 1|n=5,p=0.10)=1-Pr(y= 0|n=5,p=0.10)=1-0.9^{5}\\approx 0.41\n\\]\nNote that the p-value represents the probability of observing the given results - or more extreme ones - assuming the null hypothesis is true.\nTherefore, we fail to reject \\(H_0\\) and conclude that the data do not provide convincing evidence that the prevalence of the disease is greater than 10%. This means that if we had to choose between a disease prevalence of 10% or 20%, even though this hypothesis test does not confirm the null hypothesis, we would likely stick with 10%, since we did not find sufficient evidence to suggest a higher prevalence.\nBayesian Method\nFrom Bayesian perspective we can write:\n\\(H_0\\): 10% prevalence \\(H_1\\): 20% prevalence\nWe can also assume that priors related to the hypothesis same and equal, i.e, \\(Pr(H_0)=Pr(H_1)=0.5\\).\nConsidering the scenario with 5 samples, we write the likelihood for \\(H_0\\) and \\(H_1\\) as:\n\\[\nPr(y=1|H_0) = \\begin{pmatrix} 5 \\\\ 1 \\end{pmatrix} \\times 0.1 \\times 0.9^{4} \\approx 0.33\n\\]\n\\[\nPr(y=1|H_1) = \\begin{pmatrix} 5 \\\\ 1 \\end{pmatrix} \\times 0.2 \\times 0.8^{4} \\approx 0.41\n\\]\nWe get the posterior probability for \\(H_0\\) as:\n\\[\nPr(H_0|y=1) = \\frac{Pr(y=1|H_0)\\times Pr(H_0)}{Pr(y=1)}=\\frac{0.5\\times 0.33}{0.5\\times 0.33+0.5\\times 0.41} \\approx 0.45\n\\]\nand for \\(H_1\\) as:\n\\[\nPr(H_1|y=1) = 1- 0.45 = 0.55\n\\]\nThe posterior probabilities of \\(H_0\\) and \\(H_1\\) are close to each other. As a result, with equal priors and a low sample size, it is difficult to make a decision with strong confidence based on the observed data. However, \\(H_1\\) has a higher posterior probability than \\(H_0\\), so if we had to make a decision at this point, we should choose \\(H_1\\), meaning the disease prevalence is 20%. Note that this decision contradicts the conclusion reached using the frequentist approach.\nWe can write this example using a DAG, where we can represent the relationships between the prior probabilities, likelihoods, and posterior probabilities.\nHypothesis (H₀ and H₁): These represent the two possible prevalence values (10% for H₀, 20% for H₁).\nData (y): The observed data (the number of successes in 5 samples, e.g., the presence of disease).\nLikelihoods (Pr(y=1|H₀) and Pr(y=1|H₁)): The likelihood of observing the data under each hypothesis.\nPrior Probabilities (Pr(H₀) and Pr(H₁)): The prior belief about the hypotheses (both have a prior probability of 0.5).\nPosterior Probabilities (Pr(H₀|y=1) and Pr(H₁|y=1)): The updated belief about the hypotheses after observing the data.\n\n\n\n\n\n\nNow, we can summarise the results for all four seperate sample sizes using frequentist p-value and Bayesian methods, where\nFrequentist\n\\(H_0\\): 10% prevalence and \\(H_1\\): \\(&gt;10\\)% prevalence\n\\(Pr(y \\text{ or more} \\mid 10\\% \\text{ prevalence})\\) = p-value\nBayesian\n\\(H_0\\): 10% prevalence and \\(H_1\\): 20% prevalence\nPosterior for \\(H_0\\): \\(Pr(10\\% \\text{ prevalence} \\mid n, y)\\)\nPosterior for \\(H_1\\): \\(Pr(20\\% \\text{ prevalence} \\mid n, y)\\)\nAnd the results we get:\n\n\n\n\n\n\n\n\n\nData\np-value\nPosterior\n\n\n\n\n\n\n\\(H_0\\): 10% prevalence\n\\(H_0\\): 10% prevalence\n\\(H_1\\): 20% prevalence\n\n\nn = 5, y = 1\n0.41\n0.45\n0.55\n\n\nn = 10, y = 2\n0.26\n0.39\n0.61\n\n\nn = 15, y = 3\n0.18\n0.34\n0.66\n\n\nn = 20, y = 4\n0.13\n0.29\n0.71\n\n\n\nIn each of these scenarios, the frequentist approach produces a p-value higher than the significance level say 5%, leading us to fail to reject the null hypothesis for any of the samples. In contrast, the Bayesian method consistently assigns a higher posterior probability to the model where prevalence = 20%. As a result, the conclusions drawn from these two approaches are contradictory.\nHowever, if we had structured the frequentist approach differently, say setting the null hypothesis as \\(H_0\\): 20% prevalence, we would have reached different conclusions.\nThis highlights the frequentist method’s sensitivity to the choice of the null hypothesis. In contrast, the Bayesian approach yields consistent results regardless of the order in which we evaluate the models.\n\n\nComparison of Two Means\nNow, let us explain another example, where we want to find out if a new drug lowers systolic blood pressure (SBP) compared to standard of care (SOC).\nThe study involves two groups of patients: Group A (treatment group), where 10 patients are given the new medication, and Group B (control group), where 10 patients receive the standard of care (SOC), which includes existing medication for SBP.\nAfter a period of four weeks, the SBP of all participants is recorded for analysis.\nNow, we will use both the frequentist and Bayesian approaches to determine whether there are any differences between the treatment and SOC.\nFrequentist\nFrom the frequentist perspective, we wil test:\n\\(H_0\\): There is no difference in mean SBP between the two groups.\n\\(H_1\\): The treatment group has a different mean SBP than the SOC group.\nConsidering equal variance assumption, we use t-test as follows:\n\n\nCode\ngroup_A &lt;- c(120, 118, 122, 115, 119, 117, 121, 116, 118, 119)\ngroup_B &lt;- c(130, 128, 135, 132, 129, 131, 134, 133, 137, 130)\nt.test(group_A, group_B, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  group_A and group_B\nt = -11.834, df = 18, p-value = 6.315e-10\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -15.77898 -11.02102\nsample estimates:\nmean of x mean of y \n    118.5     131.9 \n\n\nWe can see that the p-value is extremely small (much less than 0.05), which means we reject the null hypothesis. Si, there is strong evidence that the new drug significantly reduces systolic blood pressure compared to the placebo.\nBayesian\nNow, we will use Bayesian approach to estimate the posterior probability distribution of the difference in mean systolic blood pressure between the treatment and SOC groups, and extract a 95% credible interval.\nIn most scenarios, we do not have prior information regarding the effectiveness of the new drug. However, sometimes we do have prior knowledge about the effectiveness of the SOC. Let us assume that, from a past study with 15 patients, for the SOC group, after a period of four weeks, the average SBP is around 110 mmHg with a variance of 25. This constitutes our prior knowledge about the control/SOC group.\nNow, using these information we can get the posterior distributions for both groups and then compare their differences. Note that our variable of interest in this example is in continuous scale (SBP measured in mmHg), hence, we will use normal distributions for the SBP.\nHere, we have observe data from 10 individuals for each groups. The posterior mean and variance calculation involves combining prior and observed data for the SOC (control) group, i.e., Group B. Whereas, for the treatment group we do not have any prior knowledge, i.e., the posterior for this group will reflect the observed data. Then we calculate the posterior mean and variance for the difference in means and extract the 95% credible interval for the difference.\nWe can write this mathematically as follows:\n\n\\(\\mu_A\\): True mean SBP of treatment group\n\\(\\mu_B\\): True mean SBP of SOC group\n\\(\\delta = \\mu_A - \\mu_B\\): Difference in means, this is our parameter of interest\n\nFor observed data, Groups A (treatment) and B (SOC) with observed sample sizes \\(n_A = n_B = 10\\), we get observed means \\(\\bar{x}_A\\), \\(\\bar{x}_B\\), and variances \\(s_A^2\\), \\(s_B^2\\).\nWe will model SBP as Normally distributed:\n\n\\(X_A \\sim N(\\mu_A, \\sigma^2_A)\\)\n\\(X_B \\sim N(\\mu_B, \\sigma^2_B)\\)\n\nNow, prior information on Group B, that we get from a previous study, where \\(\\mu_{B,0} = 110\\), and prior variance \\(\\sigma^2_{B,0} = 25\\) from sample size \\(n_{B,0} = 15\\).\nFor the treatment group (i.e., A), we get the posterior distribution as:\n\\[\n\\mu_A \\mid \\text{data} \\sim N\\left( \\bar{x}_A, \\frac{s_A^2}{n_A} \\right)\n\\]\nNow, for the SOC group (i.e., B) we write the prior\n\\[\n\\mu_B \\sim N\\left( \\mu_{B,0}, \\frac{\\sigma_{B,0}^2}{n_{B,0}} \\right)\n\\]\nHence, with observed data \\(\\bar{x}_B\\) from \\(n_B = 10\\) samples, the posterior becomes:\n\\[\n\\mu_B \\mid \\text{data} \\sim N\\left( \\mu_{B,\\text{post}}, \\sigma^2_{B,\\text{post}} \\right)\n\\]\nWhere with simple calculation we write:\n\\[\n\\mu_{B,\\text{post}} = \\frac{ \\frac{n_{B,0}}{\\sigma^2_{B,0}} \\mu_{B,0} + \\frac{n_B}{s_B^2} \\bar{x}_B }{ \\frac{n_{B,0}}{\\sigma^2_{B,0}} + \\frac{n_B}{s_B^2} }\n\\]\nand\n\\[\n\\sigma^2_{B,\\text{post}} = \\left( \\frac{n_{B,0}}{\\sigma^2_{B,0}} + \\frac{n_B}{s_B^2} \\right)^{-1}\n\\]\nNow, we get the posterior for difference in means assuming independence between \\(\\mu_A\\) and \\(\\mu_B\\) as:\n\\[\n\\delta = \\mu_A - \\mu_B \\sim N(\\mu_{\\delta}, \\sigma^2_{\\delta})\n\\]\nWhere,\n\\[\n\\mu_{\\delta} = \\mu_{A,\\text{post}} - \\mu_{B,\\text{post}} = \\bar{x}_A - \\mu_{B,\\text{post}}\n\\]\nand\n\\[\n\\sigma^2_{\\delta} = \\sigma^2_{A,\\text{post}} + \\sigma^2_{B,\\text{post}} = \\frac{s_A^2}{n_A} + \\sigma^2_{B,\\text{post}}\n\\]\nFinally, we can calculate the 95% credible interval of \\(\\delta\\) from its posterior distribution.\n\n\nCode\nmu_prior_B &lt;- 110    \nvar_prior_B &lt;- 25    \nn_prior_B &lt;- 15       \nn_B &lt;- length(group_B)\nmean_B &lt;- mean(group_B)\nvar_B &lt;- var(group_B)\nn_A &lt;- length(group_A)\nmean_A &lt;- mean(group_A)\nvar_A &lt;- var(group_A)\nmu_post_B &lt;- (n_B * mean_B + n_prior_B * mu_prior_B) / (n_B + n_prior_B)\nvar_post_B &lt;- var_B / n_B\nmu_post_A &lt;- mean_A\nvar_post_A &lt;- var_A / n_A\nmean_diff &lt;- mu_post_A - mu_post_B\nvar_diff &lt;- var_post_A + var_post_B\nsd_diff &lt;- sqrt(var_diff)\nset.seed(1234)\ncredible_interval &lt;- quantile(rnorm(10000, mean_diff, sd_diff),prob=c(0.025,0.975))\ncat(\"Posterior mean difference in SBP:\", round(mean_diff,2), \"\\n\")\n\n\nPosterior mean difference in SBP: -0.26 \n\n\nCode\ncat(\"95% credible interval for difference in SBP:\", round(credible_interval,2), \"\\n\")\n\n\n95% credible interval for difference in SBP: -2.43 1.94 \n\n\nThe results shows, on average, the treatment group has a slightly lower SBP than the SOC group, just 0.26 mmHg lower, which is a very small difference. We can also see the 95% credible interval spans both negative and positive values, including zero, we can’t say with confidence whether the treatment lowers, increases, or has no effect on SBP.\nThis result from Bayesian analysis again contradicts with the frequentist method due to the consideration of prior information related to the SOC in the analysis.\nWe will explore more about how different prior information might influence our results throughout the course.\nNote that in this example, we incorporate prior information as past data and thus update the posterior, which is in line with the Bayesian philosophy. It is also important to note that we might arrive at the same conclusion using a frequentist method, if we consider these historical data from \\(n_{B,0}\\) samples.\nIn the following section, we discuss more on this type of Bayesian update accordingly.",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#bayesian-updating",
    "href": "M01_2.html#bayesian-updating",
    "title": "Week 2: Bayesian Inference",
    "section": "Bayesian Updating",
    "text": "Bayesian Updating\n\nSuppose we observe some data \\(D_1\\) and update our posterior belief \\(p(\\theta|D_1)\\) based on prior \\(p(\\theta)\\). Now, if we observe some more data, say \\(D_2\\), then can update our belief from \\(p(\\theta|D_1)\\) to \\(p(\\theta|D_1,D_2)\\). We can explain the Bayesian updating using a step-by-step process.\nFor example, we want to know the more about the severe side effects of an antibacterial drug. Here, we are considering the variable as either the drug has side effect or not. Initially, we might not know anything about the drug, in this case we do not have a prior information. Now we start to observe or collect data and can see the following patterns of the probabilities related to side effects.\n\n\n\n\n\nBayesian Learning, McElreath (2020)\n\n\n\n\nThis figure illustrates how our understanding of the probability of severe side effects from an antibacterial drug evolves as we collect more data. The x-axis represents the probability of side effects, while the y-axis represents the density of this probability distribution. The solid lines in each plot represent our updated belief based on collected data, whereas the dashed lines represent our prior belief before incorporating the new data. As we observe more cases, our confidence in estimating the probability of side effects changes dynamically.\nIn the first plot (n = 1, YES), we have only one observation, meaning we have very little information to work with. The probability distribution is relatively simple and does not strongly favor any particular probability value. Since the first observed case showed side effects, the probability of severe side effects starts increasing. In contrast, in the second plot (n = 2, NO), the second observation indicates no side effects, which alters our belief and introduces more uncertainty. The probability distribution now has a curved shape, reflecting this new piece of evidence.\nAs more data points are collected (n = 3, YES and n = 4, YES), our probability distribution starts forming a clearer peak. This means that we are beginning to refine our estimate of how likely the drug is to cause severe side effects. By n = 5 (YES), the probability distribution has a well-defined peak, indicating that we are gaining confidence in our estimate. However, in n = 6 (NO), the introduction of a new negative observation (no side effects) shifts our belief again. This suggests that the probability of side effects is not as high as previously estimated, and our model adjusts accordingly.\n\n\n\n\n\n\nInvariance to data-order\nThis Bayesian learning process for \\(\\theta\\) is very intuitive, where we can write\n\\[\\begin{align}\np(\\theta|D_1,D_2) = \\frac{p(D_1,D_2|\\theta)\\times p(\\theta)}{\\sum_{\\theta^*}p(D_1,D_2|\\theta^*)\\times p(\\theta^*)}\n\\end{align}\\] If we consider \\(D_1\\) and \\(D_2\\) are independent, then \\[\\begin{align}\np(D_1,D_2|\\theta)=p(D_1|\\theta)p(D_2|\\theta)\n\\end{align}\\] This leads to a very simple formation of considering \\[\\begin{align}\np(\\theta|D_2,D_1) = \\frac{p(D_2|\\theta)\\times p(\\theta|D_1)}{\\sum_{\\theta^*}p(D_2,D_1|\\theta^*)\\times p(\\theta^*)}\n\\end{align}\\]\nThis implies, under the independence condition, the order of Bayesian updating has no effect on the final posterior. Hence, going back to Figure above: we will get same final result (i.e., posterior probability of side effect) for considering all six observations.",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#summary",
    "href": "M01_2.html#summary",
    "title": "Week 2: Bayesian Inference",
    "section": "Summary",
    "text": "Summary\nToday’s lecture explored Bayesian inference, focusing on how to derive it analytically, we also learn Bayesian updating, posterior odds and Directed Acyclic Graphs (DAGs).",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#live-tutorial-and-discussion",
    "href": "M01_2.html#live-tutorial-and-discussion",
    "title": "Week 2: Bayesian Inference",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "M01_2.html#tutorial-exercises",
    "href": "M01_2.html#tutorial-exercises",
    "title": "Week 2: Bayesian Inference",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.\n\n\n\n\nKruschke, J. 2014. Doing Bayesian Data Analysis: A Tutorial with r, JAGS, and Stan. Academic Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan (2nd Edition). Chapman; Hall/CRC.",
    "crumbs": [
      "Week 2: **Bayesian Inference**"
    ]
  },
  {
    "objectID": "brief_module_02.html",
    "href": "brief_module_02.html",
    "title": "Module 2: Chaotics?",
    "section": "",
    "text": "Summary\nIn this part, we will be strengthening our understanding of Bayesian analysis by exploring how we represent, summarise, and work with probability distributions. We begin by examining what prior and posterior distributions are and how they function in the Bayesian framework.\nNext, we dig deeper into prior distributions. We will learn to distinguish between informative, non-informative, and weakly informative priors. Informative priors reflect strong existing knowledge or expert opinion. Non-informative priors attempt to contribute as little information as possible, and weakly informative priors strike a balance, offering gentle regularisation without overwhelming the data. By understanding these types, we can make principled choices when defining priors in our models.\nWe will learn how to interpret and communicate Bayesian results using posterior summary, i.e., using means, medians, credible intervals, and other descriptive statistics.\nWe will also look at exact inference, the process of analytically computing posterior distributions when mathematical forms are tractable. While exact solutions are ideal, they are not always possible, which is why much of Bayesian analysis relies on computational methods.\nWe then explore generative models, a fundamental concept in Bayesian thinking where we define a complete data-generating process based on assumptions and parameters. These models help us simulate new data and test whether our assumptions are realistic.\nAs we build more complex models, we face the challenge of untraceability, where we can no longer compute posterior distributions analytically. To solve this, we turn to powerful computational methods, in particular, Markov chain Monte Carlo (MCMC) techniques. These methods allow us to approximate posterior distributions through simulation, enabling inference in otherwise intractable models.\nWe will learn about key MCMC algorithms, starting with the Metropolis-Hastings algorithm, which generates samples by proposing changes and accepting them with a probability that ensures proper convergence. We then study Gibbs sampling, a special case that works efficiently when we can sample from conditional distributions directly. Finally, we introduce Hamiltonian Monte Carlo (HMC), an advanced method that uses concepts from physics to sample more efficiently in high-dimensional spaces.\nTo ensure the reliability of our results, we also focus on MCMC diagnostics. We’ll learn to assess whether our MCMC algorithms have converged and whether the samples we’ve drawn are sufficient to represent the true posterior. This step is crucial for validating the quality of our inference.\nLastly, we explore prior and posterior predictive checks. These techniques allow us to test whether our models make reasonable predictions, both before and after observing the data. Predictive checks help us evaluate how well our model fits and whether our assumptions hold in practice.\nBy the end of this section, we will have gained practical and theoretical tools to build, refine, and evaluate Bayesian models using both exact and approximate inference. These skills will prepare us to handle a wide range of problems where uncertainty and complexity demand more than just point estimates.",
    "crumbs": [
      "**Module 2:** Chaotics?"
    ]
  },
  {
    "objectID": "M02_1.html",
    "href": "M02_1.html",
    "title": "Week 3: Prior and Posterior",
    "section": "",
    "text": "Learning\n– L02: Demonstrate how to specify and fit simple Bayesian models with appropriate attention to the role of the prior distribution and the data model.\n– L04: Demonstrate proficiency in using statistical software packages (R) to specify and fit models, assess model fit, detect and remediate non-convergence, and compare models.\nBy the end of this week you should be able to:\n– Understand the importance of prior distributions.\n– Calculate posterior using Bayesian exact inference.\n– Distinguish between different types of Prior distributions\n– Formulate examples.",
    "crumbs": [
      "Week 3: **Prior and Posterior**"
    ]
  },
  {
    "objectID": "M02_1.html#learning",
    "href": "M02_1.html#learning",
    "title": "Week 3: Prior and Posterior",
    "section": "",
    "text": "Outcomes\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 3: **Prior and Posterior**"
    ]
  },
  {
    "objectID": "M02_1.html#prior-to-posterior-distribution",
    "href": "M02_1.html#prior-to-posterior-distribution",
    "title": "Week 3: Prior and Posterior",
    "section": "Prior to Posterior Distribution",
    "text": "Prior to Posterior Distribution\n\n\n\n\nGelman et al. (2013)\n\\[\\begin{align}\n\\text{initial belief} \\xrightarrow[]{\\text{Bayes rule + data}} \\text{new belief}\n\\end{align}\\]\nFirst, let’s look at some real-world examples of how prior distributions play a crucial role, particularly in health and medical data.\nWe already got a flavour that prior distributions allow researchers to incorporate existing knowledge or expert opinions into their statistical models, improving the precision of their conclusions. For example, when designing treatments/interventions, prior knowledge helps tailor them to individual patients, enhancing both personalisation and effectiveness.\nImagine historical data shows that a certain drug has a 70% success rate in patients with similar profiles. By using an informative prior distribution, we can incorporate this insight to estimate the likelihood of success for a new patient. If fresh data suggests an 80% effectiveness rate, Bayesian methods refine this estimate, leading to a more balanced probability—say, around 75%. This synthesis of prior knowledge with new evidence results in more reliable conclusions.\nPrior information is also invaluable when historical data is limited or when researching new treatments. In such cases, a non-informative prior—such as assuming an initial effectiveness probability of 0.5—can be used, gradually updating as more data becomes available.\n\nPosterior Summary\n\nThe posterior distribution is a valid probability distribution obtained through Bayesian inference, representing the updated beliefs about a parameter after incorporating prior knowledge and observed data. While the full posterior distribution provides a comprehensive summary of uncertainty, policymakers often require point estimates to facilitate decision-making. Common Bayesian point estimates include the posterior mean, posterior median, and maximum a posteriori (MAP) estimate. Among these, the posterior mean and median are generally preferred over the MAP estimate because MAP focuses solely on the mode of the posterior density, ignoring the overall distribution’s shape and probability mass. This can lead to misleading inferences, especially when the posterior distribution is skewed or multimodal. Moreover, since MAP is a mode, it may lie far from the regions of high probability mass, making it a less robust estimator compared to the mean or median.\n\n\n\n\n\nPosterior Point Estimates; Lambert (2018)\n\n\n\n\nBeyond point estimates, it is also important to quantify uncertainty, which is where credible intervals come into play. A credible interval provides an interval within which the true parameter value is likely to lie with a specified probability (e.g., 95%). Unlike frequentist confidence intervals, credible intervals have a direct probabilistic interpretation: if a 95% credible interval for a parameter is \\([a, b]\\), we can say there is a 95% probability that the parameter lies within this range, given the observed data and prior information. This makes credible intervals particularly useful for policy decisions, as they provide a natural way to express uncertainty in estimates, allowing decision-makers to weigh risks and benefits accordingly.\nExample\nConsider a policymaker estimating the proportion of a population that supports a new public health initiative. Suppose they collect survey data from 1,000 people, where 600 respondents express support. A Bayesian approach models this as a binomial likelihood with a Beta prior (a common conjugate prior for proportions). If the prior is \\(\\text{Beta}(2,2)\\), which represents a weak prior belief that the proportion is roughly uniform between 0 and 1, the posterior distribution is updated using the observed data:\n\\[\n\\theta | \\text{data} \\sim \\text{Beta}(2 + 600, 2 + 400) = \\text{Beta}(602, 402)\n\\]\nFrom this posterior, the policymaker can derive different point estimates:\n\nPosterior Mean: Given a \\(\\text{Beta}(a, b)\\) distribution, the mean is $ $, which in this case is:\n\n\\[\n\\frac{602}{602 + 402} = 0.60\n\\]\nThis suggests that, on average, the Bayesian model estimates 60% of the population supports the initiative.\n\nPosterior Median: This is the value that splits the posterior probability into two equal halves. For a Beta distribution, the median can be approximated numerically, where we can write the median approximately\n\n\\[\n\\frac{a-1/3}{a+b-2/3} = \\frac{602-1/3}{602+402-2/3} \\approx 0.6\n\\] and in this case, it is close to 0.6.\n\nMaximum a Posteriori (MAP): The MAP estimate is the mode of the Beta distribution, which for \\(\\text{Beta}(\\alpha, \\beta)\\) is:\n\n\\[\n\\frac{a - 1}{a + b - 2} = \\frac{601}{1000} = 0.601\n\\]\nWhile close to the posterior mean, the MAP estimate can be problematic in other cases, particularly with skewed distributions, since it focuses only on the density’s peak rather than the overall probability mass.\n\nCredible Interval: To express uncertainty, the policymaker can compute a 95% credible interval, which provides a range where the true proportion likely falls. For the \\(\\text{Beta}(602, 402)\\) distribution, the central 95% credible interval (obtained numerically) is approximately \\((0.576, 0.623)\\).\n\nThis means that, given the data and prior, there is a 95% probability that the true proportion of public support lies between 57.6% and 62.3%. This interval gives a clearer sense of uncertainty than a single point estimate and helps policymakers make informed decisions while considering potential variations in public opinion.\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\n\na &lt;- 602  \nb &lt;- 402  \n\ntheta_vals &lt;- seq(0, 1, length.out = 1000)\nposterior_density &lt;- dbeta(theta_vals, a, b)\n\nposterior_mean &lt;- a / (a + b)\nposterior_median &lt;- qbeta(0.5, a, b)\nposterior_map &lt;- (a - 1) / (a + b - 2)\ncredible_interval &lt;- qbeta(c(0.025, 0.975), a, b)\n\nposterior_df &lt;- data.frame(theta = theta_vals, density = posterior_density)\n\np = ggplot(posterior_df, aes(x = theta, y = density)) +\n  geom_line(color = \"blue\", size = 1) +\n  \n  geom_vline(xintercept = posterior_mean, color = \"red\", linetype = \"dashed\", size = 1) +\n  \n  geom_vline(xintercept = posterior_median, color = \"green\", linetype = \"dotted\", size = 1) +\n  \n  geom_vline(xintercept = posterior_map, color = \"purple\", linetype = \"dotdash\", size = 1) +\n  \n  geom_ribbon(aes(ymin = 0, ymax = density), \n              data = subset(posterior_df, theta &gt;= credible_interval[1] & theta &lt;= credible_interval[2]),\n              fill = \"gray\", alpha = 0.3) +\n  \n  geom_vline(xintercept = credible_interval[1], color = \"gray\", linetype = \"solid\", size = 1) +\n  geom_vline(xintercept = credible_interval[2], color = \"gray\", linetype = \"solid\", size = 1) +\n  \n  labs(title = \"Posterior Distribution of θ\",\n       x = \"θ (Proportion)\",\n       y = \"Density\",\n       caption = \"Red: Mean, Green: Median, Purple: MAP, Gray: 95% Credible Interval\") +\n  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank())\nlibrary(plotly)\nggplotly(p)\n\n\n\n\n\n\n\n\nExact Inference\n\n\nThe exact inference in Bayesian modelling refers to the precise calculation of posteriors. It involves the direct calculation of the posterior distribution by applying Bayes’ theorem considering the given prior distribution and data likelihood.\nWe have already seen from the Bernoullie distribution example that, choosing a particular type of prior distribution results in a posterior that belongs to the same distribution family. This property that posterior distribution has the same functional form as the prior distribution is called conjugacy. Priors that have this feature are called conjugate priors. In the Bernoulli model example, the conjugate nature of the Beta prior ensures that the posterior distribution remains a Beta distribution, which provides analytical tractability.\nBelow we present some common pairs of likelihoods and priors related to conjugacy:\n\n\n\n\n\n\n\n\nLikelihood\nConjugate Prior\nPosterior Distribution\n\n\n\n\n\\(\\text{Bernoulli}\\)\n\\(\\text{Beta}\\)\n\\(\\text{Beta}\\)\n\n\n\\(\\text{Poisson}\\)\n\\(\\text{Gamma}\\)\n\\(\\text{Gamma}\\)\n\n\n\\(\\text{Normal}\\) (Known variance)\n\\(\\text{Normal}\\)\n\\(\\text{Normal}\\)\n\n\n\\(\\text{Normal}\\) (Unknown variance)\n\\(\\text{Normal-Gamma}\\)\n\\(\\text{Normal-Gamma}\\)\n\n\n\n\nThe method is computationally feasible only when the models involved are simple or when they allow for analytical solutions. Bayesian exact inference can be challenging with complex models. Furthermore, if we opt to use a non-conjugate prior, the posterior distribution cannot be easily simplified into a common distribution form and derivation of marginal likelihood will be challenging, if we want to.\nIn this case, to obtain the posterior distribution we need numerical methods or approximations, as analytical solutions are no longer straightforward or feasible. This complexity of the calculations requires the use of advanced computational techniques such as Markov chain Monte Carlo (MCMC) techniques. We will discuss more on the solutions for these types of situations in our next lecture.",
    "crumbs": [
      "Week 3: **Prior and Posterior**"
    ]
  },
  {
    "objectID": "M02_1.html#more-insights-into-prior",
    "href": "M02_1.html#more-insights-into-prior",
    "title": "Week 3: Prior and Posterior",
    "section": "More Insights into Prior",
    "text": "More Insights into Prior\n\nUnderstanding the appropriate selection of prior distributions in modelling is crucial. Prior distributions can be categorised as informative, non-informative, weakly informative, or based on expert judgment. This section explores into these various types of prior distributions, examining their applicability and relevance.\n\nInformative Prior\n\n\n\nAn informative prior is a type of prior distribution that is based on information that provides significant idea about the parameters being estimated. Since informative priors reflect more certainty, they tend to be more concentrated around a certain value. The informative prior has a strong influence on the posterior distribution, especially when the data is limited or noisy. In cases with substantial data, the posterior may still be mostly driven by the observed data, but the prior will still play a role in shaping the final outcome.\nTo explain it more, suppose, we want to know the efficacy rate of a certain vaccine in patients with similar profiles. Let’s consider that the efficacy rate of the vaccine range from 70% to 90%, which we know from literature. Now from a pilot study we observe 16 successes out of 20 trials, i.e., the efficacy rate is still 80%. Hence, we can easily get the posterior distribution of the efficacy rate from \\(\\text{Beta}(24, 6)\\) distribution, if we consider the prior average success rate of 80%, which is calculated from the range 70% to 90%. Numerically, we can plot the prior and posterior distributions as:\n\n\nCode\nlibrary(ggplot2)\na_prior &lt;- 8\nb_prior &lt;- 2\nn &lt;- 20  \ny &lt;- 16  \na_post &lt;- a_prior + y\nb_post &lt;- b_prior + (n - y)\npr_values &lt;- seq(0, 1, length.out = 1000)  \nprior &lt;- dbeta(pr_values, a_prior, b_prior) \nposterior &lt;- dbeta(pr_values, a_post, b_post)\ndata &lt;- data.frame(\n  p = rep(pr_values, 2),\n  density = c(prior, posterior),\n  Distribution = rep(c(\"Prior\", \"Posterior\"), each = length(pr_values))\n)\nggplot(data, aes(x = p, y = density, color = Distribution)) +\n  geom_line(size = 1.2) +\n  labs(\n    title = \"Prior [Beta(8,2)], i.e., 80% vs Posterior Distributions [Beta(24,6)]\",\n    x = \"Efficacy Rate\",\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Prior\" = \"blue\", \"Posterior\" = \"red\"))\n\n\n\n\n\n\n\n\n\nCode\n#\na_prior &lt;- 9\nb_prior &lt;- 4\nn &lt;- 20  \ny &lt;- 16  \na_post &lt;- a_prior + y\nb_post &lt;- b_prior + (n - y)\npr_values &lt;- seq(0, 1, length.out = 1000)  \nprior &lt;- dbeta(pr_values, a_prior, b_prior) \nposterior &lt;- dbeta(pr_values, a_post, b_post)\ndata &lt;- data.frame(\n  p = rep(pr_values, 2),\n  density = c(prior, posterior),\n  Distribution = rep(c(\"Prior\", \"Posterior\"), each = length(pr_values))\n)\nggplot(data, aes(x = p, y = density, color = Distribution)) +\n  geom_line(size = 1.2) +\n  labs(\n    title = \"Prior [Beta(9,4)], i.e., 70% vs Posterior Distributions [Beta(25,8)]\",\n    x = \"Efficacy Rate\",\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Prior\" = \"blue\", \"Posterior\" = \"red\"))\n\n\n\n\n\n\n\n\n\nCode\n#\na_prior &lt;- 10\nb_prior &lt;- 1\nn &lt;- 20  \ny &lt;- 16  \na_post &lt;- a_prior + y\nb_post &lt;- b_prior + (n - y)\npr_values &lt;- seq(0, 1, length.out = 1000)  \nprior &lt;- dbeta(pr_values, a_prior, b_prior) \nposterior &lt;- dbeta(pr_values, a_post, b_post)\ndata &lt;- data.frame(\n  p = rep(pr_values, 2),\n  density = c(prior, posterior),\n  Distribution = rep(c(\"Prior\", \"Posterior\"), each = length(pr_values))\n)\nggplot(data, aes(x = p, y = density, color = Distribution)) +\n  geom_line(size = 1.2) +\n  labs(\n    title = \"Prior [Beta(10,1)], i.e., 90% vs Posterior Distributions [Beta(26,5)]\",\n    x = \"Efficacy Rate\",\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"Prior\" = \"blue\", \"Posterior\" = \"red\"))\n\n\n\n\n\n\n\n\n\nThe informative prior in this example allows you to start with a reasonable expectation (from historical data) and update it with observed results. This is especially useful when the sample size is small or the observed data alone might not be robust enough to guide decision-making.\n\n\n\nNon-informative Prior\n\n\n\nA non-informative prior (also known as an uninformative prior or objective prior or diffuse prior) is a type of prior distribution used in Bayesian statistics that is intended to have minimal influence on the posterior distribution. It reflects a lack of specific prior knowledge about the parameter of interest, allowing the data to drive the inference as much as possible.\nNon-informative priors are often used when the goal is to let the observed data dominate the analysis. These are useful in cases where objectivity is critical or where prior knowledge is genuinely unavailable, but they may not always be appropriate when prior information exists.\nNon-informative priors often reflect a high degree of uncertainty about the parameter’s value, and might end up with parameter estimates similar to the estimates obtained from frequentist approach.\nBefore going into details, let’s explain some prior distribution concepts:\nImproper Priors: Prior distributions that do not integrate to 1 and therefore are not valid probability distributions but can still be useful for inference if they lead to proper posterior distributions. For instance, \\(p(\\theta) \\propto 1/\\theta\\) for scale parameters.\nFlat Priors: Priors that are constant over the range of the parameter (often used for parameters with bounded support).\nNow, we will discuss some common approaches to defining non-informative priors:\n\nUniform Priors:\n\nUniform priors assign equal probability across all possible values of a parameter, assuming no preference for any particular value (e.g., \\(p(\\theta) \\propto 1\\), where for a probability parameter \\(\\theta\\) in a Bernoulli model, a uniform prior on \\(\\text{Unif}[0, 1]\\) implies no prior belief about the likelihood of success. From the plot below we cannot visualize the likelihood function (scaled), as the posterior distribution and likelihood functions are same.\n\n\nCode\nlibrary(ggplot2)\nset.seed(123)  \nn &lt;- 100       \ntrue_theta &lt;- 0.8  \ndata &lt;- rbinom(n, size = 1, prob = true_theta)  \nsuccesses &lt;- sum(data)  \nfailures &lt;- n - successes\n# Uniform prior: P(theta) ∝ 1 on [0, 1]\n# The uniform prior is equivalent to Beta(1, 1).\na_prior &lt;- 1  \nb_prior &lt;- 1  \na_post &lt;- a_prior + successes\nb_post &lt;- b_prior + failures\ntheta_vals &lt;- seq(0, 1, length.out = 1000)\nlikelihood &lt;- dbinom(successes, size = n, prob = theta_vals)\ntheta_vals &lt;- seq(0, 1, length.out = 1000)\nprior_density &lt;- dbeta(theta_vals, a_prior, b_prior)\nposterior_density &lt;- dbeta(theta_vals, a_post, b_post)\n# likelihood - scaled\nlikelihood_scaled &lt;- likelihood / max(likelihood) * max(posterior_density)\nplot_data &lt;- data.frame(\n  theta = rep(theta_vals, 3),\n  density = c(prior_density, posterior_density, likelihood_scaled),\n  type = rep(c(\"Prior (Uniform)\", \"Posterior\", \"Likelihood (Scaled)\"), each = length(theta_vals))\n)\nggplot(plot_data, aes(x = theta, y = density, color = type)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Prior, Likelihood, and Posterior Distributions\",\n    x = expression(theta),\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"green\",\"red\",\"blue\"))\n\n\n\n\n\n\n\n\n\n\nJeffreys’ Priors:\n\nJeffreys’ prior is a non-informative prior derived based on the Fisher information matrix, ensuring invariance under reparameterisation. This prior is proportional to the square root of the determinant of the Fisher information: \\(p(\\theta) \\propto \\sqrt{|I(\\theta)|}\\), where \\(I(\\theta)\\) is the Fisher information.\nJeffreys’ prior is invariant under reparameterisation means that if you change the parameterization of a model (i.e., if you make a transformation of the parameters), the form of the Jeffreys’ prior does not change.\nBinomial distribution\nLet us now explain this using Binomial distribution, where we observe 16 successes out of 20 trials, i.e., the efficacy rate is 80% observed from a pilot survey.\nNow, we write the Fisher information matrix as: \\(I(\\theta)=\\frac{n}{\\theta(1-\\theta)}\\). Hence, we get Jeffreys prior for \\(\\theta\\) as:\n\\[\np(\\theta) \\propto \\sqrt{\\left| \\frac{n}{\\theta(1-\\theta)}\\right|} \\propto \\frac{1}{\\sqrt{\\theta(1-\\theta)}}\n\\]\nwhere, we can ignore \\(\\sqrt{n}\\) using the proportional sign, as it is free from the model parameter \\(\\theta\\). Hence, we can plot the distributions as:\n\n\nCode\nlibrary(ggplot2)\njeffreys_prior &lt;- function(theta) {\n  return(1 / sqrt(theta * (1 - theta)))  \n}\nlikelihood &lt;- function(theta, k, n) {\n  return(choose(n, k) * theta^k * (1 - theta)^(n - k))  \n}\nn &lt;- 20  \nk &lt;- 16 \ntheta_vals &lt;- seq(0.01, 0.99, length.out = 1000)  \nprior_density &lt;- jeffreys_prior(theta_vals)\nlikelihood_density &lt;- likelihood(theta_vals, k, n)\nposterior_density &lt;- likelihood_density * prior_density\nprior_density &lt;- prior_density / sum(prior_density)\nlikelihood_density &lt;- likelihood_density / sum(likelihood_density)\nposterior_density &lt;- posterior_density / sum(posterior_density)\nplot_data &lt;- data.frame(\n  theta = rep(theta_vals, 3),\n  density = c(prior_density, likelihood_density, posterior_density),\n  type = rep(c(\"Prior (Jeffreys)\", \"Likelihood\", \"Posterior\"), each = length(theta_vals))\n)\nggplot(plot_data, aes(x = theta, y = density, color = type)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Jeffreys Prior, Likelihood, and Posterior for Binomial Model\",\n    x = expression(theta),\n    y = \"Scaled Density\",\n    color = \"Distribution\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"green\", \"red\", \"blue\"))\n\n\n\n\n\n\n\n\n\nNormal Distribution\nLet’s consider a normal distribution with known variance \\(\\sigma^2\\). We write the Fisher information for the mean parameter \\(\\mu\\) as \\(I(\\mu) =\\frac{n}{\\sigma^2}\\), where \\(n\\) is the sample size. Hence, we write the Jeffreys prior for \\(\\mu\\) as the proportional to the square root of the Fisher information, i.e.,\n\\[\np(\\mu) \\propto \\sqrt{|I(\\mu)|} = \\sqrt{\\frac{n}{\\sigma^2}}\n\\]\nSince, \\(\\sigma^2\\) is a constant, hence for the mean of a normal distribution, it’s constant, i.e., Jeffreys prior is \\(p(\\mu) \\propto 1\\), and \\(-\\infty \\le \\mu \\le \\infty\\).\nThus, the prior is uniform over the parameter space. We write the posterior for \\(\\mu\\) follows normal distribution with mean \\(\\bar{y}\\) (sample mean) and variance \\(\\sigma^2/n\\).\nNow let us explain with an eample. Suppose a new antihypertensive drug is tested in a clinical trial, and researchers measure the reduction in systolic blood pressure (in mmHg) among patients. Where, most patients experience a blood pressure reduction of around 5 mmHg, but there is natural variability, which is about 2 standard deviation. Now considering Jeffreys non-informative prior, we can get the posterior distribution of the systolic blood pressure.\nFollowing this we draw the density plots using R code as follows:\n\n\nCode\nlibrary(ggplot2)\nset.seed(123)  \nn &lt;- 100        \ntrue_mu &lt;- 5    \nsigma &lt;- 2      \ndata &lt;- rnorm(n, mean = true_mu, sd = sigma)  \n# Fisher information for the mean is: I(mu) = n / sigma^2\nfisher_info &lt;- n / sigma^2 \n# Jeffreys prior is proportional to sqrt(I(mu)). For the mean of a normal distribution, it's constant, i.e., uniform over the parameter space\njeffreys_prior &lt;- function(mu) {\n  return(rep(1, length(mu)))  \n}\nsample_mean &lt;- mean(data)\nposterior_mean &lt;- sample_mean\nposterior_sd &lt;- sigma / sqrt(n)\n\nlikelihood &lt;- function(mu) {\n  return(dnorm(mu, mean = sample_mean, sd = sigma / sqrt(n)))\n}\n\nmu_vals &lt;- seq(true_mu - 3 * sigma, true_mu + 3 * sigma, length.out = 1000)\n\nprior_density &lt;- jeffreys_prior(mu_vals)\nlikelihood_density &lt;- likelihood(mu_vals)\nposterior_density &lt;- dnorm(mu_vals, mean = posterior_mean, sd = posterior_sd)\n\nlikelihood_density &lt;- likelihood_density / max(likelihood_density) * max(posterior_density)\n\nplot_data &lt;- data.frame(\n  mu = rep(mu_vals, 3),\n  density = c(prior_density, likelihood_density, posterior_density),\n  type = rep(c(\"Prior (Jeffreys)\", \"Likelihood (Scaled)\", \"Posterior\"), each = length(mu_vals))\n)\nggplot(plot_data, aes(x = mu, y = density, color = type)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Jeffreys Prior, Likelihood, and Posterior for Normal Model\",\n    x = expression(mu),\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"green\", \"red\", \"blue\"))\n\n\n\n\n\n\n\n\n\nFurther Notes\nDespite being called “non-informative,” their choice can still involve subjective decisions. Some priors, like uniform priors, may appear non-informative in one parameterisation but informative in another (e.g., uniform in \\(\\theta\\) vs. \\(\\log(\\theta)\\)).\nTo explain this, consider a the case where we defined a uniform prior for \\(\\theta\\), i.e., \\(p(\\theta) = \\text{constant}\\), implies that all values of \\(\\theta\\) are equally likely. Now, suppose we reparameterise the problem using a new variable, say logit transformation:\n\\[\n\\phi = \\log \\frac{\\theta}{1 - \\theta}\n\\]\nIf \\(\\theta\\) follows a \\(\\text{Unif}[0, 1]\\) prior, what does this imply about \\(\\phi\\)? Using the change of variables formula for probability densities:\n\\[\np(\\phi) = p(\\theta) \\left| \\frac{d\\theta}{d\\phi} \\right|\n\\]\nSince \\(\\theta = \\frac{e^\\phi}{1 + e^\\phi}\\), its derivative is:\n\\[\n\\frac{d\\theta}{d\\phi} = \\frac{e^\\phi}{(1 + e^\\phi)^2}\n\\]\nSubstituting this into the density transformation:\n\\[\np(\\phi) \\propto \\frac{\\exp(\\phi)}{(1 + \\exp(\\phi))^2}\n\\]\nwhich is the logistic distribution rather than a uniform distribution. This shows that a uniform prior on \\(\\theta\\) induces a highly structured prior on \\(\\phi\\), meaning that the prior is no longer flat in the transformed space. We will learning more about this in hierarchical modelling.\nThis concept is crucial in Bayesian statistics, as it shows that non-informative priors are not always truly non-informative; their informativeness depends on the chosen parameter space.\n\n\nWeakly Informative Prior\n\n\n\nA weakly informative prior distribution in Bayesian statistics is characterised as proper, yet it is designed to provide information that is intentionally less robust than the actual prior knowledge available.\nLet’s explain this with the example of efficacy rate of the vaccine. If we consider a \\(\\text{Beta}(1,1)\\) prior for the parameter \\(\\theta\\), then we have already discussed that the prior is a flat line, which represents a non-informative situation. Now, considering a \\(\\text{Beta}(0.5,0.5)\\) prior might lead to a distribution similar to Jefferys’ prior. Whereas, if we consider a \\(\\text{Beta}(2,2)\\) prior, then it favours a middle value (0.5) but still flexible. This prior avoids extreme values (near 0 or 1) unless strongly supported by the data. Below, we can plot all these three different priors.\n\n\nCode\nlibrary(ggplot2)\nlibrary(viridis)\ntheta_vals &lt;- seq(0, 1, length.out = 1000)\nbeta_1_1 &lt;- dbeta(theta_vals, 1, 1)    # Uniform prior\nbeta_0_5_0_5 &lt;- dbeta(theta_vals, 0.5, 0.5)  # Jeffreys' prior\nbeta_2_2 &lt;- dbeta(theta_vals, 2, 2)    # Weakly informative prior\nplot_data &lt;- data.frame(\n  theta = rep(theta_vals, 3),\n  density = c(beta_1_1, beta_0_5_0_5, beta_2_2),\n  type = rep(c(\"Beta(1,1) - Uniform\", \"Beta(0.5,0.5) - Jeffreys\", \"Beta(2,2) - Weakly Informative\"), each = length(theta_vals))\n)\nggplot(plot_data, aes(x = theta, y = density, color = type)) +\n  geom_line(size = 1) +\n  labs(title = \"Comparison of Different Beta Priors\",\n       x = expression(theta), \n       y = \"Density\", \n       color = \"Distribution\") +\n  ylim(0,3) +\n  theme_minimal() +\n  scale_color_viridis_d(option = \"cvidis\")\n\n\n\n\n\n\n\n\n\nNow assume that the success rate of the vaccine is about 50%, and we can consider the prior distribution as \\(\\text{Beta}(2,2)\\). This symmetric pattern favours the efficacy rate around 0.5 but also allows a wide range of plausible values. Now, suppose 30 trials out of \\(n = 50\\) shows success. Hence, we get the posterior distribution as \\(\\text{Beta}(32,22)\\). Below you can see the density plots of the distributions.\n\n\nCode\nlibrary(ggplot2)\na_prior &lt;- 2\nb_prior &lt;- 2\nn &lt;- 50\ny &lt;- 30\na_post &lt;- a_prior + y\nb_post &lt;- b_prior + (n - y)\np &lt;- seq(0, 1, length.out = 1000)\n\nprior &lt;- dbeta(p, a_prior, b_prior)\n# Scaled for visualisation\nlikelihood &lt;- dbinom(y, n, p) * 100  \nposterior &lt;- dbeta(p, a_post, b_post)\n\nplot_data &lt;- data.frame(\n  p = p,\n  Likelihood = likelihood,\n  Posterior = posterior,\n  Prior = prior\n)\ndata_long &lt;- reshape2::melt(plot_data, id = \"p\", variable.name = \"Distribution\", value.name = \"Density\")\nggplot(data_long, aes(x = p, y = Density, color = Distribution)) +\n  geom_line(size = 1) +\n  labs(\n    title = \"Prior (weakly informative), Likelihood, and Posterior Distributions\",\n    x = expression(theta),\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"green\", \"red\", \"blue\"))\n\n\n\n\n\n\n\n\n\nWeakly informative priors are common in modern Bayesian modelling, where it balances interpretability, robustness, and computational efficiency. Even though weakly informative priors are designed to be robust, they still influence the posterior, especially in small-sample scenarios. What counts as weakly informative is context-dependent. For example, a \\(\\text{Normal}(0,10^2)\\) prior on a regression coefficient might be weakly informative in a standard model but too weak in a context where coefficients are typically small. We will explain more about the weakly informative prior when we will learn the Bayesian regression and hierarchical models.",
    "crumbs": [
      "Week 3: **Prior and Posterior**"
    ]
  },
  {
    "objectID": "M02_1.html#summary",
    "href": "M02_1.html#summary",
    "title": "Week 3: Prior and Posterior",
    "section": "Summary",
    "text": "Summary\nToday’s lecture focused on understanding different types of prior distributions and their role in deriving the posterior distribution. Below, we provide a javascript output to facilitate user-friendly sensitivity analysis of beta-binomial distribution.",
    "crumbs": [
      "Week 3: **Prior and Posterior**"
    ]
  },
  {
    "objectID": "M02_1.html#live-tutorial-and-discussion",
    "href": "M02_1.html#live-tutorial-and-discussion",
    "title": "Week 3: Prior and Posterior",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 3: **Prior and Posterior**"
    ]
  },
  {
    "objectID": "M02_1.html#tutorial-exercises",
    "href": "M02_1.html#tutorial-exercises",
    "title": "Week 3: Prior and Posterior",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.\n\n\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis (3rd Edition). Chapman; Hall/CRC.\n\n\nLambert, Ben. 2018. A Student’s Guide to Bayesian Statistics. SAGE Publications Ltd.",
    "crumbs": [
      "Week 3: **Prior and Posterior**"
    ]
  },
  {
    "objectID": "M02_2.html",
    "href": "M02_2.html",
    "title": "Week 4: Generative Models and Tools",
    "section": "",
    "text": "Learnings\nL03: Explain how these generative models can be used for inference, prediction and model criticism.\nL04: Demonstrate proficiency in using statistical software packages (R) to specify and fit models, assess model fit, detect and remediate non-convergence, and compare models.\nBy the end of this week you should be able to:\n– Create generative models\n– Understand traceable and untraceable solutions\n– Explain the convergence of MCMC.\n– Conduct prior predictive check.\n– Conduct posterior predictive check.",
    "crumbs": [
      "Week 4: **Generative Models and Tools**"
    ]
  },
  {
    "objectID": "M02_2.html#learnings",
    "href": "M02_2.html#learnings",
    "title": "Week 4: Generative Models and Tools",
    "section": "",
    "text": "Outcomes\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 4: **Generative Models and Tools**"
    ]
  },
  {
    "objectID": "M02_2.html#generative-models",
    "href": "M02_2.html#generative-models",
    "title": "Week 4: Generative Models and Tools",
    "section": "Generative Models",
    "text": "Generative Models\n\n\nA generative model is designed to generate new data points by capturing the intricate probability distributions of existing datasets. By learning these distributions, the model can produce data that reflects the characteristics of original datasets, simulating realistic examples. A generative model can also be used to understand how a set of observed data could have arisen from a set of underlying causes, which we will discuss more later in this course.\nIn Bayesian modelling, generative models are particularly useful as they provide a framework for estimating the likelihood of data under different hypotheses. This capability enhances Bayesian inference processes, allowing for more effective prior and posterior distribution updates. The flexibility of generative models in simulating various scenarios can also improve the robustness and accuracy of Bayesian models, and refines the decision-making and predictive capabilities.\nNote that, in this course, we use “generative model” broadly to consider the origins of a particular dataset. However, this term also has a more specific definition, especially as it contrasts with “discriminative models”, for details see Bernardo et al. (2007).\n\nExample\nLet’s explain this using the example we discussed earlier related to the vaccine efficacy rate. Let use assume that we know the true vaccine effectiveness rate, which is \\(0.8\\), and given this information we can recreate the data.\n\nData generation:\n\nIn a generative modelling context, we simulate data say for 100 individuals by considering success probability \\(\\theta = 0.8\\).\n\n\nCode\nn &lt;- 100\ntheta &lt;- 0.8  \nsize &lt;- 1  \nset.seed(1234)  \ndata &lt;- rbinom(size, n, theta)\npaste(\"Number of success: \",data,\" out of \",n, \"individuals\")\n\n\n[1] \"Number of success:  85  out of  100 individuals\"\n\n\nWe can see that our simulation using one replication yields probability 0.85, whereas, actual data shows \\(\\theta = 0.8\\). Hence, to reflect actual data we need to replicate the data simulation for multiple times, which yields an average value for \\(\\theta \\approx 0.8\\) and upper and lower 95% credible interval (0.72,0.88). This is very simple example of how we can generate data when the model parameter is known. We will utilise this concept later in this course to simulate data and use Bayesian models to provide results based on disitions.\n\n\nCode\nlibrary(ggplot2)\n\nn &lt;- 100     \ntheta &lt;- 0.8    \nsize &lt;- 1000  \n\nset.seed(123)  \ndata &lt;- rbinom(size, n, theta)\nci &lt;- quantile(data, probs = c(0.025, 0.975))\ndf &lt;- data.frame(Successes = data)\np = ggplot(df, aes(x = Successes)) +\n  geom_histogram(\n    breaks = seq(-0.5, n + 0.5, by = 1),\n    fill = \"skyblue\",\n    color = \"black\",\n    boundary = -0.5\n  ) +\n  scale_x_continuous(\n    breaks = NULL,  \n    name = NULL     \n  ) +\n  labs(\n    title = paste(\"Histogram of Simulated Binomial Data (n =\", n, \", θ =\", theta, \")\"),\n    y = \"Frequency\"\n  ) +\n  geom_vline(xintercept = ci[1], linetype = \"dashed\", color = \"red\", size = 1) +\n  geom_vline(xintercept = ci[2], linetype = \"dashed\", color = \"red\", size = 1) +\n  annotate(\"text\", x = ci[1], y = max(table(data)) * 0.9, label = paste0(\"2.5%: \", ci[1]), color = \"red\", angle = 90, vjust = -0.5) +\n  annotate(\"text\", x = ci[2], y = max(table(data)) * 0.9, label = paste0(\"97.5%: \", ci[2]), color = \"red\", angle = 90, vjust = -0.5) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_blank(),  \n    axis.ticks.x = element_blank()  \n  )\nlibrary(plotly)\nggplotly(p)",
    "crumbs": [
      "Week 4: **Generative Models and Tools**"
    ]
  },
  {
    "objectID": "M02_2.html#solving-untraceability",
    "href": "M02_2.html#solving-untraceability",
    "title": "Week 4: Generative Models and Tools",
    "section": "Solving Untraceability",
    "text": "Solving Untraceability\n\n\nWe can indentify an untraceable solution in Bayesian inference, where the posterior distribution cannot be expressed in a closed-form expression. This phenomenon occurs when the likelihood function and the prior distribution are incompatible, which results in complex integrals within Bayes’ theorem that are analytically intractable.\nThe opposite of untraceable solution is known as the traceable solution, which we have discussed in our previous lectures in the light of exact Bayesian inference. We already know that exact Bayesian inference means computing the posterior analytically, without approximations. And a tractable solution is one that can be computed exactly and efficiently, without requiring numerical approximations or complex sampling methods like Markov Chain Monte Carlo (MCMC). Thus traceable solution can be obtained when the prior distribution and the likelihood function are chosen such that the posterior distribution remains within a recognised family of probability distributions. For example, when a conjugate prior is used, the posterior has a known or a same functional form as the prior, i.e., for a Bernoulli model, that we have already discussed earlier in Lecture 2: \\(\\text{Prior: } \\theta \\sim \\text{Beta}(a,b)\\) and \\(\\text{Posterior: } \\theta|y \\sim \\text{Beta}(a+y,b+n-y)\\).\nUse of non-conjugate priors and often complex likelihood functions, such as those involving hierarchical models, mixture models, or high-dimensional data yields a posterior form of distribution with untraceable solutions, even if we consider conjugate priors for the model parameters.\nFor a non-Gaussian likelihood (i.e., it does not follow a normal distribution), choosing a uniform prior leads to a situation where the resulting posterior distribution cannot be easily determined or expressed in a simple mathematical form and also yields untraceable solution.\n\n\nAlgorithms\nDifferent types of sampling algorithms have been developed to tackle untraceable solutions, such as rejection sampling, variational inference, Laplace approximation, sequential Monte Carlo, Markov chain Monte Carlo (MCMC) etc. In this couse, we will learn how to use and implement the MCMC algorithms to solve real-life problems. We refer Gelman et al. (2013) for details on this for those interested to explore more.",
    "crumbs": [
      "Week 4: **Generative Models and Tools**"
    ]
  },
  {
    "objectID": "M02_2.html#markov-chain-monte-carlo-mcmc",
    "href": "M02_2.html#markov-chain-monte-carlo-mcmc",
    "title": "Week 4: Generative Models and Tools",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\n\n\nIn this course, we will focus on learning Markov chain Monte Carlo (MCMC) algorithm, which is a sampling based approach to obtain the posterior distribution.\nA Markov chain is a stochastic process where the next state depends only on the current state, not on the sequence of states that preceded it. This property is called the Markovian or Markov property. The transition between states is defined by a transition probability matrix or kernel. On the other hand, Monte Carlo methods involve random sampling to estimate numerical quantities, such as integrals or expectations, to approximate the final solution. MCMC combines the Monte Carlo with Markov chains to generate samples.\nBasic concept and generic structure for MCMC can be explained as follows. Say we are interested in parameter \\(\\theta\\), then we state:\n\nSelect \\(\\theta^{(0)}\\) at an arbitrary point\nAt iteration \\(t\\), sample from a transition distribution \\(\\theta^{(t)}|\\theta^{(t-1)}\\), i.e., to generate a new value \\(\\theta = \\theta^{(t)}\\) given the previous value \\(\\theta = \\theta^{(t-1)}\\).\nRepeat the previous step until a specified maximum number of iterations is reached, or until specified convergence criterion is satisfied.\n\nHence, the MCMC states, there exist a transition distribution that guarantee that\n\\[\np(\\theta\\in \\{\\theta^{(t)},\\theta^{(t+1)},...,\\theta^{(q)} \\}) \\rightarrow p(\\theta|\\text{data}); \\text{   as   } q\\rightarrow \\infty\n\\]\nThe function that determines the probability for selecting the next location, is called the transition distribution.\n\n\nCode\n#set.seed(42)\n#n_iter &lt;- 10\n#theta &lt;- numeric(n_iter)\n#theta[1] &lt;- 0\n#sigma &lt;- 1\n#for (t in 2:n_iter) {\n#  theta[t] &lt;- rnorm(1, mean = theta[t - 1], sd = sigma)\n#}\n#x_vals &lt;- theta[1:(n_iter - 1)]\n#y_vals &lt;- theta[2:n_iter]\n#plot(x_vals, y_vals, type = \"b\", pch = 19, col = \"blue\",\n#     xlab = expression(theta[t-1]),\n#     ylab = expression(theta[t]),\n#     main = expression(paste(\"Transition plot: \", theta[t-1], \" vs \", theta[t])))\n#for (i in 1:(n_iter - 2)) {\n#  arrows(x_vals[i], y_vals[i], x_vals[i + 1], y_vals[i + 1],\n#         length = 0.1, col = \"darkgreen\", lwd = 2)\n#}\n#text(x_vals, y_vals, labels = paste0(\"t=\", 2:n_iter), pos = 3, cex = 0.8)\n\n#library(ggplot2)\n#library(ggrepel)\n#set.seed(007)\n#n_iter &lt;- 10\n#theta &lt;- numeric(n_iter)\n#theta[1] &lt;- 0\n#sigma &lt;- 1\n#for (t in 2:n_iter) {\n#  theta[t] &lt;- rnorm(1, mean = theta[t - 1], sd = sigma)\n#}\n#df &lt;- data.frame(\n#  x = theta[1:(n_iter - 1)],\n#  y = theta[2:n_iter],\n#  theta_label = round(theta[2:n_iter], 2)\n#)\n#df$arrows_xend &lt;- c(df$x[-1], NA)\n#df$arrows_yend &lt;- c(df$y[-1], NA)\n#df_arrows &lt;- df[1:(nrow(df) - 1), ]\n#p1 &lt;- ggplot(df, aes(x = x, y = y)) +\n#  geom_path(color = \"steelblue\", linewidth = 1, alpha = 0.6) +\n#  geom_segment(data = df_arrows,\n#               aes(xend = arrows_xend, yend = arrows_yend),\n#               arrow = arrow(length = unit(0.3, \"cm\"), type = \"closed\"),\n#               color = \"firebrick\", linewidth = 1.5, linetype = \"dashed\") +\n#  geom_point(color = \"blue\", size = 3) +\n#  geom_text_repel(aes(label = theta_label), size = 3) +\n#  labs(\n#    x = expression(theta[t-1]),\n#    y = expression(theta[t]),\n#    title = expression(paste(\"Transition plot: \", theta[t-1], \" vs \", theta[t]))\n#  ) +\n#  theme_minimal()\n\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(plotly)\nset.seed(007)\nn_iter &lt;- 10\ntheta &lt;- numeric(n_iter)\ntheta[1] &lt;- 0\nsigma &lt;- 1\nfor (t in 2:n_iter) {\n  theta[t] &lt;- rnorm(1, mean = theta[t - 1], sd = sigma)\n}\ndf &lt;- data.frame(\n  x = theta[1:(n_iter - 1)],\n  y = theta[2:n_iter],\n  z = 2:n_iter,\n  theta_label = round(theta[2:n_iter], 2)\n)\np2 &lt;- plot_ly(df, x = ~x, y = ~y, z = ~z, type = 'scatter3d', mode = 'lines+markers+text',\n        line = list(color = 'firebrick', width = 5, dash = 'dash'),\n        marker = list(size = 5, color = 'blue'),\n        text = ~theta_label,\n        textposition = 'top right') %&gt;%\n  layout(\n    scene = list(\n      xaxis = list(title = \"theta[t-1]\"),\n      yaxis = list(title = \"theta[t]\"),\n      zaxis = list(title = \"t\")\n    ),\n    title = \"Transition Plot of θ (in 3D)\"\n  )\np2\n\n\n\n\n\n\nNow, we will discuss some popular and common MCMC methods we use in practice to solve real-life applications.\n\nCommon MCMC methods\n\nSome common MCMC algorithms include Metropolis-Hastings (MH) algorithm, Gibbs sampling, Hamiltonian Monte Carlo (HMC) etc. MH is a general MCMC method that generates candidate samples from a proposal distribution. A candidate is accepted or rejected based on an acceptance probability, ensuring the chain converges to the target distribution. Whereas, Gibbs sampling is a special case of the Metropolis-Hastings algorithm. Updates one variable at a time by sampling from its conditional distribution while keeping other variables fixed. The Hamiltonian Monte Carlo (HMC) algorithm uses gradient information from the target distribution to propose new samples, making it more efficient for high-dimensional problems.\n\n\nMetropolis-Hastings (MH) Algorithm\nThe MH algorithm generates a sequence of samples that gradually approximates a target distribution say \\(p(\\theta)\\). It starts with an arbitrary value say \\(\\theta^{(0)}\\), and then uses a proposal distribution \\(q(\\theta^* | \\theta_t)\\) to propose a new value \\(\\theta^*\\). In the next step it accepts or rejects \\(\\theta^*\\) using an acceptance probability:\n\\[\nA = \\min \\left(1, \\frac{p(\\theta^*) q(\\theta_t | \\theta^*)}{p(\\theta_t) q(\\theta^* | \\theta_t)} \\right)\n\\]\nNow, we generate a random number \\(u\\) (say) from uniform distribution with (0,1), and when \\(u &lt; A\\), we accept the new point. That is, if accepted, then we set \\(\\theta_{t+1} = \\theta^*\\), otherwise keep the current sample, i.e., \\(\\theta_{t+1} = \\theta_t\\). We then iterate this process to generate a sequence of samples, and over time, the samples approximate \\(p(\\theta)\\).\nMetropolis Algorithm\nA simpler version and special case of the MH algorithm is the Metropolis algorithm, where the proposal distribution is symmetric, i.e., \\(q(\\theta^* | \\theta_t) = q(\\theta_t | \\theta^*)\\). Hence, in this case, we write the acceptance probability as:\n\\[\nA = \\min \\left(1, \\frac{p(\\theta^*) }{p(\\theta_t) } \\right)\n\\]\nHere, we can see as the proposal distribution is symmetric and it cancles out.\nExample\nSuppose, we want to sample from a standard normal distribution \\(N(0,1)\\) using the Metropolis-Hastings algorithm, starting from an arbitrary initial point. This allows us to observe how the MCMC chain gradually converges to the target distribution. We write the target density \\(\\theta \\sim N(0,1)\\) as:\n\\[\np(\\theta) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{\\theta^2}{2}\\right)\n\\]\n\n\nCode\nlibrary(coda)\nmetropolis_hastings &lt;- function(target_density, proposal_sd, n_iter, initial_value) {\n  chain &lt;- numeric(n_iter)\n  chain[1] &lt;- initial_value\n  \n  for (i in 2:n_iter) {\n    proposal &lt;- rnorm(1, mean = chain[i - 1], sd = proposal_sd)\n    acceptance_prob &lt;- min(1, target_density(proposal) / target_density(chain[i - 1]))\n    if (runif(1) &lt; acceptance_prob) {\n      chain[i] &lt;- proposal\n    } else {\n      chain[i] &lt;- chain[i - 1]\n    }\n  }\n  \n  return(chain)\n}\ntarget_density &lt;- function(x) dnorm(x, mean = 0, sd = 1)\n\nset.seed(123)\nn_iter &lt;- 10000\nn_chains &lt;- 3\nchains &lt;- list(\n  chain1 = metropolis_hastings(target_density, proposal_sd = 1, n_iter, initial_value = 10),\n  chain2 = metropolis_hastings(target_density, proposal_sd = 1, n_iter, initial_value = -10),\n  chain3 = metropolis_hastings(target_density, proposal_sd = 1, n_iter, initial_value = 5)\n)\n\nmcmc_chains &lt;- mcmc.list(\n  mcmc(chains$chain1),\n  mcmc(chains$chain2),\n  mcmc(chains$chain3)\n)\n\nsummary(mcmc_chains)\n\n\n\nIterations = 1:10000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 10000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n     -0.025039       1.066546       0.006158       0.020382 \n\n2. Quantiles for each variable:\n\n    2.5%      25%      50%      75%    97.5% \n-2.05216 -0.70284 -0.01213  0.65225  1.94265 \n\n\nCode\n#par(mfrow = c(1, 3))\n#for (i in 1:n_chains) {\n#  plot(mcmc_chains[[i]], type = \"l\", col = \"blue\", \n#       main = paste(\"Chain\", i), \n#       xlab = \"Iteration\", ylab = \"Value\",\n#       trace = TRUE, density = FALSE)\n#}\n\nlibrary(lattice)\nxyplot(mcmc_chains, col=c(\"blue\", \"red\", \"green\"), lwd=1.5)\n\n\n\n\n\n\n\n\n\nCode\nmcmc_chains_mh &lt;- mcmc_chains\n\n\n\n\nGibbs Sampling\nThe Gibbs sampling algorithm is a special case of the Metropolis-Hastings (MH) algorithm and is particularly useful when we want to do sampling from high-dimensional joint distributions. Instead of sampling all variables at once, Gibbs sampling updates one variable at a time, conditioning on the others.\nSuppose we have a joint distribution \\(p(\\theta_1, \\theta_2)\\), and the algorithm starts with arbitrary initial values for all variables, i.e., \\(\\theta_1^{(0)}\\) and \\(\\theta_2^{(0)}\\). For each variable then we sample from its conditional distribution, i.e., for \\(\\theta_1\\) we use conditional distribution \\(p(\\theta_1|\\theta_2)\\) and then for \\(\\theta_2\\) we use \\(p(\\theta_2|\\theta_1)\\). This means we sample \\(\\theta_1\\) given all other current values, i.e., in our example this is \\(\\theta_2\\) and then sample \\(\\theta_2\\) given \\(\\theta_1\\). We then repeat the process for many iterations until the samples converge to the target distribution.\nWe write the algorithm for \\(n\\) number of parameters, where we want to sample from a joint distribution over parameters \\(p(\\theta_1, \\theta_2, \\dots, \\theta_n)\\).\nInitialisation by choosing starting values: \\(\\left(\\theta_1^{(0)}, \\theta_2^{(0)}, \\dots, \\theta_n^{(0)}\\right)'\\)\nThen iterate for $t = 1, 2, $, and update each parameter one at a time using its conditional distribution:\n\\[\\begin{align*}\n\\theta_1^{(t)} &\\sim p(\\theta_1 \\mid \\theta_2^{(t-1)}, \\dots, \\theta_n^{(t-1)}) \\\\\n\\theta_2^{(t)} &\\sim p(\\theta_2 \\mid \\theta_1^{(t)}, \\theta_3^{(t-1)}, \\dots, \\theta_n^{(t-1)}) \\\\\n&\\vdots \\\\\n\\theta_n^{(t)} &\\sim p(\\theta_n \\mid \\theta_1^{(t)}, \\dots, \\theta_{n-1}^{(t)})\n\\end{align*}\\]\nHence, the samples approximate the target distribution \\(p(\\theta_1, \\dots, \\theta_n)\\).\nExample\nLet’s use Gibbs Sampling to sample from a bivariate normal distribution where the marginal distributions of each variable are normal, but the two variables are correlated. We’ll then assess the convergence using trace plots, autocorrelation plots, and the Gelman-Rubin diagnostic.\nThe joint density is the bivariate normal distribution:\n\\[\np(\\theta_1,\\theta_2) = \\frac{1}{2\\pi\\sqrt{1-\\rho^2}}\\exp\\left(-\\frac{1}{2(1-\\rho^2)}\\left(\\theta_1^2-2\\rho \\theta_1 \\theta_2 +\\theta_2^2 \\right) \\right)\n\\]\nwhere \\(\\rho\\) is the correlation between \\(\\theta_1\\) and \\(\\theta_2\\).\n\n\nCode\nlibrary(coda)\ngibbs_sampling &lt;- function(n_iter, rho, initial_values) {\n  x &lt;- numeric(n_iter)\n  y &lt;- numeric(n_iter)\n  \n  x[1] &lt;- initial_values[1]\n  y[1] &lt;- initial_values[2]\n  \n  for (i in 2:n_iter) {\n    x[i] &lt;- rnorm(1, mean = rho * y[i - 1], sd = sqrt(1 - rho^2))\n    y[i] &lt;- rnorm(1, mean = rho * x[i], sd = sqrt(1 - rho^2))\n  }\n  \n  return(data.frame(theta_2 = x, theta_1 = y))\n}\n\nset.seed(123)         \nn_iter &lt;- 1000       \nrho &lt;- 0.8           \ninitial_values &lt;- c(0, 0)  \n\nchain1 &lt;- gibbs_sampling(n_iter, rho, c(0, 0))\nchain2 &lt;- gibbs_sampling(n_iter, rho, c(10, 10))\nchain3 &lt;- gibbs_sampling(n_iter, rho, c(-10, -10))\n\nmcmc_chains &lt;- mcmc.list(\n  mcmc(as.matrix(chain1)),\n  mcmc(as.matrix(chain2)),\n  mcmc(as.matrix(chain3))\n)\n\nsummary(mcmc_chains)\n\n\n\nIterations = 1:1000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 1000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean    SD Naive SE Time-series SE\ntheta_2 0.01841 1.026  0.01874        0.03854\ntheta_1 0.01097 1.018  0.01858        0.03912\n\n2. Quantiles for each variable:\n\n          2.5%     25%     50%    75% 97.5%\ntheta_2 -1.943 -0.6345 0.02087 0.6801 1.827\ntheta_1 -1.902 -0.6288 0.01558 0.6646 1.902\n\n\nCode\n#par(mfrow = c(2, 3))\n#for (i in 1:3) {\n#  plot(mcmc_chains[[i]][, \"x\"], type = \"l\", col = \"blue\",\n#       main = paste(\"Chain\", i, \"(x)\"), \n#       xlab = \"Iteration\", ylab = \"Value\",\n#       density = FALSE)\n#  plot(mcmc_chains[[i]][, \"y\"], type = \"l\", col = \"red\",\n#       main = paste(\"Chain\", i, \"(y)\"), \n#       xlab = \"Iteration\", ylab = \"Value\",\n#       density = FALSE)\n#}\n\nlibrary(lattice)\nxyplot(mcmc_chains, col=c(\"blue\", \"red\", \"green\"), lwd=1.5)\n\n\n\n\n\n\n\n\n\nCode\nmcmc_chains_gibbs &lt;- mcmc_chains\n\n\n\n\nHamiltonian Monte Carlo (HMC)\nHamiltonian Monte Carlo (HMC) is a powerful MCMC algorithm that uses information about the gradient of the log-probability density to efficiently sample from complex distributions. HMC is inspired by Hamiltonian mechanics, which describes the motion of objects in a physical system. Here, we use a system in Hamiltonian mechanics and is defined by \\(H(\\theta,p)=L(\\theta)+K(p)\\), where \\(L(\\theta)\\) is the negative log of the target density and \\(K(p)\\) is a kinetic energy, which we usually model using a Gaussian distribution.\nThe sampling steps involves by initialising with a position \\(\\theta_t\\) and sample \\(p_t\\) from a Gaussian distribution. Then simulate Hamiltonian dynamics using gradient of \\(L(\\theta)\\) and hence update the position and momentum iteratively. After the simulation, we propose a new state \\((\\theta^*,p^*)\\) and accept or reject the proposed step using Metropolis criterion:\n\\[\nA = \\min \\left(1, \\frac{\\exp(-H(\\theta^*, p^*))}{\\exp(-H(\\theta_t, p_t))} \\right)\n\\]\nAnd, if accepted, move to \\(\\theta^*\\), and for rejection stay at \\(\\theta_t\\). We then repeat for many iterations to generate samples.\nThere is an adaptive version of HMC that automatically tunes trajectory lengths, which is also known as the No-U-Turn Sampler (NUTS). In this course, we will use Stan compiler, which uses NUTS to obtain posterior distributions of the Bayesian model parameters.\nExample\nBelow, we provide an example of implementing HMC for normal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\), using R with the rstan package, which includes a highly optimised implementation of HMC.\n\n\n\nCode\nlibrary(rstan)\nlibrary(bayesplot)\n\nset.seed(42)\ntrue_mu &lt;- 5.0\ntrue_sigma &lt;- 2.0\nn_samples &lt;- 100\ny &lt;- rnorm(n_samples, mean = true_mu, sd = true_sigma)\n\nhist(y, breaks = 20, col = \"lightblue\", main = \"Observed Data\", xlab = \"y\")\n\n\n\n\n\n\n\n\n\nCode\nstan_code &lt;- \"\ndata {\n  int&lt;lower=0&gt; N;        // Number of observations\n  vector[N] y;           // Observed data\n}\nparameters {\n  real mu;               // Mean\n  real&lt;lower=0&gt; sigma;   // Standard deviation\n}\nmodel {\n  mu ~ normal(0, 10);    // Prior for mu\n  sigma ~ normal(0, 10); // Prior for sigma\n  y ~ normal(mu, sigma); // Likelihood\n}\n\"\nstan_data &lt;- list(\n  N = length(y),\n  y = y\n)\n\nfit &lt;- stan(\n  model_code = stan_code,\n  data = stan_data,\n  iter = 2000,      \n  warmup = 1000,    \n  chains = 3,       \n  seed = 1234         \n)\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.7e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 1:                0.029 seconds (Sampling)\nChain 1:                0.055 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 8e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.045 seconds (Warm-up)\nChain 2:                0.022 seconds (Sampling)\nChain 2:                0.067 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 7e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.026 seconds (Warm-up)\nChain 3:                0.022 seconds (Sampling)\nChain 3:                0.048 seconds (Total)\nChain 3: \n\n\nCode\nprint(fit, pars = c(\"mu\", \"sigma\"))\n\n\nInference for Stan model: anon_model.\n3 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=3000.\n\n      mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat\nmu    5.06       0 0.21 4.65 4.92 5.06 5.20  5.49  2598    1\nsigma 2.11       0 0.16 1.83 2.00 2.10 2.21  2.45  2926    1\n\nSamples were drawn using NUTS(diag_e) at Mon Jun 30 09:43:24 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nCode\n#library(bayesplot)\n#mcmc_trace(fit, pars = c(\"mu\", \"sigma\"))\n#mcmc_areas(fit, pars = c(\"mu\", \"sigma\"))\n#mcmc_acf_bar(fit, pars = c(\"mu\", \"sigma\"))\n\nlibrary(coda)\nposterior_samples &lt;- as.array(fit)\nmcmc_chains &lt;- mcmc.list(\n  mcmc(as.matrix(posterior_samples[,1,1:2])),\n  mcmc(as.matrix(posterior_samples[,2,1:2])),\n  mcmc(as.matrix(posterior_samples[,3,1:2]))\n)\n\nlibrary(lattice)\nxyplot(mcmc_chains, col=c(\"blue\", \"red\", \"green\"), lwd=1.5)\n\n\n\n\n\n\n\n\n\nCode\nmcmc_chains_hmc &lt;- mcmc_chains\n\n\n# use shinystan\n#library(shinystan)\n#launch_shinystan(fit)",
    "crumbs": [
      "Week 4: **Generative Models and Tools**"
    ]
  },
  {
    "objectID": "M02_2.html#mcmc-diagnostics",
    "href": "M02_2.html#mcmc-diagnostics",
    "title": "Week 4: Generative Models and Tools",
    "section": "MCMC Diagnostics",
    "text": "MCMC Diagnostics\n\n\nWhen we run Markov Chain Monte Carlo (MCMC) methods for Bayesian inference, we need to make sure our samples actually represent the true posterior distribution. MCMC doesn’t guarantee good results on its own, so we rely on MCMC diagnostics to check for issues like lack of convergence, autocorrelation, and poor mixing.\nNote that there are two major schools of thought regarding MCMC diagnostics: one prefers running a single chain for a longer number of iterations, while the other prefers running multiple chains for relatively shorter iterations. We also need to understand “burn-in” and “thinning” of the MCMC samples. A burn-in refers to discard early samples to allow the chain to reach the stationary distribution. Typical burn-in might be 10–50% of the chain. The thinning approach is related to reduce autocorrelation. Thinning means keeping only every \\(n\\)th sample from your MCMC chain and discarding the rest. For example, if you thin by 10, you keep sample 10, 20, 30, etc., and discard the rest.\nNow, let’s discuss more on the MCMC diagnostics below:\n\nConvergence\nWe first check whether the MCMC chain has actually converged to the posterior distribution. To check this we rely mainly on visual method: trace plots. If the trace plot looks like a “hairy caterpillar” without trends or long drifts, that’s a good sign. If we see big jumps or slow drifting, we might need a longer burn-in period or better tuning.\nIf we run multiple chains as indicated in the examples above, the we can compare their variance, which is also known as Gelman-Rubin Diagnostic and denote the estimate as \\(\\hat{R}\\). An \\(\\hat{R}\\) near 1 (or less than 1.1) typically indicates convergence, i.e., all chains are settled into the same stationary distribution. It is also important to check the Gelman-Rubin plot, which usually show how \\(\\hat{R}\\) decreases over iterations.\n\n\nAutocorrelation\nWe can also use autocorrelation plots to check how correlated our MCMC samples are with previous ones. Ideally, the correlation should drop off quickly, if it lingers then we may need to adjust our proposal distribution for MH or thinning interval or run the chain for more iterations.\nAnother measurement diagnostic is the effective sample size (ESS). If ESS is low, it means we’re getting fewer independent samples than expected. Increasing the total iterations or improving sampling efficiency (e.g., using Hamiltonian Monte Carlo instead of Metropolis-Hastings) can help to increase the ESS. Usually, ESS &lt; 100 might not be a good indicator, which might lead the posterior estimates unreliable. ESS &gt; 400 is generally considered good, where you can get reasonably accurate estimates of posterior means, variances, and quantiles (like credible intervals).\n\n\nMixing & Efficiency\nFor MCMC algorithms such as Metropolis or Metropolis-Hastings, even if our chain is converging, we want to make sure it’s exploring the full posterior efficiently. Poor mixing shows up when the chain gets stuck in one region for too long before jumping elsewhere. If we notice this, the we can tweak the sampling algorithm to improve mixing.\nFor such algorithms, we aim for an acceptance rate between 20% and 50%. If it’s too low, our proposals might be too aggressive; if it’s too high, they might be too conservative. Advanced algorithms, such as NUTS does not need such coareful considerations.\nHence, in this course, we will mainly use the HMC-NUTS algorithm to obtain posterior distributions from the Bayesian models and check for MCMC diagnostics related to this algorithm.\n\n\nCode for MH\n\n\nCode\n## MH\n#par(mfrow = c(1, 3))\n#for (i in 1:n_chains) {\n#  autocorr.plot(mcmc_chains_mh[[i]], main = #paste(\"Autocorrelation: Chain\", i), lag.max = 50)\n#}\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nextract_acf &lt;- function(chain, lag.max = 40) {\n  acf_values &lt;- acf(chain, plot = FALSE, lag.max = lag.max)\n  data.frame(Lag = acf_values$lag, ACF = acf_values$acf)\n}\nacf_data &lt;- bind_rows(\n  extract_acf(mcmc_chains_mh[[1]]) %&gt;% mutate(Chain = \"Chain 1\"),\n  extract_acf(mcmc_chains_mh[[2]]) %&gt;% mutate(Chain = \"Chain 2\"),\n  extract_acf(mcmc_chains_mh[[3]]) %&gt;% mutate(Chain = \"Chain 3\")\n)\nggplot(acf_data, aes(x = Lag, y = ACF, fill = Chain)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Autocorrelation of MCMC Chains - MH\", x = \"Lag\", y = \"Autocorrelation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\ngelman_diag &lt;- gelman.diag(mcmc_chains_mh)\nprint(gelman_diag)\n\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\n[1,]          1          1\n\n\nCode\ngelman.plot(mcmc_chains_mh)\n\n\n\n\n\n\n\n\n\n\n\nCode for Gibbs\n\n\nCode\n## gibbs\n#par(mfrow = c(2, 3))\n#for (i in 1:3) {\n#  autocorr.plot(mcmc_chains_gibbs[[i]][, \"x\"], main = paste(\"Autocorrelation: Chain\", i, \"(x)\"))\n#  autocorr.plot(mcmc_chains_gibbs[[i]][, \"y\"], main = paste(\"Autocorrelation: Chain\", i, \"(y)\"))\n#}\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nextract_acf &lt;- function(chain, lag.max = 40) {\n  acf_values &lt;- acf(chain, plot = FALSE, lag.max = lag.max)\n  data.frame(Lag = acf_values$lag, ACF = acf_values$acf)\n}\nacf_data &lt;- bind_rows(\n  extract_acf(mcmc_chains_gibbs[[1]][, \"theta_1\"]) %&gt;% mutate(Chain = \"Chain 1 (theta_1)\"),\n  extract_acf(mcmc_chains_gibbs[[1]][, \"theta_2\"]) %&gt;% mutate(Chain = \"Chain 1 (theta_2)\"),\n  extract_acf(mcmc_chains_gibbs[[2]][, \"theta_1\"]) %&gt;% mutate(Chain = \"Chain 2 (theta_1)\"),\n  extract_acf(mcmc_chains_gibbs[[2]][, \"theta_2\"]) %&gt;% mutate(Chain = \"Chain 2 (theta_2)\"),\n  extract_acf(mcmc_chains_gibbs[[3]][, \"theta_1\"]) %&gt;% mutate(Chain = \"Chain 3 (theta_1)\"),\n  extract_acf(mcmc_chains_gibbs[[3]][, \"theta_2\"]) %&gt;% mutate(Chain = \"Chain 3 (theta_2)\")\n)\nggplot(acf_data, aes(x = Lag, y = ACF, fill = Chain)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Autocorrelation of Gibbs Sampling Chains\", x = \"Lag\", y = \"Autocorrelation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nlibrary(coda)\ngelman_diag &lt;- gelman.diag(mcmc_chains_gibbs)\nprint(gelman_diag)\n\n\nPotential scale reduction factors:\n\n        Point est. Upper C.I.\ntheta_2          1       1.02\ntheta_1          1       1.02\n\nMultivariate psrf\n\n1\n\n\nCode\ngelman.plot(mcmc_chains_gibbs)\n\n\n\n\n\n\n\n\n\n\n\nCode for HMC\n\n\nCode\n## HMC\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nextract_acf &lt;- function(chain, lag.max = 40) {\n  acf_values &lt;- acf(chain, plot = FALSE, lag.max = lag.max)\n  data.frame(Lag = acf_values$lag, ACF = acf_values$acf)\n}\nacf_data &lt;- bind_rows(\n  extract_acf(mcmc_chains_hmc[[1]][, 1]) %&gt;% mutate(Chain = \"Chain 1 (Param 1)\"),\n  extract_acf(mcmc_chains_hmc[[1]][, 2]) %&gt;% mutate(Chain = \"Chain 1 (Param 2)\"),\n  extract_acf(mcmc_chains_hmc[[2]][, 1]) %&gt;% mutate(Chain = \"Chain 2 (Param 1)\"),\n  extract_acf(mcmc_chains_hmc[[2]][, 2]) %&gt;% mutate(Chain = \"Chain 2 (Param 2)\"),\n  extract_acf(mcmc_chains_hmc[[3]][, 1]) %&gt;% mutate(Chain = \"Chain 3 (Param 1)\"),\n  extract_acf(mcmc_chains_hmc[[3]][, 2]) %&gt;% mutate(Chain = \"Chain 3 (Param 2)\")\n)\nggplot(acf_data, aes(x = Lag, y = ACF, fill = Chain)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Autocorrelation of HMC MCMC Samples\", x = \"Lag\", y = \"Autocorrelation\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\ngelman_diag &lt;- gelman.diag(mcmc_chains_hmc)\nprint(gelman_diag)\n\n\nPotential scale reduction factors:\n\n      Point est. Upper C.I.\nmu             1       1.00\nsigma          1       1.01\n\nMultivariate psrf\n\n1\n\n\nCode\ngelman.plot(mcmc_chains_hmc)\n\n\n\n\n\n\n\n\n\nCode\n#rhats &lt;- rhat(fit)\n#rhats\n#mcmc_rhat(rhats) + yaxis_text(hjust = 1)\n#ratios_cp &lt;- neff_ratio(fit)\n#print(ratios_cp)\n#mcmc_neff(ratios_cp, size = 2) + yaxis_text(hjust = 1)",
    "crumbs": [
      "Week 4: **Generative Models and Tools**"
    ]
  },
  {
    "objectID": "M02_2.html#prior-and-posterior-predictive-checks",
    "href": "M02_2.html#prior-and-posterior-predictive-checks",
    "title": "Week 4: Generative Models and Tools",
    "section": "Prior and Posterior Predictive Checks",
    "text": "Prior and Posterior Predictive Checks\n\n\n\n\nPrior Predictive Checks\nPrior predictive checks involve simulating data from the model before observing real data to assess whether the chosen prior distribution is reasonable. Then we compare this simulated data with the actual observation, if available. This step helps to ensure that our prior assumptions align with the real-world outcomes.\nPrior predictive check is important in Bayesian simulations. We can use this to understand the influence of prior on possible outcomes. It can also help us to avoid overly informed prior, which might lead to a strong influence on the posterior distribution. For example, if a prior on disease prevalence suggests 90% probability when we know it’s closer to 5%, then the prior is not reasonable. Thus, for implausible simulated data, we can say that the prior is too vague, too strong, or poorly chosen.\n\nSteps to perform prior predictive check\n\nBefore collecting data, choose a prior for the parameter, say \\(\\theta\\), if we are modelling a Bernoulli process, where assume the prior follows Beta distribution, i.e., \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\).\nFor instance, let’s assume:\n\\[\n\\theta \\sim \\text{Beta}(1, 1) \\quad \\text{(Uniform prior, meaning all values are equally likely).}\n\\]\nNow, we generate or simulate data from the prior distribution, i.e., first sample \\(\\theta\\) from the prior and then generate data \\(y\\) using a defined model.\nFor example, if we are modelling the efficacy rate of a certain vaccine in patients with similar profiles, then\n\nSample \\(\\theta^{sim} \\sim \\text{Beta}(1,1)\\).\nSimulate \\(y^{sim} \\sim \\text{Bernoulli}(\\theta^{sim})\\) for \\(n\\) trials.\n\nThis gives us a distribution of possible datasets before seeing real data.\nIn the next step, we compare the simulated data with what we would expect or any available data. This can be done by plotting histograms or summary statistics of the simulated data. We can then check if the range and spread of simulated values make sense.\nFor example, if we assume a Beta(2, 2) prior, we expect \\(\\theta\\) to be centered around 0.5. Whereas, for Beta(1, 1) (i.e., Uniform prior), simulated values will be more spread out. If we assume Beta(100, 5), then most values will be very high (\\(\\theta\\) close to 1).\nNow, if simulated data looks unrealistic, the prior may need adjusting. This step prevents misleading results and improves Bayesian inference quality.\n\n\nCode\nlibrary(ggplot2)\nlibrary(gridExtra)\nprior_predictive_check &lt;- function(alpha, beta, n_trials = 10, n_sim = 1000, observed_successes = NULL) {\n  theta_samples &lt;- rbeta(n_sim, alpha, beta)\n  simulated_successes &lt;- rbinom(n_sim, size = n_trials, prob = theta_samples)\n  df &lt;- data.frame(Theta = theta_samples, Successes = simulated_successes)\n  p1 &lt;- ggplot(df, aes(x = Theta)) +\n    geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n    labs(title = bquote(\"Prior Distribution: Beta(\"~.(alpha)*\",\"~.(beta)~\")\"),\n         x = expression(theta), y = \"Frequency\") +\n    theme_minimal() +\n    xlim(0, 1)\n  p2 &lt;- ggplot(df, aes(x = Successes)) +\n    geom_bar(fill = \"coral\", color = \"black\", alpha = 0.7) +\n    labs(title = paste(\"Simulated Successes (n =\", n_trials, \")\"),\n         x = \"Number of Successes\", y = \"Frequency\") +\n    theme_minimal() +\n    scale_x_continuous(limits = c(0, n_trials), breaks = 0:n_trials)\n  if (!is.null(observed_successes)) {\n    p2 &lt;- p2 + \n      geom_vline(xintercept = as.numeric(observed_successes), \n                 color = \"blue\", linetype = \"dashed\", linewidth = 1.2) +\n      annotate(\"text\", \n               x = as.numeric(observed_successes), \n               y = max(table(df$Successes)) * 0.45, \n               label = paste(\"Observed Success:\", observed_successes), \n               color = \"blue\", angle = 90, vjust = -0.5, hjust = -0.1)\n  }\n  gridExtra::grid.arrange(p1, p2, ncol = 2)\n}\n# Without observed data\n#prior_predictive_check(alpha = 2, beta = 2, n_trials = 10)\n# With observed number of successes\n#prior_predictive_check(alpha = 2, beta = 2, n_trials = 10, observed_successes = 7)\n# Example 1: Uniform Prior (Beta(1,1)) - No strong belief\n#prior_predictive_check(alpha = 1, beta = 1)\n# Example 2: Informative Prior (Beta(2,2)) - Believes success rate around 50%\n#prior_predictive_check(alpha = 2, beta = 2)\n# Example 3: Strong Prior (Beta(100,5)) - Believes success rate is high\n#prior_predictive_check(alpha = 100, beta = 5)\n# Example 4: Weak Prior (Beta(0.5, 0.5)) - Encourages extreme values (0 or 1)\n#prior_predictive_check(alpha = 0.5, beta = 0.5)\n\n\nLet’s now look at two examples in a health and medical context:\n\nPerfect Example: Estimating the probability of a successful treatment for a specific medical condition based on prior knowledge.\n\nSuppose, a new drug treatment has been tested on patients with a specific medical condition. Previous studies indicate that the drug has a 70% success rate in patients with similar characteristics. We are uncertain about the exact probability of success, but the previous studies give us a reasonable prior belief about the success rate.\nThe Beta(7,3) prior reflects our belief that the treatment has a 70% success rate on average. This prior configuration is centered around 0.7, with a reasonable spread, allowing for some variation while keeping the treatment’s effectiveness as a reasonable estimate.\n\n\nCode\nprior_predictive_check(alpha = 7, beta = 3, n_trials = 10, n_sim = 1000, observed_successes = 7)\n\n\n\n\n\n\n\n\n\nHistogram of \\(\\theta\\) shows most of the values around 0.7, reflecting the prior belief that the drug has a 70% success rate. For the simulated data we can see out of 10 trials, on average, 7 out of 10 patients are expected to succeed, but some variability (e.g., 6, 8 successes) is expected due to the spread of the prior.\n\nImperfect Example: Assuming a unreasonably high success rate for a new treatment without proper evidence.\n\nNow, assume a new drug for treating a condition has been introduced, but no clinical trials have been conducted yet. Despite this lack of data, the manufacturer assumes an overly optimistic success rate of 90% based on limited anecdotal evidence.\nBeta(90,10) prior assumes a very high success rate (about 90%), which is unrealistic without substantial evidence. This prior leads to a very narrow distribution, with values almost always close to 0.9, indicating extremely high success rates.\n\n\nCode\nprior_predictive_check(alpha = 90, beta = 10, n_trials = 10, n_sim = 1000, observed_successes = 7)\n\n\n\n\n\n\n\n\n\nHere, histogram of \\(\\theta\\) shows that most values are close to 0.9, and similarly, simulations result show an unrealistic scenario for a new drug with little evidence backing the success rate.\n\n\nPosterior Predictive Checks\nA posterior predictive check is a technique used in Bayesian statistics to assess how well a fitted model explains the observed data. It involves generating or simulating new data from the posterior predictive distribution and comparing it to the observed data to check for discrepancies. If the generated data looks similar to the observed data, the model is considered a good fit; if not, the model may be inadequate.\nUse of posterior predictive checks help to detect model misspecification, can provide intuitive, visual validation of the model’s performance.\n\nSteps to perform posterior predictive check\n\nWe first estimate the posterior distribution of the model parameters, say \\(\\theta\\) given the observed data \\(y\\). Then draw samples from the posterior predictive distribution (say \\(p(\\tilde{y}|y)\\), where \\(\\tilde{y}\\) is the predicted data), which represents hypothetical new data (\\(\\tilde{y}\\)) generated by the model.\nHence, use visualisations (e.g., histograms, density plots, scatter plots) or statistical summaries (e.g., mean, variance) to compare the simulated data to the actual observed data.\nIf the simulated data deviates significantly from the observed data, it suggests the model may be misspecified.\nLet’s consider a Bernoulli model, where suppose we have a dataset of \\(n\\) observations. Hence, the posterior predictive distribution for new data \\(\\tilde{y}\\) is:\n\\[\np(\\tilde{y} \\mid y) = \\int p(\\tilde{y} \\mid \\theta) p(\\theta \\mid y) \\, d\\theta\n\\]\nTo check if our model fits well, we generate new (simulated) data \\(\\tilde{y}^{(sim)}\\), and the compare \\(\\tilde{y}^{(sim)}\\) to observed data \\(y\\).\nIf simulated data is similar to observed data, the model is reasonable. Whereas, if there is a mismatch, the model might be misspecified (e.g., wrong prior, incorrect likelihood assumption).\nExample\nSuppose we want to estimate the probability of a successful treatment for a specific medical condition using Bayesian inference. The model assumes that each patient’s treatment outcome follows a Bernoulli distribution, and we use a Beta prior to express our prior beliefs about the success rate. We then perform a posterior predictive check to assess the model fit by simulating new data and comparing it to observed outcomes.\nAssume that we collected data from 30 patients who underwent treatment, of which 18 patients recovered (successes), while 12 did not recover (failures). Considering a uniform prior for \\(\\theta\\), i.e., \\(\\theta \\sim \\text{Beta}(1,1)\\), we can get the following plots for posterior distribution, and posterior predictive checks.\n\n\nCode\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(bayesplot)\nposterior_predictive_check &lt;- function(a, b, n_trials, successes, n_sim = 1000) {\n  posterior_alpha &lt;- a + successes\n  posterior_beta &lt;- b + (n_trials - successes)\n  theta_samples &lt;- rbeta(n_sim, posterior_alpha, posterior_beta)\n  bernoulli_samples &lt;- rbinom(n_sim, size = n_trials, prob = theta_samples)\n  df &lt;- data.frame(Theta = theta_samples, Successes = bernoulli_samples)\n  p1 &lt;- ggplot(df, aes(x = Theta)) +\n    geom_density(fill = \"skyblue\", alpha = 0.7) +\n    labs(title = paste(\"Posterior Distribution: Beta(\", posterior_alpha, \",\", posterior_beta, \")\"),\n         x = expression(theta), y = \"Density\") +\n    theme_minimal() + xlim(0,1)\n  p2 &lt;- ggplot(df, aes(x = Successes)) +\n    geom_bar(fill = \"coral\", color = \"black\", alpha = 0.7) +\n    geom_vline(xintercept = successes, color = \"red\", linetype = \"dashed\", size = 1) +\n    labs(title = paste(\"Posterior Predictive Check (n =\", n_trials, \")\"),\n         x = \"Number of Successful Treatments\", y = \"Frequency\") +\n    theme_minimal()\n  #\n  y_rep &lt;- t(replicate(n_sim*0.01, rbeta(n_sim, posterior_alpha, posterior_beta)))\n  color_scheme_set(\"blue\")\n  p3 = ppc_dens_overlay(theta_samples, y_rep) # +    labs(title=\"Posterior Predictive Check: Density Overlay\")\n  #\n  gridExtra::grid.arrange(p1, p2, p3, ncol = 2)\n}\n# n=30 patients, y=18 successful treatments, Beta(1,1) prior\nposterior_predictive_check(a = 1, b = 1, n_trials = 30, successes = 18)\n\n\n\n\n\n\n\n\n\nWe can see that the observed number of successful treatments (18) falls within the distribution of simulated outcomes, which implies that the model is consistent with the data.\nOn the other hand, if the observed value significantly deviates, it may indicate model misspecification, requiring adjustments to the prior or likelihood assumptions.",
    "crumbs": [
      "Week 4: **Generative Models and Tools**"
    ]
  },
  {
    "objectID": "M02_2.html#summary",
    "href": "M02_2.html#summary",
    "title": "Week 4: Generative Models and Tools",
    "section": "Summary",
    "text": "Summary\nToday’s lecture focused on understanding generative models, and how we can use generative models to answer research questions using Bayesian methods. We learn when to use exact inference and MCMC based optimisations, together their types. Finally we illustrate the prior and posterior predictive checks.\nBelow, we provide a javascript output to facilitate user-friendly visualisation of MCMC sampling algorithms, developed by Prof. Armando Teixeira-Pinto.",
    "crumbs": [
      "Week 4: **Generative Models and Tools**"
    ]
  },
  {
    "objectID": "M02_2.html#live-tutorial-and-discussion",
    "href": "M02_2.html#live-tutorial-and-discussion",
    "title": "Week 4: Generative Models and Tools",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 4: **Generative Models and Tools**"
    ]
  },
  {
    "objectID": "M02_2.html#tutorial-exercises",
    "href": "M02_2.html#tutorial-exercises",
    "title": "Week 4: Generative Models and Tools",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.\n\n\n\n\nBernardo, JM, MJ Bayarri, JO Berger, AP Dawid, D Heckerman, AFM Smith, and M West. 2007. “Generative or Discriminative? Getting the Best of Both Worlds.” Bayesian Statistics 8 (3): 3–24.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis (3rd Edition). Chapman; Hall/CRC.",
    "crumbs": [
      "Week 4: **Generative Models and Tools**"
    ]
  },
  {
    "objectID": "brief_module_03.html",
    "href": "brief_module_03.html",
    "title": "Module 3: Bayeswatch - Gaussian!",
    "section": "",
    "text": "Summary\nIn this module, we will start learning about how Bayesian methods can help us understand cause-and-effect relationships, not just patterns in the data. To do this, we’ll begin by understanding three important ideas such as the estimand, estimator, and estimate.\nWe will then look closely at a common type of Bayesian model: linear regression where the outcome is normally (or Gaussian) distributed. We’ll learn how to build this model step by step, i.e., choosing priors, fitting the model with data, and interpreting the results. We’ll also talk about what kinds of posterior summaries we can get from these models, like mean, median and credible intervals.\nTo see how this compares to more traditional statistics, we’ll take a look at frequentist regression, the kind you have already learned in basic statistics courses. Both Bayesian and frequentist methods can give similar answers, but the Bayesian approach gives us more flexibility by including prior knowledge and showing results as full probability distributions instead of just single estimates and p-values.\nAfter that, we’ll focus on how to choose priors for the model, especially for the part that measures how much the data varies (the variance). Many traditional models use something called the inverse gamma prior, but it can sometimes lead to unrealistic results. Instead, we’ll try more modern and reliable options like the half-Cauchy or exponential priors, which often lead to better results and easier interpretation.\nWe’ll also explore what happens when we use different types of priors for the regression coefficients. A weakly informative prior is a gentle way of guiding the model, where it doesn’t push the results too strongly, but helps prevent extreme values. On the other hand, an informative prior gives the model stronger guidance based on expert knowledge or previous studies. We’ll compare both types and see how they affect the results.\nFinally, we’ll bring everything together by comparing Bayesian regression with informative priors to frequentist regression. We’ll see how Bayesian methods allow us to build models that learn not just from the data in front of us, but also from what we already know, something that can be very helpful in real-world problems with limited data.\nBy the end of this module, you’ll be more comfortable with building and interpreting Bayesian models, thinking about cause-and-effect, and making thoughtful choices about prior information. This will give you a strong foundation for analysing complex problems in a clear and reliable way.",
    "crumbs": [
      "**Module 3:** Bayeswatch - Gaussian!"
    ]
  },
  {
    "objectID": "M03_1.html",
    "href": "M03_1.html",
    "title": "Week 5: Logical Connections",
    "section": "",
    "text": "Learnings\n– LO1: Explain the difference between Bayesian and frequentist concepts of statistical inference.\n– LO2: Demonstrate how to specify and fit simple Bayesian models with appropriate attention to the role of the prior distribution and the data model.\n– LO5: Engage in specifying, checking and interpreting Bayesian statistical analyses in practical problems using effective communication with health and medical investigators.\nBy the end of this week you should be able to:\n– Understand Bayesian model and causality\n– Explain the terms Estimand, Estimator & Estimate\n– Understand the difference between Bayesian and classical Regression.\n– Interpret real-life problems in Bayesian context.",
    "crumbs": [
      "Week 5: **Logical Connections**"
    ]
  },
  {
    "objectID": "M03_1.html#learnings",
    "href": "M03_1.html#learnings",
    "title": "Week 5: Logical Connections",
    "section": "",
    "text": "Outcomes\n\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 5: **Logical Connections**"
    ]
  },
  {
    "objectID": "M03_1.html#causality-and-bayesian-inference",
    "href": "M03_1.html#causality-and-bayesian-inference",
    "title": "Week 5: Logical Connections",
    "section": "Causality and Bayesian Inference",
    "text": "Causality and Bayesian Inference\n\n\n\nA causal inference is a conclusion about a cause-and-effect relationship between two or more things. In simple terms, it’s answering the question: “Did X cause Y?” For example, if a study finds that people who drink more water tend to have better skin, causal inference would be the process of figuring out whether drinking water actually causes better skin or if that’s just a coincidence, or maybe due to some other factor (like diet or lifestyle). On the other hand, a correlation is when two things tend to happen together. It shows a relationship, but not necessarily a cause-and-effect one.\nLet’s explore with another example. We might think, people who carry lighters are more likely to get lung cancer. This means that there is an association between carrying lighters and lung cancer, but it doesn’t imply that carrying a lighter causes lung cancer. Other factors, like smoking, could be the actual cause.\n\n\n\n\n\nConcept of correlation and causation\n\n\n\n\nNow, when we conduct causal inference, we must first develop a causal model that is separate from a purely Bayesian (or statistical) model, because observational data by itself is not enough to establish causality. This idea is widely accepted across different philosophical traditions, even though interpretations of how to approach it can vary greatly.\nThe most cautious view holds that causation is fundamentally unprovable. We can take a slightly less conservative view, which holds that we are able to infer causation, but only under strict and carefully defined conditions, such as randomisation and experimental control. However, many scientific questions cannot be addressed experimentally due to feasibility constraints or ethical concerns.\nIn fields like health and medicine, we often introduce various control variables into a statistical model, observe changes in estimates, and construct a causal narrative. This approach assumes that only omitted variables can bias causal conclusions, yet even included variables can introduce confusion.\nEven if we construct a causal model that appears to make accurate predictions, it may still misrepresent causation. If we rely on such a model to guide interventions, we risk producing unintended or misleading outcomes.\nIn this course, we will not discuss causal modelling in details, and for interested readers we refer books by Pearl, Glymour, and Jewell (2016) and Hernán and Robins (2025).\n\nEstimand, Estimator & Estimate\n\nNow, let us learn the terms estimand, estimator, and estimate. The estimand is the quantity of interest, i.e., the true value we seek to determine. We define an estimator as the method or procedure that we impliment to mimic/get the estimand. Finally, an estimate is the result (e.g., numerical value) we obtain from applying a specific estimator to data.\nSuppose, we want to find out the average effectiveness of a vaccine for a respiratory disease among children in Australia. Our estimand is “the true average vaccine effectiveness for this respiratory disease among all children in Australia.”\nSince we can’t measure every child, we take a random sample of 10,000 children (say) and record how well the vaccine protects them from the disease. Using this data, we now need to choose an estimator, which is a method to estimate our estimand.\nThe simplest approach that we can take is to calculate the average vaccine effectiveness in our sample. In this case, the sample average acts as our estimator, providing an estimate of the true vaccine effectiveness. If our sample average shows 85% effectiveness, then 85% is our estimate based on the sample average estimator.\nNow, in a Bayesian framework, the estimand remains the same, which is the true but unknown average vaccine effectiveness among all children in Australia, that we can obtain from the true distribution of the vaccine effectiveness. However, instead of just using the sample average calculated from the 10,000 respondents/children, our estimator in the Bayesian context is the method of obtaining the posterior distribution of vaccine effectiveness. And we already know that this distribution is derived by combining prior knowledge with the likelihood of the observed data using Bayes theorem.\n\n\nBayesian Regression Context\nNow, let us explain this in the context of Bayesian model, or regression problem. Considering the example explained earlier, suppose we are interested in understanding how vaccine efficacy for a respiratory-related disease among children in Australia is influenced by certain predictors, such as age, pre-existing health conditions, and socioeconomic status. In this case, our estimand is the set of true regression coefficients that describe the relationship between these predictors and vaccine efficacy.\nA perfect way to determine these coefficients would be to measure vaccine efficacy and collect all relevant predictor data (e.g., age, health conditions, and socioeconomic status) for every child in Australia and then fit a model to the entire population data. However, this is practically infeasible. Instead, we can estimate the regression coefficients using a sample observation, and as we have already mentioned, we can take a random sample of 10,000 children in Australia and collect data on their vaccine efficacy and the relevant predictors. Using this data, we fit a Bayesian model (e.g., a Bayesian linear regression model) to estimate the relationship between the predictors and vaccine efficacy.\nIn the Bayesian framework, we approach this problem by starting with a prior distribution for each regression coefficient, reflecting our prior beliefs about the relationships. For example, based on previous studies, we might believe that younger children have slightly lower vaccine efficacy and use this belief into the prior.\nThe sample data provides the likelihood, representing the probability of observing the data given different values of the regression coefficients. By applying Bayes theorem, we combine the prior with the likelihood to produce a posterior distribution for each regression coefficient. The posterior distributions represent our updated beliefs about the coefficients after observing the data.\nFor example, when considering the outcome variable on a continuous scale (i.e., without any transformation), if the posterior mean for the age coefficient is -0.5, it suggests that, on average, each additional year of age is associated with a 0.5 percentage point decrease in vaccine efficacy. Similarly, if the posterior mean for the socioeconomic status coefficient is 2.0, it suggests that higher socioeconomic status is associated with a 2 percentage point increase in vaccine efficacy.\nHere, estimand is the true regression coefficients that describe the relationships between the predictors and vaccine efficacy. Estimator is the posterior distributions of the regression coefficients that we get based on the Bayesian model. These distributions summarise the uncertainty about the coefficients. And estimate is the specific value derived from the posterior distributions, such as the posterior mean, median, or mode for each coefficient.\nThis Bayesian approach allows us not only to estimate the coefficients but to quantify our uncertainty about them, providing a more comprehensive understanding of the predictor influences on vaccine efficacy.",
    "crumbs": [
      "Week 5: **Logical Connections**"
    ]
  },
  {
    "objectID": "M03_1.html#model-development---gaussian-context",
    "href": "M03_1.html#model-development---gaussian-context",
    "title": "Week 5: Logical Connections",
    "section": "Model Development - Gaussian Context",
    "text": "Model Development - Gaussian Context\n\n\nWe have discussed about Directed Acyclic Graph (DAG) in one of our previous lectures. Today we will explain DAG for conceptualising and developing Bayesian regression models. It also helps to visually represent any potetntial causal relationships among variables and ensures proper adjustment for confounding factors.\nNow we explain this more with an example related to Bone Mineral Density (BMD) measured in \\(g/cm^2\\). For example, we want to know, how does body mass index (BMI) impact bone mineral density (BMD)?\nHere, we are trying to model the relationship between BMI (independent or exposure variable) and BMD (dependent or outcome or endpoint variable). Thus can draw the DAG as:\n\n\n\n\n\n\nIn a Bayesian context, we need to define the likelihood of observing the data given certain parameters and then combine that with a prior belief about those parameters.\n\nLikelihood, Prior and Posterior\nLet’s identify the likelihood and priors for the example:\nBMD is modeled as a random variable that follows a normal distribution:\n\\[\n\\text{BMD} \\sim N(\\mu, \\sigma^2)\n\\]\nThis notation means that for each observed value of BMD, the values are assumed to come from a normal distribution with mean \\(\\mu\\), which is the location of the distribution and variance \\(\\sigma^2\\), which indicates the spread or uncertainty around the mean.\nWe can now model the mean of the BMD as a function of the exposure variable, BMI. That is, the mean \\(\\mu\\) of BMD is determined by a linear relationship with BMI:\n\\[\n\\mu = \\beta_0 + \\beta_1 \\cdot \\text{BMI}\n\\]\nwhere, \\(\\beta_0\\) is the intercept term, which is the expected BMD when BMI is zero, and \\(\\beta_1\\) is the slope term, which indicates the change in BMD per unit change in BMI.\nSo, the likelihood function tells us how probable it is to observe the given BMD data, given specific values for the parameters \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\).\nNow, let’s define the prior distributions for the model parameters:\n\\[\n\\beta_0 \\sim N(\\mu_0, \\sigma_0^2)\n\\]\nThis states that \\(\\beta_0\\) follows a normal distribution with mean \\(\\mu_0\\) and variance \\(\\sigma_0^2\\). The prior for \\(\\beta_0\\) could represent our belief about the intercept term before observing the data. For instance, if we think that \\(\\beta_0\\) (the baseline BMD when BMI is 0) should be close to 0, then we would choose \\(\\mu_0 = 0\\), with some reasonable uncertainty \\(\\sigma_0^2\\).\n\\[\n\\beta_1 \\sim N(\\mu_1, \\sigma_1^2)\n\\]\nThis states that \\(\\beta_1\\), the effect of BMI on BMD, is also normally distributed with a mean of \\(\\mu_1\\) and variance \\(\\sigma_1^2\\). In this case, \\(\\mu_1\\) reflects our prior belief about the effect of BMI on BMD, and \\(\\sigma_1^2\\) reflects the uncertainty about that effect.\n\\[\n\\sigma^2 \\sim \\text{IG}(a, b)\n\\]\nThis represents the prior belief about the variance of the error terms (the variability in BMD not explained by BMI). It is modeled using an Inverse Gamma distribution, which is a conjugate prior distribution and denoted as $ (a, b)$, where: \\(a\\) is a shape parameter, and \\(b\\) is a scale parameter.\nThe inverse gamma distribution is commonly used for modelling variance parameters because it is from a conjugate family and produces positive values and can model both large and small variances.\nOnce we have the likelihood (how the data are distributed given the parameters) and the priors (our initial beliefs about the parameters), we can easily get the posterior distribution and in this case, the posterior distribution for the parameters \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\). This is proportional to:\n\\[\np(\\beta_0, \\beta_1, \\sigma^2 | \\text{BMD}, \\text{BMI}) \\propto p(\\text{BMD} | \\beta_0, \\beta_1, \\sigma^2) \\cdot p(\\beta_0) \\cdot p(\\beta_1) \\cdot p(\\sigma^2)\n\\]\nGiven the joint posterior distribution, we can write the full conditional distributions for parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) as:\n\\[\np(\\beta_0 \\mid \\beta_1, \\sigma^2, \\text{BMD}, \\text{BMI}) \\propto N(\\text{BMD} \\mid \\beta_0 + \\beta_1 \\cdot \\text{BMI}, \\sigma^2) \\cdot N(\\beta_0 \\mid \\mu_0, \\sigma_0^2)\n\\]\nThis is the product of a likelihood and a normal prior, so the conditional posterior is also normal:\n\\[\n\\beta_0 \\mid \\cdot \\sim N(\\mu_{\\beta_0}^*, \\sigma_{\\beta_0}^{2*})\n\\]\nWhere:\n\\[\n\\sigma_{\\beta_0}^{2*} = \\left( \\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2} \\right)^{-1}; \\quad \\mu_{\\beta_0}^* = \\sigma_{\\beta_0}^{2*} \\left( \\frac{1}{\\sigma^2} \\sum_{i=1}^n (\\text{BMD}_i - \\beta_1 \\text{BMI}_i) + \\frac{\\mu_0}{\\sigma_0^2} \\right)\n\\]\nNow, the conditional distribution for \\(\\beta_1\\):\n\\[\np(\\beta_1 \\mid \\beta_0, \\sigma^2, \\text{BMD}, \\text{BMI}) \\propto N(\\text{BMD} \\mid \\beta_0 + \\beta_1 \\cdot \\text{BMI}, \\sigma^2) \\cdot N(\\beta_1 \\mid \\mu_1, \\sigma_1^2)\n\\]\nHence, the full conditional is normal:\n\\[\n\\beta_1 \\mid \\cdot \\sim N(\\mu_{\\beta_1}^*, \\sigma_{\\beta_1}^{2*})\n\\]\nWhere:\n\\[\n\\sigma_{\\beta_1}^{2*} = \\left( \\frac{\\sum \\text{BMI}_i^2}{\\sigma^2} + \\frac{1}{\\sigma_1^2} \\right)^{-1}; \\quad \\mu_{\\beta_1}^* = \\sigma_{\\beta_1}^{2*} \\left( \\frac{1}{\\sigma^2} \\sum \\text{BMI}_i (\\text{BMD}_i - \\beta_0) + \\frac{\\mu_1}{\\sigma_1^2} \\right)\n\\]\nNow, we can write the Conditional distribution for \\(\\sigma^2\\) as:\n\\[\np(\\sigma^2 \\mid \\beta_0, \\beta_1, \\text{BMD}, \\text{BMI}) \\propto N(\\text{BMD} \\mid \\beta_0 + \\beta_1 \\cdot \\text{BMI}, \\sigma^2) \\cdot \\text{IG}(a, b)\n\\]\nCombining the likelihood (Gaussian) and Inverse Gamma prior, the full conditional is also Inverse Gamma:\n\\[\n\\sigma^2 \\mid \\cdot \\sim \\text{IG} \\left(a + \\frac{n}{2},\\ b + \\frac{1}{2} \\sum_{i=1}^n (\\text{BMD}_i - \\beta_0 - \\beta_1 \\text{BMI}_i)^2 \\right)\n\\]\n\n\nBayesian DAG\nBy using these information in the above example, we write DAG for the Bayesian model as:\n\n\n\n\n\n\nHere, circles represent variables, while squares denote model parameters and hyperparameters. We can also present the DAG in a more visually informative form below, including the hyperparameters along with their corresponding distributions.\n\n\n\n\n\n\nNow, let’s break down the concepts of estimand, estimator, and estimate using the Bayesian model represented by the DAG. In this example, the estimand is the quantity or parameter that we aim to estimate from the data, i.e., here the estimands are the parameters we want to infer, \\(\\beta_0\\) (intercept), \\(\\beta_1\\) (slope), and \\(\\sigma^2\\) (variance). Now, estimator is the method we are using, i.e., the Bayesian model with MCMC method to get the posterior distributions of the parameters \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\). In this Bayesian modelling framework, the estimates are the specific values derived from the posterior distributions of the parameters. For example, the mean or median of the posterior distribution of \\(\\beta_0\\) could be an estimate of the intercept. Similarly, the mean or median of the posterior distribution of \\(\\beta_1\\) could be an estimate of the slope.",
    "crumbs": [
      "Week 5: **Logical Connections**"
    ]
  },
  {
    "objectID": "M03_1.html#model-baesd-results",
    "href": "M03_1.html#model-baesd-results",
    "title": "Week 5: Logical Connections",
    "section": "Model-Baesd Results",
    "text": "Model-Baesd Results\n\n\n\nPriors\nWe consider the following prior distributions for the model parameters. Say, we do not know any previous information at the baseline for BMD ( i.e., intercept). Suppose, we do not have any knowledge about the effect of BMI on BMD. Hence, we can consider \\(\\beta_0,\\beta_1 \\sim N(0, 10^2)\\). Furthermore, let us assume the the variance parameter follows inverse-gamma distribution with hyper-parameters \\(a=2\\) and \\(b=1\\).\nThe prior structure we use follows a pattern of weakly informative priors. These help with regularisation by gently pulling parameter estimates toward zero, unless the data provide strong evidence for a large effect. This also helps avoid overestimating effects, a problem sometimes known as the winner’s curse.\nHence, following this structure, we can draw the prior distributions as follows:\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(invgamma)  \n\nx_norm &lt;- seq(-40, 40, length.out = 500)\nnormal_density &lt;- dnorm(x_norm, mean = 0, sd = 10)\ndf_beta &lt;- tibble(\n  x = x_norm,\n  density = normal_density\n)\np1 &lt;- ggplot(df_beta, aes(x, density)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  labs(\n    title = expression(\"Prior for \" ~ beta[0]),\n    x = expression(beta[0]),\n    y = \"Density\"\n  ) +\n  theme_minimal()\np2 &lt;- ggplot(df_beta, aes(x, density)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  labs(\n    title = expression(\"Prior for \" ~ beta[1]),\n    x = expression(beta[1]),\n    y = \"Density\"\n  ) +\n  theme_minimal()\nx_sigma2 &lt;- seq(0.001, 5, length.out = 500)\nsigma2_density &lt;- dinvgamma(x_sigma2, shape = 2, rate = 1)\ndf_sigma2 &lt;- tibble(\n  x = x_sigma2,\n  density = sigma2_density\n)\np3 &lt;- ggplot(df_sigma2, aes(x, density)) +\n  geom_line(color = \"firebrick\", size = 1.2) +\n  labs(\n    title = expression(\"Prior for \" ~ sigma^2),\n    x = expression(sigma^2),\n    y = \"Density\"\n  ) +\n  theme_minimal()\n(p1 | p2 | p3) + plot_annotation(title = \"Prior Distributions\")\n\n\n\n\n\n\n\n\n\nNow, we use these prior distributions to obtain the posterior distributions for the model parameters. We use R package “brms” to impliment the Bayesian model.\nNote that not all R packages (such as brms) have the option to use the inverse Gamma distribution directly in the R front-end function. We could avoid using Bayesian R packages and write individual Stan code to include the Inverse Gamma prior, but at this stage, that is beyond the scope of this course. Writing in Stan requires a strong understanding of mathematical coding, which may not be suitable for the general audience or students taking this course. Having said that, we encourage interested readers or students to stay in touch with the course coordinator for a deeper dive into writing core Stan code.\nNow, a simple solution related to the issue of inverse Gamma distribution is to use a distribution that approximates the inverse Gamma, and here Student’s t-distribution can be a good candidate for such distribution. While they are not equivalent, under certain conditions they can be closely approximated. Now, if\n\\[\n\\sigma^2 \\sim \\text{IG}(a, b), \\quad x \\mid \\sigma^2 \\sim N(0, \\sigma^2)\n\\]\nthen marginally,\n\\[\nx \\sim \\text{Student-}t_{(\\nu = 2a)}\\left(0, \\sqrt{\\frac{b}{a}}\\right)\n\\]\nThat is, we can approximate by considering:\n\nDegrees of freedom: \\(\\nu = 2a\\)\nLocation: \\(\\mu=0\\) (for variance parameter we restrict it to the \\(\\mathbb{R}^+\\))\nScale: \\(\\sqrt{\\frac{b}{a}}\\)\n\nNow, for \\(\\text{IG}(a=2,b=1)\\), we get the approximation as: \\(\\text{Student-t}(4,0,0.7)\\). Although, in practice, we have seen that for \\(\\sigma\\), use of \\(\\text{Student-t}(3,0,1)\\) is often a popular choice for the prior.\nHence, in this course, we will use Student-t distribution instead of the inverse Gamma to represnt the prior distribution for the Bayesian regression coefficient. Note that, in our next lecture, we will also discuss some other possible candidates for the prior distributions.\nNow, we can draw the prior distribution plots for inverse Gamma and Student-t distributions with different hyper-parmaters as follows:\n\n\nCode\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nig_density &lt;- function(alpha, beta, x) {\n  return(dgamma(1 / x, shape = alpha, rate = beta) / (x^2))  \n}\np1 &lt;- ggplot(data.frame(x = seq(0.001, 20, length.out = 1000)), aes(x = x)) +\n  stat_function(fun = function(x) ig_density(2, 1, x), col = \"red\", lwd = 2) +\n  labs(y = \"Density\", x = expression(sigma), \n       title = \"Inverse-Gamma(2, 1)\") +\n  theme_minimal()\np2 &lt;- ggplot(data.frame(x = seq(0.001, 20, length.out = 1000)), aes(x = x)) +\n     stat_function(fun = function(x) ig_density(1.5, 1.5, x), col = \"red\", lwd = 2) +\n     labs(y = \"Density\", x = expression(sigma), \n          title = \"Inverse-Gamma(1.5, 1.5)\") +\n     theme_minimal()\np3 &lt;- ggplot(data.frame(x = seq(0.001, 20, length.out = 1000)), aes(x = x)) +\n  stat_function(fun = function(x) dt(x / 0.707, df = 4) / 1, col = \"blue\", lwd = 2) +\n  labs(y = \"Density\", x = expression(sigma), \n       title = \"Student-t(4, 0, 0.707)\") +\n  theme_minimal()\np4 &lt;- ggplot(data.frame(x = seq(0.001, 20, length.out = 1000)), aes(x = x)) +\n  stat_function(fun = function(x) dt(x / 1, df = 3) / 1, col = \"blue\", lwd = 2) +\n  labs(y = \"Density\", x = expression(sigma), \n       title = \"Student-t(3, 0, 1)\") +\n  theme_minimal()\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nPosteriors\nWe fit the model using the brms package with a single MCMC chain and 2000 iterations. While this example uses one chain, multiple chains can also be specified. Note that for \\(\\sigma\\), we use Student-t prior distribution with degrees of freedom \\(\\nu = 3\\) and scale 1. Finally, we obtain a summary of the posterior distributions for the model parameters as follows:\n\n\nCode\nlibrary(brms)\nlibrary(tidyverse)\n\nbmd_data &lt;- read.csv(\"bmd_restricted.csv\")\nbmd_data$bmi &lt;- bmd_data$weight_kg/(bmd_data$height_cm/100)^2\nbmd_data &lt;- tibble(\n  BMD = bmd_data$bmd,\n  BMI = bmd_data$bmi,\n)\n# model\nbmd_model &lt;- brm(\n  formula = BMD ~ BMI,\n  data = bmd_data,\n  family = gaussian(),\n  prior = c(\n    prior(normal(0, 10), class = \"b\", coef = \"BMI\"),   # N(mean, sd) Slope priors\n    prior(normal(0, 10), class = \"Intercept\"),   # N(mean, sd) Intercept prior\n    prior(student_t(3, 0, 1), class = \"sigma\")  \n  ),\n  iter = 2000,\n  chains = 1,\n  cores = 3,\n  seed = 123\n)\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.052 seconds (Warm-up)\nChain 1:                0.043 seconds (Sampling)\nChain 1:                0.095 seconds (Total)\nChain 1: \n\n\nCode\nsummary(bmd_model)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: BMD ~ BMI \n   Data: bmd_data (Number of observations: 169) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.42      0.07     0.29     0.55 1.00     1302      852\nBMI           0.01      0.00     0.01     0.02 1.00     1317      852\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.16      0.01     0.14     0.17 1.00      719      614\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\n#posterior_summary(bmd_model)\n#plot(bmd_model)\n\n\nThe Bayesian model output indicates that the estimated intercept is 0.42, representing the expected BMD when BMI is zero, a value that isn’t realistic but is standard when interpreting linear models. The coefficient for BMI is 0.01, suggesting that for every one-unit increase in BMI, BMD increases by approximately 0.01 units on average. The 95% credible interval for this estimate ranges from 0.01 to 0.02, providing strong evidence that the effect of BMI on BMD is positive. The residual standard deviation (noise not explained by BMI) is estimated at 0.16 with a tight credible interval.\n\n\nMCMC Diagnostics\nWe see that the convergence diagnostics are excellent, with Rhat values equal to 1.00, showing that the MCMC chains have mixed well. Additionally, the Bulk Effective Sample Size (Bulk_ESS) and Tail Effective Sample Size (Tail_ESS) are both comfortably above 500. ESS values should ideally be over 400–500 for each parameter. Lower ESS suggests that the model might need more iterations or better mixing. The results indicate that the posterior estimates are based on a sufficient number of effective samples and can be considered stable and reliable.\nBelow we provide the histogram and trace plots related to the posterior distributions of the model parameters. We also provide the autocorrelation plot and posterior predictive check based on the posterior distributions.\n\n\nCode\nplot(bmd_model)\n\n\n\n\n\n\n\n\n\nCode\npost_samples &lt;- as.array(bmd_model)\n#acf(post_samples[, , \"b_Intercept\"], main = \"Autocorrelation: Intercept\")\nacf(post_samples[, , \"b_BMI\"], main = \"Autocorrelation: BMI\")\n\n\n\n\n\n\n\n\n\nCode\npp_check(bmd_model)\n\n\n\n\n\n\n\n\n\nThe trace plots show a well-mixed pattern for the MCMC samples. Similarly, we can see that the autocorrelation is low after a few iterations, i.e., the chains are well-mixed. Finally, the posterior predictive check provides further indication that the distribution match with the replications.",
    "crumbs": [
      "Week 5: **Logical Connections**"
    ]
  },
  {
    "objectID": "M03_1.html#bayesian-vs.-frequentist",
    "href": "M03_1.html#bayesian-vs.-frequentist",
    "title": "Week 5: Logical Connections",
    "section": "Bayesian vs. Frequentist",
    "text": "Bayesian vs. Frequentist\n\n\nNow, let us compare the posterior distributions we obtained from the Bayesian model with the estimates from frequentist linear regression model.\n\n\nCode\nlibrary(jtools)\nlm_model &lt;- lm(BMD ~ BMI, data = bmd_data)\njtools::summ(lm_model)\n\n\n\n\n\n\nObservations\n169\n\n\nDependent variable\nBMD\n\n\nType\nOLS linear regression\n\n\n\n\n \n\n\n\n\nF(1,167)\n27.98\n\n\nR²\n0.14\n\n\nAdj. R²\n0.14\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.42\n0.07\n6.11\n0.00\n\n\nBMI\n0.01\n0.00\n5.29\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nCode\nlm_coef &lt;- coef(summary(lm_model))\nbmi_est &lt;- lm_coef[\"BMI\", \"Estimate\"]\nbmi_se &lt;- lm_coef[\"BMI\", \"Std. Error\"]\nci_low &lt;- bmi_est - 1.96 * bmi_se\nci_high &lt;- bmi_est + 1.96 * bmi_se\n\npost &lt;- as_draws_df(bmd_model)\npost_bmi &lt;- post$b_BMI\nbayes_mean &lt;- mean(post_bmi)\nbayes_ci &lt;- quantile(post_bmi, probs = c(0.025, 0.975))\n\np &lt;- ggplot() +\n  geom_density(aes(x = post_bmi), fill = \"skyblue\", alpha = 0.5, color = NA) +\n  geom_vline(xintercept = bmi_est, color = \"red\", size = 1) +\n  geom_vline(xintercept = ci_low, linetype = \"dashed\", color = \"red\", size = 0.8) +\n  geom_vline(xintercept = ci_high, linetype = \"dashed\", color = \"red\", size = 0.8) +\n  geom_vline(xintercept = bayes_mean, color = \"blue\", size = 1) +\n  geom_vline(xintercept = bayes_ci[1], linetype = \"dashed\", color = \"blue\", size = 0.8) +\n  geom_vline(xintercept = bayes_ci[2], linetype = \"dashed\", color = \"blue\", size = 0.8) +\n  labs(\n    title = \"Bayesian vs Frequentist Estimate of BMI\",\n    subtitle = \"Red: Frequentist (Mean & 95% Confidence Interval)\\nBlue: Bayesian (Posterior Mean & 95% Credible Interval)\",\n    x = \"Coefficient for BMI\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(size = 12))\nlibrary(plotly)\nggplotly(p)\n\n\n\n\n\n\nThe graph compares our Bayesian and frequentist estimates of the BMI coefficient. The x-axis shows the BMI coefficient, and the y-axis shows the density. For the Bayesian estimate, we use a blue shaded area, with a solid blue vertical line representing the posterior mean, and the shaded area showing the 95% credible interval. For the frequentist estimate, we use two red dashed vertical lines to mark the maximum likelihood estimate (MLE) and the 95% confidence interval. This comparison helps us see the differences in how we estimate the BMI coefficient and the uncertainty in each method.\nIn our next lecture, we will look at how different types of prior distributions affect the posterior in a Bayesian hierarchical model, and how these results differ from those in the frequentist approach.",
    "crumbs": [
      "Week 5: **Logical Connections**"
    ]
  },
  {
    "objectID": "M03_1.html#summary",
    "href": "M03_1.html#summary",
    "title": "Week 5: Logical Connections",
    "section": "Summary",
    "text": "Summary\nToday’s lecture focused on understanding the relationship between causality and Bayesian inference, and how we can clearly distinguish between estimators, estimands, and estimates. We explored Bayesian linear regression in detail, highlighting its key differences compared to frequentist regression.",
    "crumbs": [
      "Week 5: **Logical Connections**"
    ]
  },
  {
    "objectID": "M03_1.html#live-tutorial-and-discussion",
    "href": "M03_1.html#live-tutorial-and-discussion",
    "title": "Week 5: Logical Connections",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 5: **Logical Connections**"
    ]
  },
  {
    "objectID": "M03_1.html#tutorial-exercises",
    "href": "M03_1.html#tutorial-exercises",
    "title": "Week 5: Logical Connections",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.\n\n\n\n\nHernán, M, and J Robins. 2025. Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P Jewell. 2016. Causal Inference in Statistics: A Primer. John Wiley & Sons.",
    "crumbs": [
      "Week 5: **Logical Connections**"
    ]
  },
  {
    "objectID": "M03_2.html",
    "href": "M03_2.html",
    "title": "Week 6: Prior Tweaks and More",
    "section": "",
    "text": "Learnings\n– LO1: Explain the difference between Bayesian and frequentist concepts of statistical inference.\n– LO2: Demonstrate how to specify and fit simple Bayesian models with appropriate attention to the role of the prior distribution and the data model.\n– LO4: Demonstrate proficiency in using statistical software packages (R) to specify and fit models, assess model fit, detect and remediate non-convergence, and compare models.\n– LO5: Engage in specifying, checking and interpreting Bayesian statistical analyses in practical problems using effective communication with health and medical investigators.\nBy the end of this week you should be able to:\n– Understand different aspects of prior distributions for variance parameter.\n– Explain which prior to use for Bayesian model with multiple variables.\n– Compare Bayesian and frequentist models.\n– Interpret real-life problems in Bayesian context.",
    "crumbs": [
      "Week 6: **Prior Tweaks and More**"
    ]
  },
  {
    "objectID": "M03_2.html#learnings",
    "href": "M03_2.html#learnings",
    "title": "Week 6: Prior Tweaks and More",
    "section": "",
    "text": "Outcomes\n\n\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 6: **Prior Tweaks and More**"
    ]
  },
  {
    "objectID": "M03_2.html#prior-options",
    "href": "M03_2.html#prior-options",
    "title": "Week 6: Prior Tweaks and More",
    "section": "Prior Options",
    "text": "Prior Options\nIn our previous lecture, using Bayesian model, we explained the effect of body mass index (BMI) on bone mineral density (BMD). We used weakly informative prior, and we discussed in one of our previous lectures that use of weakly informative priors are common in modern Bayesian modelling, as it balances interpretability and robustness of the posterior distribution. Now, we will explain how different types of prior distributions can be used for the model variance and slope parameters.",
    "crumbs": [
      "Week 6: **Prior Tweaks and More**"
    ]
  },
  {
    "objectID": "M03_2.html#prior-for-variability",
    "href": "M03_2.html#prior-for-variability",
    "title": "Week 6: Prior Tweaks and More",
    "section": "Prior for Variability",
    "text": "Prior for Variability\n\n\nHistorically, Bayesian models used Inverse-Gamma priors for variability parameters, because it has support on positive values and has nice mathematical properties (conjugate prior for the normal distribution). However, in Bayesian model (e.g., Bayesian regression), this might cause problems. For example, to represent non-informativeness, we can consider \\(\\text{IG}(a=0.001,b=0.001)\\), which uses very small values for the hyperparameters of the distribution. Even though \\(\\text{IG}(a=0.001,b=0.001)\\) appears non-informative, it biases the model toward very small variance values. It is “too informative” in a negative way, not because it is strong, but because it pretends to be weak while still influencing the outcome. Furthermore, the use of \\(\\text{IG}(a,b)\\) often favours small variances, which can lead to underestimating uncertainty in group-level effects. In addition, in regression settings, if the model is complex, the use of \\(\\text{IG}(a,b)\\) can make it difficult to achieve convergence in the MCMC sampling for the variance parameter, as the distribution can become spiky near zero, leading to unstable behavior during inference.\nWe have already discussed in our previous lecture that we can approximate the inverse Gamma prior with Student-t distribution. Still this will not aid some of the issues that we mentioned, such as not having a heavy tail. In today’s lecture will learn about some other distributions that can provide reasonable solutions.\n\nHalf-Cauchy Prior\nTo avoid the issues with inverse Gamma distribution, the half-Cauchy distribution (i.e., a Cauchy distribution restricted to positive values) is popularly used. Half-Cauchy also behaves as weakly informative prior and provides better inference (depending on the choice of hyper paramters).\nWe prefer using the half-Cauchy prior for the parameter \\(\\sigma\\) because it has several useful properties. It has heavy tails, which means it allows for large values of \\(\\sigma\\) when the data support it, rather than cutting them off or overly constraining them. Unlike the Inverse Gamma prior, it is less informative near zero and does not push the variance toward small values, which can be especially important in hierarchical models. It also provides a form of regularisation by gently pulling estimates toward smaller values without being too aggressive, allowing the data to guide the estimates more naturally.\nOne of the issues may arise for Half-Cauchy \\(\\sigma\\) prior in hierarchical Bayesian model relates to the non-conjugacy. However, use of cleaver MCMC sampling algorithm such as HMC-NUTS can provide a solution to this problem.\nNow, we can rewrite the DAG we provided in our last lecture related to the BMD model, where we replace the Inverse Gamma prior by the Half-Cauchy.\n\n\n\n\n\n\nHere, we use the Half-Cauchy prior distribution separately for the variance parameter instead of the Inverse Gamma distribution, while keeping the priors for \\(\\beta_0\\) and \\(\\beta_1\\), as normal distributions \\(N(0,10^2)\\), which gives us the following prior distributions:\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\n\nx_norm &lt;- seq(-40, 40, length.out = 500)\n\nnormal_density &lt;- dnorm(x_norm, mean = 0, sd = 10)\ndf_beta &lt;- tibble(\n  x = x_norm,\n  density = normal_density\n)\np1 &lt;- ggplot(df_beta, aes(x, density)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  labs(\n    title = expression(\"Prior for \" ~ beta[0]),\n    x = expression(beta[0]),\n    y = \"Density\"\n  ) +\n  theme_minimal()\np2 &lt;- ggplot(df_beta, aes(x, density)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  labs(\n    title = expression(\"Priors for \" ~ beta[1] * \" and \" * beta[0]),\n    x = expression(beta[1]),\n    y = \"Density\"\n  ) +\n  theme_minimal()\nx_sigma &lt;- seq(0.001, 5, length.out = 500)\nhalf_cauchy_density &lt;- dcauchy(x_sigma, location = 0, scale = 1)  # Half-Cauchy with scale = 1\ndf_sigma &lt;- tibble(\n  x = x_sigma,\n  density = half_cauchy_density\n)\np3 &lt;- ggplot(df_sigma, aes(x, density)) +\n  geom_line(color = \"firebrick\", size = 1.2) +\n  labs(\n    title = expression(\"Half-Cauchy Prior for \" ~ sigma),\n    x = expression(sigma),\n    y = \"Density\"\n  ) +\n  theme_minimal()\n#library(patchwork)\n#(p1 | p2 | p3) + plot_annotation(title = \"Prior Distributions\")\nlibrary(gridExtra)\ngrid.arrange(p2,p3,ncol=2)\n\n\n\n\n\n\n\n\n\nHence, we get the posterior summaries based on the Half-Cauchy prior distribution with hyper-parmater one as follows. Note that for ‘brms’ R package, we define the prior distributions for \\(\\sigma\\) instead of \\(\\sigma^2\\), which we write:\n\nprior(cauchy(0, 1), class = “sigma”)\n\n\n\nCode\nlibrary(brms)\nlibrary(tidyverse)\n\nbmd_data &lt;- read.csv(\"bmd_restricted.csv\")\nbmd_data$bmi &lt;- bmd_data$weight_kg/(bmd_data$height_cm/100)^2\nbmd_data &lt;- tibble(\n  BMD = bmd_data$bmd,\n  BMI = bmd_data$bmi,\n)\n\n# model\nbmd_model &lt;- brm(\n  formula = BMD ~ BMI,\n  data = bmd_data,\n  family = gaussian(),\n  prior = c(\n    prior(normal(0, 10), class = \"b\", coef = \"BMI\"),   # N(mean, sd) Slope priors\n    prior(normal(0, 10), class = \"Intercept\"),   # N(mean, sd) Intercept prior\n    prior(cauchy(0, 1), class = \"sigma\")  # Half-Cauchy prior for sigma\n  ),\n  iter = 2000,\n  chains = 1,\n  cores = 3,\n  seed = 123\n)\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.49 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.057 seconds (Warm-up)\nChain 1:                0.039 seconds (Sampling)\nChain 1:                0.096 seconds (Total)\nChain 1: \n\n\nCode\nsummary(bmd_model)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: BMD ~ BMI \n   Data: bmd_data (Number of observations: 169) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.42      0.07     0.29     0.57 1.00     1273      773\nBMI           0.01      0.00     0.01     0.02 1.00     1233      872\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.16      0.01     0.14     0.17 1.00      700      608\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\n#posterior_summary(bmd_model)\n#plot(bmd_model)\n\n\n\nThe model result suggests that the standard deviation (\\(\\sigma\\)) of residuals is estimated to be around 0.16, with the uncertainty around this estimate being quite low (Est.Error = 0.01). The credible interval for sigma is between 0.14 and 0.17, and the MCMC sampling appears to have converged well based on the Rhat and ESS values.\n\n\nExponential Prior\nAnother potential caldidate distribution for replacing the Inverse Gamma distribution for the variability parameter \\(\\sigma\\) is the Exponential distribution, i.e., \\(\\sigma \\sim \\text{Exp}(\\lambda)\\), with \\(\\lambda\\) as the hyper-parameter. This distribution is mathematically simple and computationally efficient. Using an exponential prior with a small rate (like \\(\\lambda = 1\\)) can be seen as a weakly informative prior. This means it doesn’t strongly influence the outcome but still provides some regularisation (keeping variance from growing excessively). This is helpful when you have limited prior knowledge about the variance, as it avoids over-penalising large values of the variance while still discouraging very small values.\nWe can get similar result using exponential prior distribution with rate hyper-parameter \\(\\lambda=1\\). Here in R code we need to replace\n\nprior(cauchy(0, 1), class = “sigma”)\n\nby\n\nprior(exponential(1), class = “sigma”)\n\nto get posterior distribution of the model parameter \\(\\sigma^2\\).\nIn particular, if we use \\(\\text{Half-Cauchy}(0,\\tau=1)\\) and \\(\\text{Exp}(\\lambda=1)\\), then this gives us the following prior distributions:\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyverse)\n\nexponential_density &lt;- dexp(x_sigma, rate = 1)  # Exponential with rate = 1\ndf_exp_sigma &lt;- tibble(\n  x = x_sigma,\n  density = exponential_density\n)\np4 &lt;- ggplot(df_exp_sigma, aes(x, density)) +\n  geom_line(color = \"darkorchid\", size = 1.2) +\n  labs(\n    title = expression(\"Exponential Prior for \" ~ sigma),\n    x = expression(sigma),\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\nlibrary(gridExtra)\ngrid.arrange(p3,p4,ncol=2)",
    "crumbs": [
      "Week 6: **Prior Tweaks and More**"
    ]
  },
  {
    "objectID": "M03_2.html#prior-for-slope",
    "href": "M03_2.html#prior-for-slope",
    "title": "Week 6: Prior Tweaks and More",
    "section": "Prior for Slope",
    "text": "Prior for Slope\n\n\n\nWeakly Informative & Informative\nSuppose, instead using weakly informative prior for \\(\\beta_1\\) (the slope for BMI), we want to use an informative prior. This refers to considering one unit increase in BMI, BMD increases by approximately 0.05 units on average, say we also knowfrom the past data that the standard deviation related to this is very low, i.e., 0.01. Hence, we write the prior distribution as: \\(\\beta_1 \\sim N(0.05, 0.01^2)\\).\n\n\nCode\nlibrary(brms)\nbmd_model_inform &lt;- brm(\n  formula = BMD ~ BMI,\n  data = bmd_data,\n  family = gaussian(),\n  prior = c(\n    prior(normal(0.05, 0.01), class = \"b\", coef = \"BMI\"),   # Informative prior for beta_1\n    prior(normal(0, 10), class = \"Intercept\"),   # Intercept prior\n    prior(cauchy(0, 1), class = \"sigma\")  # Half-Cauchy prior for sigma\n  ),\n  iter = 2000,\n  chains = 1,\n  cores = 3,\n  seed = 123\n)\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.055 seconds (Warm-up)\nChain 1:                0.032 seconds (Sampling)\nChain 1:                0.087 seconds (Total)\nChain 1: \n\n\nCode\nsummary(bmd_model_inform)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: BMD ~ BMI \n   Data: bmd_data (Number of observations: 169) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.36      0.07     0.23     0.50 1.00     1379      905\nBMI           0.02      0.00     0.01     0.02 1.00     1364      830\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.16      0.01     0.14     0.17 1.00      482      601\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFrom the results related to informative prior for \\(\\beta_1\\), we can say that the informative prior for \\(\\beta_1\\) tightly constrained the estimate of the effect of BMI on BMD, yielding a posterior mean estimate of 0.02 with a narrow credible interval \\([0.01, 0.02]\\). The model shows very high precision for this parameter, with low uncertainty accordingly.\n\n\nComparison\nNow we will provide a comparison of the \\(\\beta_1\\) estimates from two models: one where we used an informative prior, i.e., \\(\\beta_1\\sim N(0.05,0.01)\\) and another where we used a weakly informative prior \\(\\beta_1\\sim N(0,10^2)\\).\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(tibble)\n\nx_vals &lt;- seq(-1, 1, length.out = 1000)\nprior_df &lt;- tibble(\n  beta_1 = x_vals,\n  Informative = dnorm(x_vals, mean = 0.05, sd = sqrt(0.01)),\n  Weakly_Informative = dnorm(x_vals, mean = 0, sd = 10)\n)\nprior_df_long &lt;- pivot_longer(\n  prior_df,\n  cols = c(\"Informative\", \"Weakly_Informative\"),\n  names_to = \"Prior_Type\",\n  values_to = \"Density\"\n)\np1 &lt;- ggplot(prior_df_long, aes(x = beta_1, y = Density, color = Prior_Type, fill = Prior_Type)) +\n  geom_line(size = 1.2) +\n  geom_area(alpha = 0.2) +\n  labs(\n    title = expression(\"Prior for \" ~ beta[1]),\n    x = expression(beta[1] ~ \"Prior\"),\n    y = \"Density\",\n    color = \"\",\n    fill = \"\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\nlibrary(brms)\nlibrary(bayesplot)\nlibrary(tidyverse)\n\nposterior_inform &lt;- as_draws_df(bmd_model_inform)\nposterior_weak   &lt;- as_draws_df(bmd_model)\nbeta1_samples &lt;- bind_rows(\n  posterior_inform %&gt;% select(`b_BMI`) %&gt;% mutate(Model = \"Informative Prior\"),\n  posterior_weak %&gt;% select(`b_BMI`) %&gt;% mutate(Model = \"Weakly Informative Prior\")\n)\np2 &lt;- ggplot(beta1_samples, aes(x = b_BMI, fill = Model, color = Model)) +\n  geom_density(alpha = 0.4) +\n  labs(\n    title = expression(\"Posterior for \" ~ beta[1]),\n    x = expression(beta[1] ~ \"Posterior\"),\n    y = \"Density\",\n    fill = \"\",\n    color = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )  \nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nThis above plots show how two different types of prior information affect our estimate of a parameter, \\(\\beta_1\\). The blue curve represents the informative prior, this results in a distribution indicating higher certainty. The red curve represents the weakly informative prior, meaning we have less prior knowledge about \\(\\beta_1\\). This results in a distribution that is more spread out, indicating less certainty.",
    "crumbs": [
      "Week 6: **Prior Tweaks and More**"
    ]
  },
  {
    "objectID": "M03_2.html#bayesian-vs.-frequentist",
    "href": "M03_2.html#bayesian-vs.-frequentist",
    "title": "Week 6: Prior Tweaks and More",
    "section": "Bayesian vs. Frequentist",
    "text": "Bayesian vs. Frequentist\nNow we explain and compare the Bayesian and frequentist estimates, where we use informative prior distribution, i.e., \\(\\beta_1\\sim N(0.05,0.01)\\).\n\n\nCode\nlibrary(jtools)\nlm_model &lt;- lm(BMD ~ BMI, data = bmd_data)\njtools::summ(lm_model)\n\n\n\n\n\n\nObservations\n169\n\n\nDependent variable\nBMD\n\n\nType\nOLS linear regression\n\n\n\n\n \n\n\n\n\nF(1,167)\n27.98\n\n\nR²\n0.14\n\n\nAdj. R²\n0.14\n\n\n\n\n \n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n0.42\n0.07\n6.11\n0.00\n\n\nBMI\n0.01\n0.00\n5.29\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nCode\nlm_coef &lt;- coef(summary(lm_model))\nbmi_est &lt;- lm_coef[\"BMI\", \"Estimate\"]\nbmi_se &lt;- lm_coef[\"BMI\", \"Std. Error\"]\nci_low &lt;- bmi_est - 1.96 * bmi_se\nci_high &lt;- bmi_est + 1.96 * bmi_se\n\npost &lt;- as_draws_df(bmd_model_inform)\npost_bmi &lt;- post$b_BMI\nbayes_mean &lt;- mean(post_bmi)\nbayes_ci &lt;- quantile(post_bmi, probs = c(0.025, 0.975))\n\np &lt;- ggplot() +\n  geom_density(aes(x = post_bmi), fill = \"skyblue\", alpha = 0.5, color = NA) +\n  geom_vline(xintercept = bmi_est, color = \"red\", size = 1) +\n  geom_vline(xintercept = ci_low, linetype = \"dashed\", color = \"red\", size = 0.8) +\n  geom_vline(xintercept = ci_high, linetype = \"dashed\", color = \"red\", size = 0.8) +\n  geom_vline(xintercept = bayes_mean, color = \"blue\", size = 1) +\n  geom_vline(xintercept = bayes_ci[1], linetype = \"dashed\", color = \"blue\", size = 0.8) +\n  geom_vline(xintercept = bayes_ci[2], linetype = \"dashed\", color = \"blue\", size = 0.8) +\n  labs(\n    title = \"Bayesian (informative) vs Frequentist Estimate of BMI\",\n    subtitle = \"Red: Frequentist (Mean & 95% Confidence Interval)\\nBlue: Bayesian (Posterior Mean & 95% Credible Interval)\",\n    x = \"Coefficient for BMI\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(size = 12))\nlibrary(plotly)\nggplotly(p)\n\n\n\n\n\n\nThe above plot shows the estimates of the BMI coefficient (\\(\\beta_1\\)) for BMD using both frequentist and Bayesian methods. On the x-axis, we see the coefficient for BMI, ranging from approximately 0.008 to 0.024, while the y-axis represents density, showing the distribution of the estimates.\nIn Bayesian inference, the blue shaded area represents our posterior distribution of \\(\\beta_1\\), which incorporates both prior information and observed data. The vertical blue solid line indicates our Bayesian posterior mean, and the vertical blue dashed lines show the 95% credible interval. This interval represents the range within which \\(\\beta_1\\) lies with 95% probability, given our prior and the data. The Bayesian approach provides a more nuanced estimate that reflects both our prior beliefs and the observed data, resulting in a posterior distribution that can be more or less spread out depending on the prior and the data.\nWhereas, the frequentist approach relies solely on the observed data to provide point estimates and confidence intervals. The vertical red solid line represents our frequentist maximum likelihood estimate (MLE) of \\(\\beta_1\\), and the vertical red dashed lines show the 95% confidence interval. This interval represents the range within which \\(\\beta_1\\) would lie in 95% of repeated samples, assuming the true value is fixed.\nThe key difference highlighted by this plot is how each method estimates and interprets \\(\\beta_1\\). We can see that the informative prior shifts the mean posterior distribution. We can also see that the Bayesian credible interval is much narrower due to the influence of the informative prior, suggesting that prior information has influenced the estimate.",
    "crumbs": [
      "Week 6: **Prior Tweaks and More**"
    ]
  },
  {
    "objectID": "M03_2.html#further-model-development",
    "href": "M03_2.html#further-model-development",
    "title": "Week 6: Prior Tweaks and More",
    "section": "Further Model Development",
    "text": "Further Model Development\n\n\nGaussian Context\nFollowing the BMD example, where we explore the influence of BMI on BMD. Now, we might want to ask: What role does ‘Age’ or ‘Sex’ of the patient play in this relationship?\nAs people age, their BMD naturally decreases over time, and age also influences factors like BMI. Similarly, sex affects both BMI and BMD, with women being more likely to experience a decline in BMD, particularly in conditions like osteoporosis. These factors age and sex may act as confounders, influencing both BMI and BMD. Hence, we write the DAG using these variables:\n\n\n\n\n\n\n\nIn this case, the estimand is the specific effect of BMI on BMD, while accounting for the influence of age and sex as confounders. The goal is to isolate the effect of BMI on BMD after adjusting for these other variables.\nThe estimator we define here is the Bayesian model, i.e., the Bayesian multiple linear regression model to get the posterior distributions for the model parameters. This model adjusts for confounders like age and sex, helping us to estimate the causal effect of BMI on BMD.\nThe estimate is the posterior distribution of the estimand with some numerical values, such as mean or median derived from the posterior distribution. For example, if the posterior mean estimate is 0.03, this could represent the change in BMD associated with a one-unit increase in BMI, after accounting for the effects of age and sex.\n\nModel & DAG\nWe now develop the Bayesian model with all these four variables. Hence, we write the Bayesian model as:\n\\[\n\\text{BMD}_i \\sim N(\\beta_0 + \\beta_1 \\cdot \\text{BMI}_i + \\beta_2 \\cdot \\text{Age}_i + \\beta_3 \\cdot \\text{Sex}_i, \\sigma^2)\n\\]\nWhere, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the coefficient for BMI, \\(\\beta_2\\) is the coefficient for Age, \\(\\beta_3\\) is the coefficient for Sex (with a reference category; for example, if Sex is binary, it could be “Male” vs. “Female”). This model also includes the variability \\(\\sigma\\) of the error term. Now, assuming weakly informative prior we write:\n\n\\(\\beta_0 \\sim N(0, 10^2)\\)\n\\(\\beta_1 \\sim N(0, 10^2)\\)\n\\(\\beta_2 \\sim N(0, 10^2)\\)\n\\(\\beta_3 \\sim N(0, 10^2)\\)\n\\(\\sigma \\sim \\text{Half-Cauchy}(0, 1)\\)\n\nNote that the model equation can be also written as:\n\\[\n\\text{BMD}_i = \\beta_0 + \\beta_1 \\cdot \\text{BMI}_i + \\beta_2 \\cdot \\text{Age}_i + \\beta_3 \\cdot \\text{Sex}_i + \\epsilon_i\n\\]\nwhere, \\(\\epsilon_i\\) is the error term of the model.\nHence, we draw the DAG for this Bayesian model with prior and hyper-prior parameters as:\n\n\n\n\n\n\n\n\nResults & MCMC Diagnostics\nNow, we implement the Bayesian hierarchical model for this DAG, where we use weakly-informative priors for the model parameters.\n\n\nCode\nlibrary(brms)\nlibrary(tidyverse)\n\nbmd_data &lt;- read.csv(\"bmd_restricted.csv\")\nbmd_data$bmi &lt;- bmd_data$weight_kg/(bmd_data$height_cm/100)^2\nbmd_data &lt;- tibble(\n  BMD = bmd_data$bmd,\n  BMI = bmd_data$bmi,\n  Age = bmd_data$age,\n  Sex = as.factor(bmd_data$sex)\n)\nbmd_model_multi &lt;- brm(\n  formula = BMD ~ BMI + Age + Sex,\n  data = bmd_data,\n  family = gaussian(),\n  prior = c(\n    prior(normal(0, 10), class = \"b\", coef = \"BMI\"), # N(mean, sd)\n    prior(normal(0, 10), class = \"b\", coef = \"Age\"), # N(mean, sd)\n    prior(normal(0, 10), class = \"b\", coef = \"SexM\"), # N(mean, sd)\n    prior(normal(0, 10), class = \"Intercept\"), # N(mean, sd)\n    prior(cauchy(0, 1), class = \"sigma\")  # Half-Cauchy prior for sigma\n    ),  \n  iter = 2000,\n  chains = 1,\n  cores = 3,\n  seed = 123\n)\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.54 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.141 seconds (Warm-up)\nChain 1:                0.084 seconds (Sampling)\nChain 1:                0.225 seconds (Total)\nChain 1: \n\n\nCode\n#prior_summary(bmd_model_multi, all = FALSE)\nprint(prior_summary(bmd_model_multi, all = FALSE), show_df = FALSE)\n\n\nb_Age ~ normal(0, 10)\nb_BMI ~ normal(0, 10)\nb_SexM ~ normal(0, 10)\nIntercept ~ normal(0, 10)\n&lt;lower=0&gt; sigma ~ cauchy(0, 1)\n\n\nCode\nprint(summary(bmd_model_multi), digits=3)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: BMD ~ BMI + Age + Sex \n   Data: bmd_data (Number of observations: 169) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nIntercept    0.604     0.083    0.441    0.765 1.001     1168      803\nBMI          0.016     0.002    0.011    0.020 1.001     1071      536\nAge         -0.004     0.001   -0.006   -0.003 1.004     1128      692\nSexM         0.095     0.021    0.054    0.134 1.003      654      613\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI  Rhat Bulk_ESS Tail_ESS\nsigma    0.139     0.008    0.125    0.155 1.001      574      579\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nFrom the posterior summaries, we can see a positive association between BMI and BMD, with a mean estimate of 0.016 (95% credible interval 0.010 to 0.021). This suggests that for every one unit increase in BMI, BMD increases by approximately 0.016 units, holding other factors constant. Although the effect is small, the credible interval excludes zero, indicating high certainty in the direction of the relationship.\nAge, on the other hand, shows a negative effect on BMD, with a mean estimate of –0.004 (–0.006 to –0.003). This implies that each additional year of age is associated with a modest but consistent decline in BMD, highlighting the typical age-related deterioration in bone health.\nRegarding sex differences, the model estimates that males have on average 0.095 units higher BMD than females (0.050 to 0.139), after adjusting for BMI and age. This represents a moderate and credible sex-based difference in bone density.\nFinally, the posterior estimate of \\(\\sigma\\) (residual standard deviation) is 0.139 (0.124 to 0.154). This value reflects the remaining variability in BMD not explained by the included predictors. The relatively low magnitude suggests that the model captures the key drivers of BMD fairly well.\nWe can also observe that all \\(\\hat{R}\\) values are \\(\\approx\\) 1.00, i.e., convergence is excellent. The Bulk and Tail ESS values are all \\(&gt;600\\), i.e., sufficient posterior sample size and good mixing of chains.\nTrace Plots\n\n\nCode\nplot(bmd_model_multi)\n\n\n\n\n\n\n\n\n\nCode\n#library(bayesplot)\n#library(brms)\n#posterior &lt;- as_draws_df(bmd_model_multi)\n#mcmc_areas(\n#  posterior,\n#  pars = c(\"b_BMI\", \"b_Age\", \"b_SexM\"),\n#  prob = 0.95  # 95% credible intervals\n#)\n#mcmc_trace(\n#  posterior,\n#  pars = c(\"b_Intercept\", \"b_BMI\", \"b_Age\", \"b_SexM\")\n#)\n\n\nTrace plots for the MCMC samples also shows a nice mixing and density (histogram) plots also shows a normal distributional shape, confirms a good MCMC mixing for the model parameters.\nConditional Effects\nWe can also plot the conditional effects of the predictor variables BMI, Age and Sex. Here, conditional effects refer to the effect of say BMI on BMD considering other variables (i.e., Age and Sex) fixed and so on for Age and Sex.\n\n\nCode\n#plot(conditional_effects(bmd_model_multi, effects = \"BMI\"), points = TRUE)\n#plot(conditional_effects(bmd_model_multi, effects = \"Age\"), points = TRUE)\n#plot(conditional_effects(bmd_model_multi, effects = \"Sex\"))\n\nce &lt;- conditional_effects(bmd_model_multi)\np1 &lt;- plot(ce, effects = \"BMI\", plot = FALSE, points = TRUE)[[1]]\np2 &lt;- plot(ce, effects = \"Age\", plot = FALSE, points = TRUE)[[2]]\np3 &lt;- plot(ce, effects = \"Sex\", plot = FALSE)[[3]]\n#library(patchwork)\n#combined_plot &lt;- p1 + p2 + p3  # 1 row # OR use / to stack vertically: combined_plot &lt;- p1 / p2 / p3\n#combined_plot\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, ncol = 3)\n\n\n\n\n\n\n\n\n\nIn the first plot, we see a positive trend, as BMI increases, BMD also tends to go up. The shaded area around the line shows the uncertainty, or the range where the true trend is likely to fall. The second plot shows a negative trend with Age, meaning that as people get older, their BMD tends to decrease. In the third plot, we compare BMD between males and females. We can see that males tend to have higher BMD than females. The vertical lines (error bars) show how much BMD varies within each group.\npredictive checks\nWe can also look at the posterior predictive plot to see how well the model fits the data. As we have discussed in one of the previous lectures, if the plot looks similar to the actual data, that means the model is doing a good job. But if the predicted values are too spread out, too narrow, or miss important patterns, we might need to adjust the model by adding better predictors, transforming variables, or trying a different type of model.\n\n\nCode\n# Posterior predictive check\n#pp_check(bmd_model_multi)\n#pp_check(bmd_model_multi, type = \"hist\")\n#pp_check(bmd_model_multi, type = \"boxplot\")\n#pp_check(bmd_model_multi, type = \"scatter_avg\")\n#pp_check(bmd_model_multi, type = \"ecdf_overlay\")\n\n#library(patchwork)\np1 &lt;- pp_check(bmd_model_multi)\np2 &lt;- pp_check(bmd_model_multi, type = \"ecdf_overlay\")\n#combined_plot &lt;- p1 + p2 \n#combined_plot\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nFrom the above plots, we can see that the replications in the posterior predictive plot of BMD match the actual observed BMD.",
    "crumbs": [
      "Week 6: **Prior Tweaks and More**"
    ]
  },
  {
    "objectID": "M03_2.html#summary",
    "href": "M03_2.html#summary",
    "title": "Week 6: Prior Tweaks and More",
    "section": "Summary",
    "text": "Summary\nToday’s lecture focused on understanding priors in Bayesian regression, specifically for variance and the coefficients. We discussed weakly-informative and informative priors for \\(\\beta_1\\), and their implications in modelling. A comparison between Bayesian and frequentist approaches was made, particularly in the context of using an informative prior for \\(\\beta_1\\). Finally, we explored how to further develop the model to better understand exposure and confounders, and how these can be incorporated into a Bayesian regression framework.",
    "crumbs": [
      "Week 6: **Prior Tweaks and More**"
    ]
  },
  {
    "objectID": "M03_2.html#live-tutorial-and-discussion",
    "href": "M03_2.html#live-tutorial-and-discussion",
    "title": "Week 6: Prior Tweaks and More",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 6: **Prior Tweaks and More**"
    ]
  },
  {
    "objectID": "M03_2.html#tutorial-exercises",
    "href": "M03_2.html#tutorial-exercises",
    "title": "Week 6: Prior Tweaks and More",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.",
    "crumbs": [
      "Week 6: **Prior Tweaks and More**"
    ]
  },
  {
    "objectID": "brief_module_04.html",
    "href": "brief_module_04.html",
    "title": "Module 4: Bayeswatch - Non Gaussian!",
    "section": "",
    "text": "Summary\nIn this module, we will extend our understanding of Bayesian methods to models where the outcome is not normally distributed. Specifically, we’ll explore how Bayesian approaches can be used when the response variable is binary, such as in logistic regression. This is useful for situations where we want to predict the probability of an event happening, like whether a patient recovers. We’ll learn how to build a Bayesian logistic regression model and how to interpret the results, especially in terms of odds ratios.\nWe will then look at how to choose appropriate priors for logistic regression. Since the model estimates log-odds, a key focus will be on setting priors for the coefficients in a way that makes sense for odds ratios. We’ll explore how different choices of priors, like normal and Cauchy distributions, can influence the model. Through prior sensitivity analysis, we’ll see how robust our results are to different prior beliefs, helping us understand when our conclusions are driven by data and when they depend on prior choices.\nNext, we’ll compare Bayesian logistic regression to its frequentist counterpart. Both methods aim to estimate the same underlying relationship, but Bayesian models provide a full probability distribution for each parameter, offering richer insights. We’ll discuss how this added flexibility allows Bayesian models to incorporate prior information and manage uncertainty more explicitly, especially in small sample sizes or when data are sparse.\nAfter that, we’ll introduce more advanced prior structures using hierarchical models. We’ll look at hierarchical priors such as the normal-exponential and Cauchy-exponential hierarchies, which can help when we have multiple related predictors or when we want to shrink estimates toward zero without being too strict. These hierarchical priors can adapt to the structure of the data, improving both model stability and interpretability.\nWe’ll also explore count data models, like Poisson regression, for outcomes that are non-negative integers (e.g., number of visits to a clinic or number days in ICU). These models assume a different distribution than Gaussian or binary outcomes, and we’ll learn how to build them in a Bayesian framework. In practice, count data often show overdispersion, where the variance exceeds the mean, or an excess of zeros, leading us to explore models like the negative binomial or zero-inflated Poisson.\nFinally, we’ll discuss how to recognise when simple models are not enough and how to extend them thoughtfully. Whether it’s using a hierarchical prior for regularisation, or switching to a zero-inflated model to handle special data features, we’ll focus on matching the model to the data in a principled way. This will help you build models that are not only more accurate but also more realistic and interpretable.\nBy the end of this module, you’ll be equipped to build Bayesian models for binary and count outcomes, choose and justify priors thoughtfully, compare Bayesian and frequentist results, and handle common real-world complications like overdispersion and zero-inflation. This will give you deeper insight into modelling non-Gaussian data and greater confidence in applying Bayesian methods in practice.",
    "crumbs": [
      "**Module 4:** Bayeswatch - Non Gaussian!"
    ]
  },
  {
    "objectID": "M04_1.html",
    "href": "M04_1.html",
    "title": "Week 7: Non-Gaussian",
    "section": "",
    "text": "Learnings\n– LO1: Explain the difference between Bayesian and frequentist concepts of statistical inference.\n– LO2: Demonstrate how to specify and fit simple Bayesian models with appropriate attention to the role of the prior distribution and the data model.\n– LO5: Engage in specifying, checking and interpreting Bayesian statistical analyses in practical problems using effective communication with health and medical investigators.\nBy the end of this week you should be able to:\n– Implement non-Gaussian Bayesian model with binary outcome.\n– Understand the difference between Bayesian and classical logistic models.\n– Interpret real-life problems with binary endpoint variable in Bayesian context.\n– Formulate problems and solutions.",
    "crumbs": [
      "Week 7: **Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_1.html#learnings",
    "href": "M04_1.html#learnings",
    "title": "Week 7: Non-Gaussian",
    "section": "",
    "text": "Outcomes\n\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 7: **Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_1.html#non-gaussian-bayesian-model",
    "href": "M04_1.html#non-gaussian-bayesian-model",
    "title": "Week 7: Non-Gaussian",
    "section": "Non-Gaussian Bayesian Model",
    "text": "Non-Gaussian Bayesian Model\nA non-Gaussian Bayesian model is any Bayesian model where the likelihood, i.e., the model for the observed data is not based on the normal distribution. In many real-world situations, the assumptions of the normal distribution do not hold. The response variable might be binary, a count, a proportion, or restricted to positive values. In such cases, it is more appropriate to use a different distribution that better matches the data.\nMany of these alternative distributions come from what is known as the exponential family of distributions. This family includes a wide range of commonly used distributions such as:\n\nBernoulli for binary outcomes\n\nPoisson for count data\n\nExponential and gamma for positive continuous data\n\nCategorical and multinomial for discrete choices\n\nThese distributions have a shared mathematical form, which makes them convenient for modelling. Specifically, they can be written in a way that separates the data from the parameters in a structured form. This makes them suitable for use in generalised linear models and their Bayesian extensions.\nNon-Gaussian Bayesian models are especially useful because they allow us to model a wider range of data types, incorporate prior knowledge, and fully represent uncertainty. By using distributions from the exponential family, we take advantage of mathematical properties that simplify inference and make modelling more robust.\nIn this module, we will explore Bayesian models to tackle binary and count data and in today’s lecture we will learn models with binary outcome.",
    "crumbs": [
      "Week 7: **Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_1.html#understanding-binary-outcome",
    "href": "M04_1.html#understanding-binary-outcome",
    "title": "Week 7: Non-Gaussian",
    "section": "Understanding Binary Outcome",
    "text": "Understanding Binary Outcome\n\n\nBinary variables may seem simple at first, just yes or no, 1 or 0, success or failure. But there’s often more happening behind the scenes, especially when the outcome depends on other factors. For instance, how likely is a patient to recover depending on their treatment and medical history?\nTo study questions like this, we use regression models designed for binary outcomes. These models help us explore how different variables influence the likelihood of a particular result. Two common types are logistic regression and probit regression. Logistic regression models the log-odds of success, while probit regression uses the cumulative normal distribution to relate predictors to outcome probabilities.\nWhen we take a Bayesian approach, we build on this foundation in an important way, as we have already lean that instead of estimating fixed values for model parameters, we treat those parameters as uncertain and describe them using probability distributions. We start with prior distributions that reflect our beliefs or assumptions and then, we update these beliefs in light of the observed data using Bayes theorem, resulting in posterior distributions.\nTo make this more concrete, imagine we’re modelling the probability that a patient recovers, where recovery is coded as 1 and no recovery as 0. We might model this probability as a function of whether the patient received a particular treatment. Once we observe data, we update priors to get posterior distributions for the treatment effect. From these, we can calculate probabilities, make predictions, and evaluate the uncertainty in our conclusions.\nBelow we provide a more detailed example related to the bone mineral density (BMD) data we discussed in previous lectures. Here, our research aim is now to understand the effect of BMD on the fracture status of the bone (‘Status’), where we assume no-fracture refers to zero (reference category) and fracture refers to one.\n\nBone Mineral Density Data\nLet’s start with a descrptive summary of the variables that we want to use to develop the model. We observe that the dataset, named bmd_data, comprises 169 rows and 5 columns. Among these columns, two are categorical (factors) and three are numerical.\nFor the categorical variables, we have ‘Status’ and ‘Sex’. The ‘Status’ variable has no missing values and includes two unique categories: “No” (i.e., no fracture) and “Fra” (i.e., fracture) with counts of 119 and 50, respectively. This indicates that the majority of the observations fall under the no fracture category. Similarly, the Sex variable also has no missing values and includes two categories: “F” (female) and “M” (male), with counts of 86 and 83, respectively. We can also see that the mean BMD is 0.78, with a standard deviation of 0.17, indicating some variability around the mean. The mean BMI is 25.2, with a standard deviation of 4.41, suggesting a moderate variability. Lastly, we ca see that the mean age is 63.6 years, with a standard deviation of 12.4 years, indicating a relatively older population with some age variability. The age ranges from a minimum of 36 to a maximum of 89 years.\n\n\nCode\nlibrary(tidyverse)\nlibrary(skimr)\nbmd_data &lt;- read.csv(\"bmd_restricted.csv\")\nbmd_data$bmi &lt;- bmd_data$weight_kg/(bmd_data$height_cm/100)^2\nbmd_data$fracture_status &lt;- factor(bmd_data$fracture)\nbmd_data$fracture_status &lt;- relevel(bmd_data$fracture_status, ref = \"no fracture\")\nbmd_data &lt;- tibble(\n  Status = bmd_data$fracture_status,\n  BMD = bmd_data$bmd,\n  BMI = bmd_data$bmi,\n  Age = bmd_data$age,\n  Sex = as.factor(bmd_data$sex)\n)\nskimr:::skim_without_charts(bmd_data)\n\n\n\nData summary\n\n\nName\nbmd_data\n\n\nNumber of rows\n169\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nStatus\n0\n1\nFALSE\n2\nno : 119, fra: 50\n\n\nSex\n0\n1\nFALSE\n2\nM: 86, F: 83\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\nBMD\n0\n1\n0.78\n0.17\n0.41\n0.67\n0.79\n0.89\n1.36\n\n\nBMI\n0\n1\n25.20\n4.41\n15.43\n22.15\n24.96\n27.55\n38.54\n\n\nAge\n0\n1\n63.63\n12.36\n35.81\n54.42\n63.49\n72.08\n88.75\n\n\n\n\n\nNow, based on out research aim, i.e., understanding the effect of BMD on the fracture status of the bone (‘Status’), we can develop the Bayesian model as follows.",
    "crumbs": [
      "Week 7: **Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_1.html#bayesian-model-development",
    "href": "M04_1.html#bayesian-model-development",
    "title": "Week 7: Non-Gaussian",
    "section": "Bayesian Model Development",
    "text": "Bayesian Model Development\n\n\n\nDAG\nLet’s explain the example using a directed acyclic graph (DAG). In our research, we’re trying to understand how Bone Mineral Density (BMD) total (\\(gm/cm^2\\)) measured in spine affects whether a bone breaks or not. We group fracture status into two categories: no fracture (0) and fracture (1).\nIn this case, BMD total in spine is our main variable of interest, this is the exposure variable, because we want to see how changes in BMD are linked to the risk of fractures. At the same time, we also look at Body Mass Index (BMI), Age, and Sex. These are the confounders, which means they can influence both BMD and the risk of having a fracture. If we don’t take them into account, we might get the wrong idea about how strongly BMD is related to fractures.\nSo, we’re using this setup to better understand how all these factors: BMD, BMI, age, and sex, work together and affect whether someone has a bone fracture or not. Each factor follows a certain pattern, and by studying these patterns, we can make better predictions about bone health.\n\n\n\n\n\n\nBased on the example and DAG, we can identify the estimand as the effect of Bone Mineral Density (BMD) in the spine on the probability of a bone fracture, adjusting for (i.e. conditional on) BMI, Age, and Sex. The estimator is the Bayesian model that identifies fracture risk based on BMD, while adjusting for BMI, Age, and Sex. This model also captures uncertainty and incorporates prior knowledge, the typical features of a Bayesian approach we use. Finally, the estimate is the result we get from applying the estimator to our data. That is the posterior distribution or mean estimate of how much fracture risk increases or decreases with a unit change in BMD, holding BMI, Age, and Sex constant. This might be reported as an odds ratio (if using logistic regression in your Bayesian model), or a posterior mean effect size, or a predicted probability of fracture at different levels of BMD.\nNow, let’s build a Bayesian hierarchical model based on these information:\n\n\nBayesian model\nLet, \\(\\text{Status}_i \\in \\{0,1\\}\\): fracture status for individual \\(i\\), i.e, 0 = no fracture and 1 = fracture. Using \\(\\text{BMD}_i\\), \\(\\text{BMI}_i\\), \\(\\text{Age}_i\\), and _i$, we write the probability of fracture \\(P(\\text{Status}_i = 1 \\mid \\cdot)\\) as:\n\\[\nPr(\\text{Status}_i = 1 \\mid \\text{BMD}_i, \\text{BMI}_i, \\text{Age}_i, \\text{Sex}_i) = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}\n\\]\nwhere, we can define \\(\\eta_i\\) as:\n\\[\n\\eta_i = \\beta_0 + \\beta_1 \\cdot \\text{BMD}_i + \\beta_2 \\cdot \\text{BMI}_i + \\beta_3 \\cdot \\text{Age}_i + \\beta_4 \\cdot \\text{Sex}_i\n\\]\nAnother way of writing the above model after some algebraic manipulation:\n\\[\n\\log\\left(\\frac{Pr(\\text{Status}_i = 1\\mid \\text{BMD}_i, \\text{BMI}_i, \\text{Age}_i, \\text{Sex}_i)}{1-Pr(\\text{Status}_i = 1\\mid \\text{BMD}_i, \\text{BMI}_i, \\text{Age}_i, \\text{Sex}_i)}\\right)  = \\eta_i\n\\]\nHence, we write\n\\[\nPr(\\text{Status}_i = 1 \\mid \\text{BMD}_i, \\text{BMI}_i, \\text{Age}_i, \\text{Sex}_i) = \\text{logit}^{-1}(\\eta_i)\n\\]\ni.e.,\n\\[\n\\text{logit}(Pr(\\text{Status}_i = 1 \\mid \\text{BMD}_i, \\text{BMI}_i, \\text{Age}_i, \\text{Sex}_i)) = \\eta_i\n\\]\nThus, the inverse of the logit function \\(\\text{logit}^{-1}(\\eta_i)\\) also known as ‘sigmoid function’ can be written as:\n\\[\n\\text{sig}(\\eta_i) = \\frac{1}{1+\\exp(-\\eta_i)}\n\\]\nNote that, unlike the Bayesian model we developed for a continuous outcome or endpoint variable, there is no error variance in the current modelling structure for the binary outcome variable \\(\\text{Status}\\). Now, we define the prior distributions for the model coefficients as:\n\\[\n\\beta_j \\sim N(\\mu_j, \\sigma^2_j) \\quad \\text{for } j = 0, 1, 2, 3, 4\n\\]\nDenoting \\(\\mathbf{y}_i = \\text{Status}_i\\) and \\(\\bf{x}_i =(\\text{BMD}_i, \\text{BMI}_i, \\text{Age}_i, \\text{Sex}_i)'\\), we write the posterior distribution in matrix and vector notations as:\n\\[\np(\\boldsymbol{\\beta} \\mid \\mathbf{y}, \\mathbf{x}) \\propto p(\\mathbf{y} \\mid \\mathbf{x}, \\boldsymbol{\\beta}) \\cdot p(\\boldsymbol{\\beta})\n\\]\nWhere, \\(\\boldsymbol{\\beta} = (\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\beta_4)'\\) are the model coefficients. This Bayesian model for binary outcome lets us estimate the posterior distribution of the effect of BMD on fractures, while adjusting for BMI, Age, and Sex.\nWe write the data likelihood term \\(p(\\mathbf{y} \\mid \\mathbf{x}, \\boldsymbol{\\beta})\\) for independent observations:\n\n\\[\np(\\mathbf{y} \\mid \\mathbf{x}, \\boldsymbol{\\beta}) = \\prod_{i=1}^N\n\\left( \\frac{1}{1 + e^{-\\mathbf{x}_i\\boldsymbol{\\beta}}} \\right)^{y_i} \\cdot \\left( \\frac{e^{-\\mathbf{x}_i\\boldsymbol{\\beta}}}{1 + e^{-\\mathbf{x}_i\\boldsymbol{\\beta}}} \\right)^{1 - y_i}\n\\]\ni.e.,\n\\[\np(\\mathbf{y} \\mid \\mathbf{x}, \\boldsymbol{\\beta}) =  \\prod_{i=1}^N \\frac{e^{y_i\\boldsymbol{\\beta}'\\mathbf{x}_i}}{1 + e^{\\boldsymbol{\\beta}\\mathbf{x}_i}}\n\\]\nThis is the Bernoulli likelihood for each data point. Assuming a Gaussian prior, we get:\n\\[\np(\\boldsymbol{\\beta}) = N\\left(\\boldsymbol{\\beta} \\mid \\mathbf{0}, \\sigma_{(.)}^2 \\mathbf{I}\\right) = \\frac{1}{(2\\pi \\sigma_{(.)}^2)^{1/2}} \\exp\\left( -\\frac{1}{2\\sigma_{(.)}^2} \\boldsymbol{\\beta}' \\boldsymbol{\\beta} \\right)\n\\]\nThus we write the joint posterior distribution of the model as:\n\\[\np(\\boldsymbol{\\beta} \\mid \\mathbf{y}, \\mathbf{x}) \\propto  \\prod_{i=1}^N \\left[\\frac{e^{y_i\\boldsymbol{\\beta}'\\mathbf{x}_i}}{1 + e^{\\boldsymbol{\\beta}\\mathbf{x}_i}}\\right] \\cdot \\exp\\left( -\\frac{1}{2\\sigma_{(.)}^2} \\boldsymbol{\\beta}' \\boldsymbol{\\beta} \\right)\n\\]\nThat’s the posterior and there is no closed-form expression, because this is a non-conjugate model due to the inverse logit function.",
    "crumbs": [
      "Week 7: **Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_1.html#prior-distributions",
    "href": "M04_1.html#prior-distributions",
    "title": "Week 7: Non-Gaussian",
    "section": "Prior Distributions",
    "text": "Prior Distributions\n\n\nConsideration of a non-informative prior for logistic regression can yield a wide range or variability for the odds ratio. For example, a \\(N(0,10^{2})\\) will provide a standard deviation in the odds ratio scale as \\(\\exp(\\sqrt{100})\\approx 22,000\\), which is a very large number, and consideration of such large number can cause issues with identifiability and numerical stability, as the link function of the logistic regression can explode with large coefficients. Whereas, weakly informative priors balance flexibility and regularisation for the logistic regression scenario. They prevent extreme parameter estimates, improve convergence, and produce more realistic predictions. A paper by Andrew Gelman et al. (Gelman et al. (2008)) addresses some of these key challenges.\nTo model fracture status, let us consider weakly informative prior distributions for \\(\\boldsymbol{\\beta}\\). Now, defining a weakly informative prior is often tricky in such a situation, as it needs to be robust in nature for explaining the predictors. Several options can be considered, and in this unit, we will discuss some of them.\n\n\nNormal Distribution with Small Variance\nOne of the approaches that we can take is to consider a normal prior distribution for \\(\\beta\\) parameter with mean zero and a small variance. Say, \\(\\beta_{(.)} \\sim N(0, 3^2)\\) to define the weakly informative prior for the log-odds ratio. This will give provide the prior range for the model coeficients (i.e., parameters) with hyper-parameter standard deviation as: \\(\\exp(3)\\approx 20\\) in odds ratio scale. Note that, unlike Bayesian model we discussed in our previous lection for continuous variable (i.e., Gaussian distribution); we do not have any variance parameter for the logistic model.\nFor this Bayesian model with small hyper parametr for the prior variance, we can hence write a simpler DAG (only for visualisation, as we will be using the full model in our analysis) using only one predictor variable BMD as below. Note that we use small variance hyper parameter only for \\(\\beta_1\\), as usually we recommend to use large variance for the intercept term of the model (i.e., \\(\\beta_0 \\sim N(0, 10^2)\\)), such that we can get a flexible posterior distribution for the intrcept.",
    "crumbs": [
      "Week 7: **Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_1.html#posterior-results",
    "href": "M04_1.html#posterior-results",
    "title": "Week 7: Non-Gaussian",
    "section": "Posterior Results",
    "text": "Posterior Results\n\n\nWe will now obtain the posterior distribution of the model parameters we developed for the fracture and BMD example. Here, we will use both exposure (i.e, BMD) and confounders (e.g., BMI, Age and Sex) to generate model-based results. In this section, we will use normal prior for the \\(\\beta\\) parameters, where the weakly informative influence of the prior will be demonstrated using the small-variance example we discussed earlier. Note that we will perform a sensitivity analysis of the prior distributions in our next lecture, where we will compare both normal and Cauchy prior distributions.\nRecall, we want to obtain posterior distribution of the model parameters from the joint posterior distribution:\n\n\\[\\begin{align}\np(\\boldsymbol{\\beta} \\mid \\text{Status}, \\text{BMD}, \\text{BMI}, \\text{Age}, \\text{Sex}) &\\propto\np(\\text{Status} \\mid \\text{BMD}, \\text{BMI}, \\text{Age}, \\text{Sex}, \\boldsymbol{\\beta}) \\cdot  p(\\boldsymbol{\\beta})\n\\end{align}\\]\nwhere, \\(\\boldsymbol{\\beta} = (\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\beta_4)'\\) and \\(p(\\boldsymbol{\\beta}) = p(\\beta_0) \\cdot p(\\beta_1) \\cdot p(\\beta_2) \\cdot p(\\beta_3) \\cdot p(\\beta_4)\\), i.e. the joint prior distribution in multiplicative form.\nAssuming weakly informative prior for \\(p(\\beta_1)\\), \\(p(\\beta_2)\\), \\(p(\\beta_3)\\) and \\(p(\\beta_4)\\), we write:\n\n\\(\\beta_1,\\beta_2,\\beta_3,\\beta_4 \\sim N(0, 3^2)\\)\n\nAnd for the intercept \\(p(\\beta_0)\\) we write:\n\n\\(\\beta_0 \\sim N(0, 10^2)\\)\n\nWe impliment the model using ‘brm’ function from ‘brms’ R package as follows:\n\n\nCode\nlibrary(brms)\nlibrary(tidyverse)\n\nbmd_data &lt;- read.csv(\"bmd_drug.csv\")\nbmd_data$bmi &lt;- bmd_data$weight_kg/(bmd_data$height_cm/100)^2\nbmd_data$fracture &lt;- factor(bmd_data$fracture)\nbmd_data$fracture &lt;- relevel(bmd_data$fracture, ref = \"no fracture\")\nbmd_data &lt;- tibble(\n  Status = bmd_data$fracture,\n  BMD = bmd_data$bmdtot_spine,\n  BMI = bmd_data$bmi,\n  Age = bmd_data$age,\n  Sex = as.factor(bmd_data$sex)\n)\n\npriors_normal &lt;- c(\n  prior(normal(0, 3), class = \"b\", coef = \"BMD\"), # N(mean, sd)\n  prior(normal(0, 3), class = \"b\", coef = \"BMI\"), # N(mean, sd)\n  prior(normal(0, 3), class = \"b\", coef = \"Age\"), # N(mean, sd)\n  prior(normal(0, 3), class = \"b\", coef = \"SexM\"), # N(mean, sd)\n  prior(normal(0, 10), class = \"Intercept\") # N(mean, sd)\n)\n\nfracture_model_normal_prior &lt;- brm(\n  formula = Status ~ BMD + BMI + Age + Sex,\n  data = bmd_data,\n  family = bernoulli(link = \"logit\"),\n  prior = priors_normal,\n  chains = 3,\n  iter = 2000,\n  warmup = 1000,\n  cores = 3,\n  seed = 123\n)\n\n\nTo get the prior distributions and related hyper parameters that we implemented in the model, we can use ‘prior_summary’ function, which gives us the following output:\n\n\nCode\nprint(prior_summary(fracture_model_normal_prior, all = FALSE), show_df = FALSE)\n\n\nb_Age ~ normal(0, 3)\nb_BMD ~ normal(0, 3)\nb_BMI ~ normal(0, 3)\nb_SexM ~ normal(0, 3)\nIntercept ~ normal(0, 10)\n\n\n\nMCMC Diagnostics\nThe posterior summaries, MCMC chain and posterior predictive check plots for the model parameters can be obtained as follows. Note that in this example we used 3 MCMC chains, each with 2000 iterations and first 1000 MCMC samples as burn-in.\n\n\nCode\nsummary(fracture_model_normal_prior)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Status ~ BMD + BMI + Age + Sex \n   Data: bmd_data (Number of observations: 1077) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.56      1.57    -3.59     2.58 1.00     3296     2320\nBMD          -2.21      1.02    -4.27    -0.22 1.00     2563     1978\nBMI          -0.23      0.05    -0.33    -0.13 1.00     2128     1703\nAge           0.08      0.01     0.05     0.11 1.00     2485     2011\nSexM         -0.05      0.33    -0.69     0.58 1.00     2597     1958\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nplot(fracture_model_normal_prior)\n\n\n\n\n\n\n\n\n\nCode\nlibrary(bayesplot)\np1 &lt;- pp_check(fracture_model_normal_prior, type=\"bars\")\np2 &lt;- pp_check(fracture_model_normal_prior)\np3 &lt;- pp_check(fracture_model_normal_prior, type = \"ecdf_overlay\")\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, ncol = 3)\n\n\n\n\n\n\n\n\n\nWe can see from the MCMC plots that the chains do not show any particular upward or downward trends, nor any sudden shifts. The effective sample sizes are reasonably large, and the R-hat values for the model parameters are all 1, indicating good convergence. The posterior predictive check also supports convergence, as the replicated values of the outcome variable \\(y\\), generated from the posterior distributions, align well with the observed data. Note that, unlike the bell-shaped curve observed for the continuous outcome variable, we see two spikes at 0 and 1 for the binary outcome.\n\n\nPosterior Odds Ratio\nBy deafult the ‘brm’ model output provides mean estimates for the model parameters in log-odds scale. To get the summary statistics in odds ratios, we need to follow the R code below:\n\n\nCode\n#posterior_summary &lt;- posterior_summary(fracture_model_normal_prior)\n#odds_ratios &lt;- exp(posterior_summary[grep(\"^b_\", rownames(posterior_summary)), ])\n#library(bayesplot)\n#posterior &lt;- as_draws_df(fracture_model_normal_prior)\n#posterior_or &lt;- posterior %&gt;%\n#  dplyr::select(starts_with(\"b_\")) %&gt;%\n#  dplyr::mutate(across(everything(), exp))\n#library(coda)\n#summary(as.mcmc(posterior_or))\n\n# simple way \n#exp(fixef(fracture_model_normal_prior))\n\n# using gtsummary\nlibrary(gtsummary)\nfracture_model_normal_prior %&gt;%\n  tbl_regression(exponentiate = TRUE) %&gt;%\n  bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nexp(Beta)\n95% CI\n\n\n\n\nBMD\n0.11\n0.01, 0.80\n\n\nBMI\n0.80\n0.72, 0.88\n\n\nAge\n1.08\n1.05, 1.11\n\n\nSex\n\n\n\n\n\n\n    F\n—\n—\n\n\n    M\n0.95\n0.50, 1.79\n\n\n\nAbbreviation: CI = Credible Interval\n\n\n\n\n\n\n\n\nCode\nlibrary(jtools)\nplot_summs(fracture_model_normal_prior, exp=TRUE)\n\n\n\n\n\n\n\n\n\nNow we can provide a brief description of the posterior summary based on the estimated odds ratios. Starting with the exposure variable, i.e., bone mineral density (BMD), where we can see the mean odds ratio is 0.11, with a 95% credible interval ranging from 0.01 to 0.80. This suggests, each unit increase in BMD is associated with an 89% decrease in the odds of having a fracture, compared to not having a fracture. This also suggests a strong effect of higher BMD on fracture risk, since the entire credible interval is below 1, and we can be confident that the association between higher BMD and lower odds of fracture is statistically meaningful. In practical terms, individuals with higher bone density are substantially less likely to experience fractures.\nWe can also explain the coefficients of the confounder variables. For example, the mean odds ratio for BMI is 0.8, with a credible interval from 0.72 to 0.88. This also indicates a protective effect, albeit more modest than that of BMD. The credible interval does not contain 1 and also below 1, indicating that higher BMI is associated with lower odds of fracture in a statistically credible manner. This effect might be attributed to increased cushioning during falls or greater bone loading associated with higher body mass.\nWe can also see, age has a mean odds ratio of 1.08, with a 95% credible interval of 1.05 to 1.11. This suggests that increasing age is associated with higher odds of experiencing a fracture, that is, a one-year increase in age is associated with on average an 8% increase in the odds of having a fracture, holding all other variables constant. The interval indicates a consistent and statistically significant risk-increasing effect. This result aligns with existing literature, as aging is often linked with declining bone density and higher fall risk.\nIn contrast, the effect of sex (male) shows a mean odds ratio of 1.0022, with a 95% credible interval spanning from 0.5 to 1.8. This interval includes 1 and ranges widely, suggesting that being male has no consistent or statistically significant effect on the odds of having a fracture compared to being female. In this model, sex does not appear to be a reliable predictor of fracture risk.",
    "crumbs": [
      "Week 7: **Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_1.html#bayesian-vs.-frequentist",
    "href": "M04_1.html#bayesian-vs.-frequentist",
    "title": "Week 7: Non-Gaussian",
    "section": "Bayesian vs. Frequentist",
    "text": "Bayesian vs. Frequentist\n\n\nWe explained the effect of the exposure variable BMD, i.e., for the model parameter \\(\\beta_1\\) using a weakly informative prior \\(\\beta_1\\sim N(0,3^2)\\). Suppose previous studies suggest a moderate protective effect of BMD on fracture risk (e.g., OR apprimately 0.6 per unit increase). Following this we can write the log odds ratio of 0.6 as: \\(\\log(0.6) \\approx -0.51\\). From the prior knowledge, we believe that BMD likely reduces fracture risk, with moderate confidence, and thus we write \\(\\beta_1 \\sim N(-0.5, 0.25^2)\\). Another scenario could be no past knowledge about \\(\\beta_1\\) and thus we write \\(\\beta_1 \\sim \\mathcal{N}(0, 100^2)\\), i.e., we are trying not to influence the posterior with our prior at all.\n\n\nCode\nlibrary(ggplot2)\nx &lt;- seq(-10, 10, length.out = 1000)\ndens_weakly &lt;- dnorm(x, mean = 0, sd = 3)\ndens_informative &lt;- dnorm(x, mean = -0.5, sd = 0.25)\ndens_flat &lt;- dnorm(x, mean = 0, sd = 100)\npriors_df &lt;- data.frame(\n  beta1 = rep(x, 3),\n  density = c(dens_weakly, dens_informative, dens_flat),\n  Prior = factor(rep(c(\"N(0, 3^2)\",\n                       \"N(-0.5, 0.25^2)\",\n                       \"N(0, 100^2)\"),\n                     each = length(x)))\n)\nggplot(priors_df, aes(x = beta1, y = density, color = Prior)) +\n  geom_line(size = 1) +\n  labs(title = expression(\"Prior Distributions for \" * beta[1]),\n       x = expression(beta[1]),\n       y = \"Density\") +\n  xlim(-5, 5) +  \n  theme_minimal() +\n  theme(text = element_text(size = 14),\n        legend.title = element_blank(),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nNow, let us now compare the posterior results for these different prior scenarios with the results we obtain from the frequentist method. In particular, here for comparison purpose we only use the BMD predictor variable for all scenarios.\n\n\nCode\nlibrary(brms)\nlibrary(tidyverse)\n\nbmd_data &lt;- read.csv(\"bmd_drug.csv\")\nbmd_data$bmi &lt;- bmd_data$weight_kg/(bmd_data$height_cm/100)^2\nbmd_data$fracture &lt;- factor(bmd_data$fracture)\nbmd_data$fracture &lt;- relevel(bmd_data$fracture, ref = \"no fracture\")\nbmd_data &lt;- tibble(\n  Status = bmd_data$fracture,\n  BMD = bmd_data$bmdtot_spine,\n)\nbmd_glm &lt;- glm(Status ~ BMD, data = bmd_data, family=binomial)\n#library(jtools)\n#summ(bmd_glm)\npriors_weakly &lt;- c(\n  prior(normal(0, 3), class = \"b\", coef = \"BMD\"), # N(mean, sd)\n  prior(normal(0, 10), class = \"Intercept\") # N(mean, sd)\n)\npriors_info &lt;- c(\n  prior(normal(-0.5, 0.25), class = \"b\", coef = \"BMD\"), # N(mean, sd)\n  prior(normal(0, 10), class = \"Intercept\") # N(mean, sd)\n)\npriors_noninfo &lt;- c(\n  prior(normal(0, 100), class = \"b\", coef = \"BMD\"), # N(mean, sd)\n  prior(normal(0, 10), class = \"Intercept\") # N(mean, sd)\n)\nfracture_model_weakly &lt;- brm(\n  formula = Status ~ BMD,\n  data = bmd_data,\n  family = bernoulli(link = \"logit\"),\n  prior = priors_weakly,\n  chains = 2,\n  iter = 2000,\n  warmup = 1000,\n  cores = 2,\n  seed = 123\n)\nfracture_model_info &lt;- brm(\n  formula = Status ~ BMD,\n  data = bmd_data,\n  family = bernoulli(link = \"logit\"),\n  prior = priors_info,\n  chains = 2,\n  iter = 2000,\n  warmup = 1000,\n  cores = 2,\n  seed = 123\n)\nfracture_model_noninfo &lt;- brm(\n  formula = Status ~ BMD,\n  data = bmd_data,\n  family = bernoulli(link = \"logit\"),\n  prior = priors_noninfo,\n  chains = 2,\n  iter = 2000,\n  warmup = 1000,\n  cores = 2,\n  seed = 123\n)\n\n#library(xfun)\n#library(modelsummary)\n#modelsummary(list(\"Weakly Informative\" = fracture_model_weakly, \n#                  \"Informative\" = fracture_model_info,\n#                  \"Non-informative\" = fracture_model_noninfo,\n#                  \"Frequentist\" = bmd_glm))\n\nlibrary(gtsummary)\nfracture_model_weakly %&gt;%\n  tbl_regression(exponentiate = TRUE) %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"**Weakly Informative: N(0, 3^2)**\")\n\n\n\n\n\n\nWeakly Informative: N(0, 3^2)\n\n\nCharacteristic\nexp(Beta)\n95% CI\n\n\n\n\nBMD\n0.01\n0.00, 0.07\n\n\n\nAbbreviation: CI = Credible Interval\n\n\n\n\n\n\n\n\nCode\nfracture_model_info %&gt;%\n  tbl_regression(exponentiate = TRUE) %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"**Informative: N(-0.5, 0.25^2)**\")\n\n\n\n\n\n\nInformative: N(-0.5, 0.25^2)\n\n\nCharacteristic\nexp(Beta)\n95% CI\n\n\n\n\nBMD\n0.46\n0.29, 0.75\n\n\n\nAbbreviation: CI = Credible Interval\n\n\n\n\n\n\n\n\nCode\nfracture_model_noninfo %&gt;%\n  tbl_regression(exponentiate = TRUE) %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"**Non-informative: N(0, 100^2)**\")\n\n\n\n\n\n\nNon-informative: N(0, 100^2)\n\n\nCharacteristic\nexp(Beta)\n95% CI\n\n\n\n\nBMD\n0.01\n0.00, 0.05\n\n\n\nAbbreviation: CI = Credible Interval\n\n\n\n\n\n\n\n\nCode\nbmd_glm %&gt;%\n  tbl_regression(exponentiate = TRUE) %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"**Frequentist**\")\n\n\n\n\n\n\nFrequentist\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\nBMD\n0.01\n0.00, 0.05\n&lt;0.001\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\nAcross all models, we look at the odds ratio (OR) for BMD, which helps us understand how a change in BMD is associated with the likelihood of fractures. When examining the relationship between bone mineral density (BMD) and fracture risk, we find that the choice of prior has a substantial impact on the estimated effect.\nFrequentist vs. Weakly Informative and Non-informative\nUsing a weakly informative prior, we estimate the OR to be 0.01 with a 95% credible interval ranging from 0.00 to 0.08. This suggests a very strong protective effect of BMD against fractures, as the OR is far below 1 and the credible interval is tightly clustered around zero. The non-informative prior yields a similar result, with an OR of 0.01 and a credible interval from 0.00 to 0.05. This again indicates a strong inverse relationship between BMD and fracture risk. The frequentist model, which does not incorporate prior information, produces nearly identical results: an OR of 0.01 with a 95 percent confidence interval from 0.00 to 0.05.\nAt first glance, we might expect a non-informative prior to always lead to wider intervals because it places no constraints on the estimates, while a weakly informative prior should introduce some regularisation and potentially narrow the uncertainty. However, the reality is more nuanced.\nFor example, a weakly informative prior doesn’t dominate the likelihood, but it still pushes estimates toward reasonable regions of the parameter space (e.g., away from extremes). If the data is highly skewed or sparse, the non-informative prior may allow the posterior to ‘hug’ the observed data more tightly, leading to a narrower interval, but this is also potentially a less reliable estimate due to the data sparseness. In our BMD data situation, we can see the sparseness of the outcome variable ‘Status’, where the observed probability for not having fracture is more than 95% compared to having fracture, which is 4.6%.\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra)\n\n#bmd_data %&gt;%\n#  group_by(Status) %&gt;%\n#  summarise(Count = n()) %&gt;%\n#  mutate(Proportion = Count / sum(Count)) %&gt;%\n#  mutate(Percentage = 100* (Count / sum(Count)))\n\nstatus_table &lt;- bmd_data %&gt;%\n  group_by(Status) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Proportion = round(Count / sum(Count), 4)) %&gt;%\n  mutate(Percentage = round(100*(Count / sum(Count)), 1))\n\nstatus_table %&gt;%\n  mutate(Status = ifelse(Status == \"no fracture\", \"No Fracture\", \"Fracture\")) %&gt;%\n  kable(caption = \"Fracture Status Summary\") %&gt;%\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nFracture Status Summary\n\n\nStatus\nCount\nProportion\nPercentage\n\n\n\n\nNo Fracture\n1027\n0.9536\n95.4\n\n\nFracture\n50\n0.0464\n4.6\n\n\n\n\n\n\n\nCode\nggplot(bmd_data, aes(x = factor(Status), y = BMD)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(x = \"Fracture Status\", y = \"BMD\", title = \"BMD by Fracture Status\") +\n  scale_x_discrete(labels = c(\"0\" = \"No Fracture\", \"1\" = \"Fracture\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBy contrast, the weakly informative prior introduces some skepticism about extreme effects, which can spread the posterior distribution out more, especially if the data strongly suggest an extreme value (like an OR close to 0). This tension between the prior and the data can widen the credible interval, not narrow it.\nAnother possible reason for having a wider credible interval for weakly informative prior is related to the concentration centered closer to no effect (e.g., log-odds = 0, or OR = 1), the resulting posterior distribution is then often more regularised, and its tails can be thicker, reflecting more conservative uncertainty. This leads the credible intervals wider, not because the prior adds noise, but because it prevents overconfidence in estimates that the data alone might suggest too strongly.\nFurthermore, sometimes, if your data suggest a very strong effect (e.g., OR \\(\\approx\\) 0.01), and the weakly informative prior says ‘that is unlikely, but not impossible,’ the posterior has to balance these two inputs. That can result in a flattened posterior compared to the non-informative one, thus, provides a wider interval.\nFrequentist vs. Informative Prior\nNow, when the model uses an informative prior, we see a different picture. The estimated odds ratio (OR) for BMD is 0.46, with a 95% credible interval ranging from 0.29 to 0.72. This still indicates a protective effect, since the OR is below 1, but the effect is far less extreme than in the other models. The informative prior likely reflects prior knowledge or clinical evidence suggesting that such a strong effect is unlikely. As a result, the model adjusts the estimate toward more moderate values. We can also see that this estimate differs significantly from the frequentist estimate, which is based purely on the observational data.\nThis result shows how influential priors can be in Bayesian analysis. When we apply informative priors based on previous knowledge or research, the model becomes more conservative and less likely to overstate effects. Here, the informative prior leads to a more moderate and arguably more realistic interpretation of the effect of BMD on fracture risk.",
    "crumbs": [
      "Week 7: **Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_1.html#summary",
    "href": "M04_1.html#summary",
    "title": "Week 7: Non-Gaussian",
    "section": "Summary",
    "text": "Summary\nOverall, in today’s lecture, we discussed non-Gaussian Bayesian models, with a particular focus on binary outcome variables. We covered strategies for model development by identifying exposure and confounding variables. Additionally, we explored approaches for selecting prior distributions and compared models using informative, weakly informative, and non-informative priors, alongside the frequentist approach. This comparison emphasised the importance of carefully choosing priors in Bayesian modelling and how frequentist method can lead to a less reliable estimate if data shows a sparse pattern.",
    "crumbs": [
      "Week 7: **Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_1.html#live-tutorial-and-discussion",
    "href": "M04_1.html#live-tutorial-and-discussion",
    "title": "Week 7: Non-Gaussian",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 7: **Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_1.html#tutorial-exercises",
    "href": "M04_1.html#tutorial-exercises",
    "title": "Week 7: Non-Gaussian",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.\n\n\n\n\nGelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” The Annals of Applied Statistics 2 (4): 1360–83.",
    "crumbs": [
      "Week 7: **Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_2.html",
    "href": "M04_2.html",
    "title": "Week 8: More on Non-Gaussian",
    "section": "",
    "text": "Learnings\n– LO2: Demonstrate how to specify and fit simple Bayesian models with appropriate attention to the role of the prior distribution and the data model.\n– LO5: Engage in specifying, checking and interpreting Bayesian statistical analyses in practical problems using effective communication with health and medical investigators.\nBy the end of this week you should be able to:\n– Conduct a sensitivity analysis for a non-Gaussian Bayesian model with varying prior distributions.\n– Understand non-Gaussian Bayesian model with count outcome\n– Formulate and interpret real-life problems in Bayesian context.",
    "crumbs": [
      "Week 8: **More on Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_2.html#learnings",
    "href": "M04_2.html#learnings",
    "title": "Week 8: More on Non-Gaussian",
    "section": "",
    "text": "Outcomes\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 8: **More on Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_2.html#sensitivity-analysis",
    "href": "M04_2.html#sensitivity-analysis",
    "title": "Week 8: More on Non-Gaussian",
    "section": "Sensitivity Analysis",
    "text": "Sensitivity Analysis\n\n\nRecall, in our last lecture we demonstrated how to use Bayesian approach to model binary outcome variable, i.e., logistic regression. We also explained how we can define weakly informative, informative and non-informative prior distributions for the coefficients of the predictor variables in the model. In particular, we used normal distribution for this context.\nNow, in today’s lecture, we will explore couple of more suggestions for the prior distributions focusing on the weakly informative prior, and then we will compare the posterior results. We will also explore how sensitive is the results based on the prior distribution assumptions.\n\nCauchy Distribution\nAnother suggestion for the weakly informative prior distribution for \\(\\beta\\) parameters in logistic regression is the Cauchy prior with mean zero and scale 2.5 (Gelman et al. (2008)). The suggestion is to assign the Cauchy prior to each of the coefficients in the logistic regression except the intercept (i.e., \\(\\beta_0\\)) term. Furthermore, it is recommended to standardise continuous predictor variables when using a Cauchy prior—specifically, \\(\\text{Cauchy}(\\mu = 0, \\tau = 2.5)\\), because this helps ensure that the resulting change in the log-odds (logit scale) remains within a reasonable range, typically less than 5, when the predictor moves from one standard deviation below the mean to one standard deviation above.\nWe can draw DAG based on the Cauchy prior for only one predictor variable BMD as follows. Similarly, we can see that for the intercept \\(\\beta_0\\) we consider a normal prior distribution with a relatively wider variance.\n\n\n\n\n\n\n\n\nPrior Density Plots\nBelow, we draw two weakly informative prior density plots based on the \\(\\text{Cauchy}(0, 2.5)\\) and \\(N(0, 3^2)\\) distributions. The Cauchy distribution is represented by the blue curve. We can see, as discussed, one of the key features of the Cauchy distribution is its heavy tails. This means that it allows for more extreme values compared to other distributions like the Normal distribution. In practical terms, this implies that the Cauchy distribution is more flexible and can accommodate larger deviations from the mean. The normal distribution, showed by the red curve, has a mean of 0 and a standard deviation of 3. Unlike the Cauchy distribution, the Normal distribution is more concentrated around the mean. This means that it assumes values of \\(\\beta\\) are more likely to be close to the mean, with fewer extreme values. The tails of the Normal distribution are lighter, indicating that it is less likely to produce outliers compared to the Cauchy distribution.\nThe choice between these two distributions sometimes can significantly impact the results of a Bayesian analysis. The Cauchy distribution, with its heavy tails, is more robust to outliers and can handle more extreme values. This can be advantageous in situations where you expect the parameter \\(\\beta\\) to have a wide range of possible values. Whereas, the normal distribution, with its lighter tails, is more suitable when you have strong prior beliefs that the parameter values will be close to the mean zero.\n\n\nCode\nlibrary(ggplot2)\nx &lt;- seq(-20, 20, length.out = 1000)\nnormal_density &lt;- dnorm(x, mean = 0, sd = 3)\ncauchy_density &lt;- dcauchy(x, location = 0, scale = 2.5)\npriors_df &lt;- data.frame(\n  x = rep(x, 2),\n  density = c(normal_density, cauchy_density),\n  distribution = factor(rep(c(\"Normal (mean = 0, sd = 3)\", \"Cauchy (mean = 0, scale = 2.5)\"), each = length(x)))\n)\nggplot(priors_df, aes(x = x, y = density, color = distribution)) +\n  geom_line(size = 1.2) +\n  labs(title = \"Comparison of Prior Distributions\",\n       x = expression(beta),\n       y = \"Density\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"steelblue\", \"firebrick\")) +\n  theme(\n    legend.title = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nPosterior Comparison\nWe obtain posterior distributions of the model parameters based on both prior options, i.e., \\(\\beta_1\\sim N(0, 3^2)\\) and \\(\\beta_1\\sim \\text{Cauchy}(0, 2.5)\\). We run the Bayesian models using following R code:\n\n\nCode\nlibrary(brms)\nlibrary(tidyverse)\n\nbmd_data &lt;- read.csv(\"bmd_drug.csv\")\nbmd_data$bmi &lt;- bmd_data$weight_kg/(bmd_data$height_cm/100)^2\nbmd_data$fracture &lt;- factor(bmd_data$fracture)\nbmd_data$fracture &lt;- relevel(bmd_data$fracture, ref = \"no fracture\")\nbmd_data &lt;- tibble(\n  Status = bmd_data$fracture,\n  BMD = bmd_data$bmdtot_spine,\n)\npriors_normal &lt;- c(\n  prior(normal(0, 3), class = \"b\", coef = \"BMD\"), # N(mean, sd)\n  prior(normal(0, 10), class = \"Intercept\") # N(mean, sd)\n)\nfracture_model_normal_prior &lt;- brm(\n  formula = Status ~ BMD,\n  data = bmd_data,\n  family = bernoulli(link = \"logit\"),\n  prior = priors_normal,\n  chains = 3,\n  iter = 2000,\n  warmup = 1000,\n  cores = 3,\n  seed = 123\n)\n#summary(fracture_model_normal_prior)\n#plot(fracture_model_normal_prior)\n\nprint(prior_summary(fracture_model_normal_prior, all = FALSE), show_df = FALSE)\n\n\nb_BMD ~ normal(0, 3)\nIntercept ~ normal(0, 10)\n\n\nAnd for Cauchy prior distribution we get:\n\n\nCode\nlibrary(brms)\nlibrary(tidyverse)\n\nbmd_data &lt;- read.csv(\"bmd_drug.csv\")\nbmd_data$bmi &lt;- bmd_data$weight_kg/(bmd_data$height_cm/100)^2\nbmd_data$fracture &lt;- factor(bmd_data$fracture)\nbmd_data$fracture &lt;- relevel(bmd_data$fracture, ref = \"no fracture\")\nbmd_data &lt;- tibble(\n  Status = bmd_data$fracture,\n  BMD = bmd_data$bmdtot_spine,\n)\npriors_cauchy &lt;- c(\n  prior(cauchy(0, 2.5), class = \"b\", coef = \"BMD\"), # N(mean, sd)\n  prior(normal(0, 10), class = \"Intercept\") # N(mean, sd)\n)\nfracture_model_cauchy_prior &lt;- brm(\n  formula = Status ~ BMD,\n  data = bmd_data,\n  family = bernoulli(link = \"logit\"),\n  prior = priors_cauchy,\n  chains = 3,\n  iter = 2000,\n  warmup = 1000,\n  cores = 3,\n  seed = 123\n)\n#summary(fracture_model_cauchy_prior)\n#plot(fracture_model_cauchy_prior)\nprint(prior_summary(fracture_model_cauchy_prior, all = FALSE), show_df = FALSE)\n\n\nb_BMD ~ cauchy(0, 2.5)\nIntercept ~ normal(0, 10)\n\n\nNow we get the posterior summary statistics and MCMC trace plots as follows:\n\n\nCode\nsummary(fracture_model_normal_prior)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Status ~ BMD \n   Data: bmd_data (Number of observations: 1077) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.77      0.79    -0.79     2.33 1.00     1654     1781\nBMD          -4.48      0.96    -6.43    -2.55 1.00     1353     1656\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nsummary(fracture_model_cauchy_prior)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Status ~ BMD \n   Data: bmd_data (Number of observations: 1077) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.88      0.85    -0.79     2.52 1.00     1286     1398\nBMD          -4.61      1.03    -6.66    -2.59 1.00     1150     1207\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nlibrary(bayesplot)\np1 &lt;- mcmc_trace(as_draws_array(fracture_model_normal_prior), \n           pars = c(\"b_BMD\")) + \n  ggtitle(\"Trace Plot: Normal Prior\")\np2 &lt;- mcmc_trace(as_draws_array(fracture_model_cauchy_prior), \n           pars = c(\"b_BMD\")) + \n  ggtitle(\"Trace Plot: Cauchy Prior\")\nlibrary(gridExtra)\ngrid.arrange(p1,p2)\n\n\n\n\n\n\n\n\n\nCode\n#library(bayesplot)\n#posterior_normal &lt;- as_draws_df(fracture_model_normal_prior)\n#posterior_cauchy &lt;- as_draws_df(fracture_model_cauchy_prior)\n#mcmc_areas(\n#  list(Normal = posterior_normal,\n#       Cauchy = posterior_cauchy),\n#  pars = c(\"b_BMI\", \"b_Age\", \"b_SexM\", \"b_BMD\"),\n#  prob = 0.95\n#) + ggtitle(\"Posterior Distributions: Normal vs. Cauchy Priors\")\n# LOO model comparison\n#loo_normal &lt;- loo(fracture_model_normal_prior)\n#loo_cauchy &lt;- loo(fracture_model_cauchy_prior)\n#loo_compare(loo_normal, loo_cauchy)\n\nlibrary(gtsummary)\nfracture_model_normal_prior %&gt;%\n  tbl_regression(exponentiate = TRUE) %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"**Posterior Odds Ratio for Fracture Model (Normal Prior)**\")\n\n\n\n\n\n\nPosterior Odds Ratio for Fracture Model (Normal Prior)\n\n\nCharacteristic\nexp(Beta)\n95% CI\n\n\n\n\nBMD\n0.01\n0.00, 0.08\n\n\n\nAbbreviation: CI = Credible Interval\n\n\n\n\n\n\n\n\nCode\nlibrary(gtsummary)\nfracture_model_cauchy_prior %&gt;%\n  tbl_regression(exponentiate = TRUE) %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"**Posterior Odds Ratio for Fracture Model (Cauchy Prior)**\")\n\n\n\n\n\n\nPosterior Odds Ratio for Fracture Model (Cauchy Prior)\n\n\nCharacteristic\nexp(Beta)\n95% CI\n\n\n\n\nBMD\n0.01\n0.00, 0.07\n\n\n\nAbbreviation: CI = Credible Interval\n\n\n\n\n\n\n\n\nWe can see that both normal and Cauchy priors are providing similar posterior estimates, where Cauchy prior provides a narrower band for the 95% credible interval reflecting the narrower prior assuamption for the Cauchy distribution.",
    "crumbs": [
      "Week 8: **More on Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_2.html#prior-hierarchy",
    "href": "M04_2.html#prior-hierarchy",
    "title": "Week 8: More on Non-Gaussian",
    "section": "Prior Hierarchy",
    "text": "Prior Hierarchy\n\n\nAnother approach that we can take in Bayesian logistic modelling is to incorporate a hierarchy of prior distributions. This hierarchical setup allows for more flexibility and adaptability in capturing the underlying uncertainty and scale of model parameters. Use of the prior hierarchy is a form of adaptive shrinkage that works well in sparse signal settings and interpretable modelling, especially in medical and health data.\nWhy Use a Hyperprior? (Conceptual Justification)\nA fixed prior variance for the model parameter \\(\\beta\\) is arbitrary, and different datasets (or predictors) require different levels of regularisation. A hyperprior on the prior variance hyper-parameter lets the data inform the amount of regularisation. Now, regularisation mainly improves generalisation. For example, overly flexible models can overfit, especially with limited data; hence regularisation can provide a stability in such situation. Furthermore, by introducing a hyperprior on the variance hyper-parameter of the prior can regularise the coefficient size dynamically, which helps with out-of-sample prediction and robust inference. Below, we provide some practical reasons to use a hyperprior on the \\(\\beta\\) parameters of a Bayesian model:\n——————————– | ————————————————————– |\nAvoid overfitting | Prevents overconfident estimates in small datasets |\nNo need to pre-specify variance | Avoids hand-tuning prior scale |\nSmooths posterior | Helps MCMC converge more stably |\nImplimentation Scenario\nOne way to implement this is by introducing multiple hierarchical structures based on different types of priors. Specifically, we can explore two distinct hierarchical formulations for the coefficients in a Bayesian logistic regression, that we have already discussed:\n\nusing a normal prior with an exponential hyperprior on its scale parameter \\(\\sigma\\), and\nusing a Cauchy prior with an exponential hyperprior on its scale parameter \\(\\tau\\).\n\n\nNormal-Exponential Hierarchy\nIn the first setup, i.e., normal-exponential hierarchy, the regression coefficient (say for the exposure variable BMD) is assumed to follow a normal distribution, such that \\(\\beta_1 \\sim N(0, \\sigma^2)\\). Instead of fixing \\(\\sigma\\), we place an exponential hyperprior on it: \\(\\sigma \\sim \\text{Exp}(\\lambda)\\), and we consider different values for the hyper-hyper parameter \\(\\lambda\\). For example, considering the hyper-hyper parameter \\(\\lambda=1\\) allows the model to learn an appropriate scale from the data, shrinking coefficients more aggressively when \\(\\sigma\\) is small and allowing more flexibility when \\(\\sigma\\) is large. This structure introduces a level of adaptivity that can help prevent overfitting while still permitting the model to express sufficient complexity when warranted by the data.\n\n\n\n\n\n\n\n\nCode\nlibrary(brms)\nstanvars &lt;- stanvar(scode = \"real&lt;lower=0&gt; sigma;\", block = \"parameters\") +\n            stanvar(scode = \"sigma ~ exponential(1);\", block = \"model\")\npriors &lt;- c(\n  prior(normal(0, sigma), class = \"b\"),\n  prior(normal(0, 10), class = \"Intercept\")\n)\nfracture_model_normal_hierarchy &lt;- brm(\n  formula = Status ~ BMD,\n  data = bmd_data,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  stanvars = stanvars,\n  chains = 3,\n  iter = 2000,\n  warmup = 1000,\n  cores = 3,\n  seed = 123\n)\nprint(prior_summary(fracture_model_normal_hierarchy, all = FALSE), show_df = FALSE)\n\n\nb ~ normal(0, sigma)\nIntercept ~ normal(0, 10)\n\n\nCode\nsummary(fracture_model_normal_hierarchy)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Status ~ BMD \n   Data: bmd_data (Number of observations: 1077) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.57      0.88    -1.15     2.28 1.00     1595     1820\nBMD          -4.22      1.07    -6.36    -2.14 1.00     1431     1691\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nlibrary(gtsummary)\nfracture_model_normal_hierarchy %&gt;%\n  tbl_regression(exponentiate = TRUE) %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"**Posterior Odds Ratio (Normal-Exponential Hierarchy)**\")\n\n\n\n\n\n\nPosterior Odds Ratio (Normal-Exponential Hierarchy)\n\n\nCharacteristic\nexp(Beta)\n95% CI\n\n\n\n\nBMD\n0.01\n0.00, 0.12\n\n\n\nAbbreviation: CI = Credible Interval\n\n\n\n\n\n\n\n\nFrom the posterior summary we can see that the use of normal-exponential hierarchy adds a wider credible interval, i.e., OR=(0.0,0.12) for the model parameter.\nNote that, by default we can’t recall the prior distribution hierarchy from the ‘brms’ output. For interested students, I would suggest to explore the stan code behined this model, which is:\n\n\nCode\nlibrary(brms)\nstancode(fracture_model_normal_hierarchy)\n\n\n// generated with brms 2.22.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int&lt;lower=1&gt; Kc;  // number of population-level effects after centering\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  matrix[N, Kc] Xc;  // centered version of X without an intercept\n  vector[Kc] means_X;  // column means of X before centering\n  for (i in 2:K) {\n    means_X[i - 1] = mean(X[, i]);\n    Xc[, i - 1] = X[, i] - means_X[i - 1];\n  }\n}\nparameters {\n  vector[Kc] b;  // regression coefficients\n  real Intercept;  // temporary intercept for centered predictors\n  real&lt;lower=0&gt; sigma;\n}\ntransformed parameters {\n  real lprior = 0;  // prior contributions to the log posterior\n  lprior += normal_lpdf(b | 0, sigma);\n  lprior += normal_lpdf(Intercept | 0, 10);\n}\nmodel {\n  sigma ~ exponential(1);\n  // likelihood including constants\n  if (!prior_only) {\n    target += bernoulli_logit_glm_lpmf(Y | Xc, Intercept, b);\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n}\n\n\n\n\nCauchy-Exponential Hierarchy\nIn the second hierarchical approach, i.e., Cauchy-exponential hierarchy, we adopt a heavy-tailed Cauchy prior for each coefficient: \\(\\beta \\sim \\text{Cauchy}(0, \\tau)\\). An then to regulate the scale \\(\\tau\\), we again place an exponential hyperprior: \\(\\tau \\sim \\text{Exp}(\\lambda)\\). This configuration allows the model to dynamically adapt the amount of shrinkage, with the heavy tails of the Cauchy helping to preserve large coefficients when needed, while the exponential hyperprior controls global shrinkage.\n\n\n\n\n\n\n\n\nCode\nlibrary(brms)\nstanvars &lt;- stanvar(scode = \"real&lt;lower=0&gt; tau;\", block = \"parameters\") +\n            stanvar(scode = \"tau ~ exponential(1);\", block = \"model\")\npriors &lt;- c(\n  prior(cauchy(0, tau), class = \"b\"),\n  prior(normal(0, 10), class = \"Intercept\")\n)\nfracture_model_cauchy_hierarchy &lt;- brm(\n  formula = Status ~ BMD,\n  data = bmd_data,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  stanvars = stanvars,\n  chains = 3,\n  iter = 2000,\n  warmup = 1000,\n  cores = 3,\n  seed = 123\n)\nprint(prior_summary(fracture_model_cauchy_hierarchy, all = FALSE), show_df = FALSE)\n\n\nb ~ cauchy(0, tau)\nIntercept ~ normal(0, 10)\n\n\nCode\nsummary(fracture_model_cauchy_hierarchy)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Status ~ BMD \n   Data: bmd_data (Number of observations: 1077) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.82      0.84    -0.85     2.43 1.00     1799     2110\nBMD          -4.54      1.02    -6.50    -2.51 1.00     1653     1941\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nlibrary(gtsummary)\nfracture_model_cauchy_hierarchy %&gt;%\n  tbl_regression(exponentiate = TRUE) %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"**Posterior Odds Ratio (Cauchy-Exponential Hierarchy)**\")\n\n\n\n\n\n\nPosterior Odds Ratio (Cauchy-Exponential Hierarchy)\n\n\nCharacteristic\nexp(Beta)\n95% CI\n\n\n\n\nBMD\n0.01\n0.00, 0.08\n\n\n\nAbbreviation: CI = Credible Interval\n\n\n\n\n\n\n\n\nAs expected, we can see for the Cauchy-exponential hierarchy the 95% credible interval for the OR is estimates as OR=(0.0,0.08). Similarly, to check the stan code behined this we write:\n\n\nCode\nlibrary(brms)\nstancode(fracture_model_cauchy_hierarchy)\n\n\n// generated with brms 2.22.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int&lt;lower=1&gt; Kc;  // number of population-level effects after centering\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  matrix[N, Kc] Xc;  // centered version of X without an intercept\n  vector[Kc] means_X;  // column means of X before centering\n  for (i in 2:K) {\n    means_X[i - 1] = mean(X[, i]);\n    Xc[, i - 1] = X[, i] - means_X[i - 1];\n  }\n}\nparameters {\n  vector[Kc] b;  // regression coefficients\n  real Intercept;  // temporary intercept for centered predictors\n  real&lt;lower=0&gt; tau;\n}\ntransformed parameters {\n  real lprior = 0;  // prior contributions to the log posterior\n  lprior += cauchy_lpdf(b | 0, tau);\n  lprior += normal_lpdf(Intercept | 0, 10);\n}\nmodel {\n  tau ~ exponential(1);\n  // likelihood including constants\n  if (!prior_only) {\n    target += bernoulli_logit_glm_lpmf(Y | Xc, Intercept, b);\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n}\n\n\nThese two hierarchical strategies offer different trade-offs in terms of regularisation and flexibility. The normal-exponential hierarchy is more traditional and is often preferred for more stable, well-behaved datasets, while the Cauchy-exponential hierarchy provides robustness and flexibility in the presence of outliers or sparse, large signals. Comparing these hierarchical approaches in practice can reveal which formulation better suits a given problem, depending on the structure and noisiness of the data.",
    "crumbs": [
      "Week 8: **More on Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_2.html#bayesian-model-for-count-data",
    "href": "M04_2.html#bayesian-model-for-count-data",
    "title": "Week 8: More on Non-Gaussian",
    "section": "Bayesian Model for Count Data",
    "text": "Bayesian Model for Count Data\n\n\nIn this section, we will model count data using Bayesian methods. Count data, which typically represent the number of occurrences of an event within a fixed period or space. The Bayesian framework provides a flexible and robust methodology for modelling such data.\n\nICU Data\nLet us explain the Intensive Care Unit (ICU) data. Here, the variable ‘Ndays’ represents the number of days a child under five years old has been admitted to the ICU. This variable reflects both the clinical severity of the patient’s condition and potentially other factors influencing the duration of care. Understanding what contributes to longer or shorter ICU stays can provide insights into patient outcomes and inform clinical decision-making.\nSeveral key variables can be included as potential predictors in the model. Sex is a categorical variable with two levels: Male and Female. Admission type is another categorical variable, distinguishing between Planned and Not planned (i.e., emergency or unplanned) ICU admissions. Planned admissions typically occur in a controlled setting (such as postoperative care), whereas unplanned admissions may result from sudden health deterioration, potentially indicating more severe conditions. Age is a continuous variable measured in weeks, appropriate for the pediatric population under study (children under 5 years of age). Younger children may have different recovery trajectories and risks compared to older ones within this age bracket, making age an important control variable in the analysis. Finally, PRISM refers to the Pediatric Risk of Mortality score, a widely used clinical measure of illness severity in pediatric ICU patients. This score is measured in continuous scale and from literature we have seen that it correlates strongly with mortality risk. Lower scores (0–10) are typically associated with lower mortality rates (e.g., around 10.2%), while higher scores (21–30) correspond to significantly increased mortality risk (e.g., approximately 73.8%), with scores above 30 linked to even greater risk. As such, the PRISM score is expected to be a strong predictor of ICU length of stay, with higher scores likely corresponding to longer and more resource-intensive admissions.\n\n\nCode\nlibrary(brms)\nlibrary(tidyverse)\nicu_data &lt;- read.csv(\"icu_days_restricted.csv\")\nicu_data &lt;- tibble(\n  Ndays = icu_data$ndays,\n  Sex = factor(icu_data$sex),\n  Admission = factor(icu_data$admtype),\n  Age = icu_data$Age,\n  PRISM = icu_data$PRISM\n)\nlibrary(dplyr)\nsummary_by_sex &lt;- icu_data %&gt;%\n  group_by(Sex) %&gt;%\n  summarise(\n    Mean = mean(Ndays),\n    Variance = var(Ndays),\n    Count = n()\n  )\nsummary_by_admission &lt;- icu_data %&gt;%\n  group_by(Admission) %&gt;%\n  summarise(\n    Mean = mean(Ndays),\n    Variance = var(Ndays),\n    Count = n()\n  )\n#knitr::kable(summary_by_sex)\n#knitr::kable(summary_by_admission)\nlibrary(kableExtra)\nsummary_by_sex %&gt;%\n  kable(\"html\", caption = \"ICU Days by Sex\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\nICU Days by Sex\n\n\nSex\nMean\nVariance\nCount\n\n\n\n\nFemale\n3.628571\n4.004933\n140\n\n\nMale\n3.597701\n3.155139\n174\n\n\n\n\n\n\n\n\nCode\nsummary_by_admission %&gt;%\n  kable(\"html\", caption = \"ICU Days by Admission Type\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\nICU Days by Admission Type\n\n\nAdmission\nMean\nVariance\nCount\n\n\n\n\nNot planned\n4.094972\n3.726885\n179\n\n\nPlanned\n2.970370\n2.551354\n135\n\n\n\n\n\n\n\n\n\n\nModel Development\nThe primary goal of this modelling exercise is to examine how the clinical factor (i.e., PRISM) is associated with the ICU stay duration? We also have secondary goal, where we are interested to understand the association of the demographic factors with the length of ICU stay. Given the nature of the count outcome variable (Ndays), statistical models such as Bayesian Poisson regression may be appropriate.\nWe write the directed acyclic graph (DAG) as follows, where we use a similar type of weakly informative prior distribution for the model parameters.\n\n\n\n\n\n\nNow we write the Bayesian model using mathematical notations. Suppose, \\(\\text{Ndays}_i\\) be the ICU stay duration for patient \\(i\\), and \\(\\lambda_i\\) be the expected number of days in the ICU for patient \\(i\\). The predictor variables in the model are PRISM score, which is the exposure variable for our model. Other predictor variables are, age (in weeks), sex (coded as binary: e.g., 0 = Female, 1 = Male) and admission type (coded as binary: 0 = Planned, 1 = Not Planned)\nHence, we write:\n\\[\n\\text{Ndays}_i \\sim \\text{Poisson}(\\lambda_i)\n\\]\n\\[\n\\log(\\lambda_i) = \\beta_0 + \\beta_1 \\cdot \\text{PRISM}_i + \\beta_2 \\cdot \\text{Age}_i + \\beta_3 \\cdot \\text{Sex}_i + \\beta_4 \\cdot \\text{Admission}_i\n\\]\nEach regression coefficient has a prior distribution, typically Gaussian:\n\\[\n\\beta_j \\sim N(\\mu_j, \\sigma_j^2) \\quad \\text{for } j = 0, 1, 2, 3, 4\n\\]\nWhere:\n\n\\(\\mu_j\\) is the prior mean for coefficient \\(\\beta_j\\)\n\\(\\sigma_j^2\\) is the prior variance (reflecting uncertainty)\n\nThis formulation assumes log-link, standard for Poisson regression, and independent Gaussian priors for the coefficients.\nSuppose, \\(Y_i = \\text{Ndays}_i\\) and using vector and matrix notation,\n\\[\n\\log(\\lambda_i) = \\mathbf{x}_i \\boldsymbol{\\beta}\n\\quad \\text{where } \\mathbf{x}_i = (1, \\text{PRISM}_i, \\text{Age}_i, \\text{Sex}_i, \\text{Admission}_i)'\n\\]\nThe likelihood for the data is:\n\\[\np(\\mathbf{y} \\mid \\mathbf{x}, \\boldsymbol{\\beta}) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}\n= \\prod_{i=1}^{n} \\frac{e^{-\\exp(\\mathbf{x}_i \\boldsymbol{\\beta})} \\left[e^{(\\mathbf{x}_i \\boldsymbol{\\beta})}\\right]^{y_i}}{y_i!}\n\\]\nAssume independent Gaussian priors for each coefficient, then the joint posterior is:\n\\[\np(\\boldsymbol{\\beta} \\mid \\mathbf{y},\\mathbf{x}) \\propto\n\\prod_{i=1}^{n} \\frac{e^{-\\exp(\\mathbf{x}_i \\boldsymbol{\\beta})} \\left[e^{(\\boldsymbol{x}_i \\boldsymbol{\\beta})}\\right]^{y_i}}{y_i!}\n\\cdot\n\\prod_{j=0}^{q} \\frac{1}{\\sqrt{2\\pi \\sigma_j^2}} e^{\\left( -\\frac{(\\beta_j - \\mu_j)^2}{2\\sigma_j^2} \\right)}\n\\]\nThis posterior distribution does not have a closed-form, so we will use Markov Chain Monte Carlo (MCMC) methods to get the inference.\nUsing following R code we can get the posterior results:\n\n\nCode\nlibrary(brms)\nlibrary(tidyverse)\nicu_data &lt;- read.csv(\"icu_days_restricted.csv\")\nicu_data &lt;- tibble(\n  Ndays = icu_data$ndays,\n  Sex = factor(icu_data$sex),\n  Admission = factor(icu_data$admtype),\n  Age = icu_data$Age,\n  PRISM = icu_data$PRISM\n)\nicu_model &lt;- brm(\n  formula = Ndays ~ PRISM + Sex + Admission + Age,\n  data = icu_data,\n  family = poisson(link = \"log\"),\n  prior = c(\n    prior(normal(0, 3), class = \"b\"), \n    prior(normal(0, 10), class = \"Intercept\")\n  ),\n  iter = 2000,\n  chains = 3,\n  cores = 3,\n  seed = 123\n)\nsummary(icu_model)\n\n\n Family: poisson \n  Links: mu = log \nFormula: Ndays ~ PRISM + Sex + Admission + Age \n   Data: icu_data (Number of observations: 314) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            1.31      0.07     1.17     1.46 1.00     1614     1890\nPRISM                0.02      0.00     0.01     0.03 1.00     2829     2081\nSexMale             -0.04      0.06    -0.16     0.08 1.00     1768     1814\nAdmissionPlanned    -0.25      0.07    -0.38    -0.12 1.00      884      487\nAge                 -0.00      0.00    -0.00    -0.00 1.00     2885     2056\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nplot(icu_model)\n\n\n\n\n\n\n\n\n\nCode\nlibrary(bayesplot)\np1 &lt;- pp_check(icu_model, type=\"bars\")\np2 &lt;- pp_check(icu_model, ndraws=100, type = \"ecdf_overlay\")\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nNow for obtaining the risk ratio (or rate ratio), which is \\(\\exp(\\beta)\\), we get the posterior summary:\n\n\nCode\n#library(bayesplot)\n#posterior &lt;- as_draws_df(icu_model)\n#posterior_rr &lt;- posterior %&gt;%\n#  dplyr::select(starts_with(\"b_\")) %&gt;%\n#  dplyr::mutate(across(everything(), exp))\n#mcmc_trace(\n#  posterior_rr,\n#  pars = c(\"b_SexMale\",\"b_AdmissionPlanned\", \"b_Age\", \"b_PRISM\")\n#) +\n#  ggplot2::labs(title = \"Posterior Trace Plots of Risk Ratios\")\n\nlibrary(gtsummary)\nicu_model %&gt;%\n  tbl_regression(exponentiate = TRUE) %&gt;%\n  bold_labels() %&gt;%\n  modify_caption(\"**Posterior Risk Ratio (RR) - Poisson Model**\")\n\n\n\n\n\n\nPosterior Risk Ratio (RR) - Poisson Model\n\n\n\n\n\n\n\nCharacteristic\nexp(Beta)\n95% CI\n\n\n\n\nPRISM\n1.02\n1.01, 1.03\n\n\nSex\n\n\n\n\n\n\n    Female\n—\n—\n\n\n    Male\n0.96\n0.85, 1.08\n\n\nAdmission\n\n\n\n\n\n\n    Not planned\n—\n—\n\n\n    Planned\n0.78\n0.68, 0.89\n\n\nAge\n1.00\n1.00, 1.00\n\n\n\nAbbreviation: CI = Credible Interval\n\n\n\n\n\n\n\n\nThe model estimates how different patient characteristics influence the expected number of days spent in the ICU. Since it’s a Bayesian Poisson regression, the coefficients are exponentiated to yield risk ratios (RRs), which indicate the multiplicative change in ICU stay duration associated with each predictor. A risk ratio greater than 1 suggests a longer ICU stay, while a value less than 1 indicates a shorter stay.\nThe PRISM score, which measures the severity of illness, has a risk ratio of 1.02 with a 95% credible interval of (1.01, 1.03). This means that for each one-point increase in PRISM, the expected ICU stay increases by about 2%, and this effect is statistically credible since the interval does not include 1. Regarding sex, males have a risk ratio of 0.96 compared to females, suggesting a slightly shorter ICU stay, but the credible interval (0.85, 1.08) includes 1, indicating that this difference is not statistically meaningful.\nFor admission type, patients with planned admissions have a risk ratio of 0.78 compared to those with unplanned admissions. This implies that planned admissions are associated with a 22% shorter ICU stay, and the credible interval (0.68, 0.89) supports this as a credible finding. Lastly, age has a risk ratio of 1.00 with a very narrow credible interval, suggesting that age has no meaningful effect on ICU stay duration in this model.\n\n\nMore on Count Data\nIn this unit, we will explain some situations that may occur when modelling count data, such as overdispersion and zero inflation. However, we will not go into detail on modelling overdispersed or underdispersed data, zero-inflated situations, or models with offsets. Instead, we will provide related references for interested readers and students.\nModel for over and under dispersion\nWhen we model count data using a Bayesian Poisson model, it is important to examine whether the underlying assumptions of the Poisson distribution hold true for our dataset. One of the key assumptions of the Poisson distribution is that the mean and variance are equal. In other words, the expected value \\(\\lambda\\) of the distribution should be the same as its variance. This property simplifies modelling and interpretation but may not always reflect the characteristics of real-world count data.\nBefore proceeding with model interpretation or inference, we should check whether this assumption is met. If the variance substantially exceeds the mean, we are likely dealing with overdispersion. This situation can arise due to various factors such as unobserved heterogeneity or clustering in the data, etc. On the other hand, underdispersion occurs, where the variance is smaller than the mean, which is less common but can also signal model misspecification or constraints in the data-generating process.\nIn a Bayesian framework, we can use techniques like posterior predictive checks to assess whether the Poisson model adequately captures the variability in the data. If we detect signs of overdispersion or underdispersion, it may be necessary to consider alternative models that offer more flexibility. For example, the Negative Binomial model introduces an additional dispersion parameter that allows the variance to exceed the mean, making it a suitable alternative in the presence of overdispersion (McElreath (2020)). In brm package, we use a negetive-binomial family, i.e., negbinomial(link = 'log') by replacing the poisson.\n\nModel with Infleated Zeros\nWhen our count data exhibit more zeros than a standard Poisson or Negative Binomial model can account for, we are facing zero inflation.\nTo address this, we can employ zero-inflated models, such as the Zero-Inflated Poisson (ZIP) or Zero-Inflated Negative Binomial (ZINB). In these frameworks, we explicitly model two stages. First, a binary process determines whether an observation is a structural zero. Second, for cases not deemed structural zeros, we model counts, allowing for additional zeros, using a Poisson or Negative Binomial distribution. By separating these processes, we capture both the excess zeros and the underlying count behavior more accurately (McElreath (2020)).\nAlternatively, we might choose a hurdle model when we believe that the data-generating mechanism for zeros is fundamentally different from that for positive counts (McElreath (2020)). In a hurdle approach, the first component models the probability of observing zero versus a positive count. Once the hurdle of producing a non-zero value is crossed, a second component models the strictly positive counts using a zero-truncated distribution. This ensures that all zeros arise from the first stage, while the second stage focuses solely on positive outcomes.\nDeciding between zero-inflated and hurdle formulations depends on our substantive understanding of the process. If we think some zeros are truly “structural” and would never result in a non-zero count, a zero-inflated model is preferable. If instead we believe that the transition from zero to positive count represents a distinct event, after which counts follow a different regime, a hurdle model may offer more interpretability.\nIn both strategies, we improve model fit and inference by aligning our statistical assumptions with the realities of our data.\n\nModel with offset\nThere are also situations when we might prefer to model ratios instead of raw counts. This approach can be particularly useful when we need to adjust for differences in exposure, population size, or observation periods. For example, if we are analysing the number of hospital admissions across different regions, modelling the admission rate per 1,000 people allows us to account for differences in population size. Using raw counts in such cases could be misleading, as a region with a larger population will naturally have more admissions, even if the underlying risk is the same.\nModelling ratios or rates allows us to make fair comparisons across groups and can help control for varying levels of opportunity or risk exposure. In statistical terms, this is often done by including an ‘offset’ in the model. For instance, in a Poisson regression, we can include the log of the exposure variable (e.g., population size or person-time) as an offset term. This allows the model to estimate the rate of events per unit of exposure rather than the absolute number of events.",
    "crumbs": [
      "Week 8: **More on Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_2.html#summary",
    "href": "M04_2.html#summary",
    "title": "Week 8: More on Non-Gaussian",
    "section": "Summary",
    "text": "Summary\nIn today’s session, we focused on Bayesian modelling techniques for logistic regression and count data. We began with a sensitivity analysis of prior distributions in logistic regression, comparing the effects of using Cauchy and normal weakly informative priors. This highlighted how prior choice can influence inference, particularly when data are limited or noisy. Next, we examined hierarchical prior structures, exploring normal-exponential and Cauchy-exponential priors, which offer flexible ways to model parameter shrinkage and improve regularisation in complex models. Finally, we introduced Bayesian modelling of count data through Poisson regression, and how these models can be adapted to handle varying data characteristics and how prior specification remains crucial. We have also briefly discussed some count data situations, such as overdispersion, when the Poisson distribution might not be the right approach to model the data",
    "crumbs": [
      "Week 8: **More on Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_2.html#live-tutorial-and-discussion",
    "href": "M04_2.html#live-tutorial-and-discussion",
    "title": "Week 8: More on Non-Gaussian",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 8: **More on Non-Gaussian**"
    ]
  },
  {
    "objectID": "M04_2.html#tutorial-exercises",
    "href": "M04_2.html#tutorial-exercises",
    "title": "Week 8: More on Non-Gaussian",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.\n\n\n\n\nGelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su. 2008. “A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models.” The Annals of Applied Statistics 2 (4): 1360–83.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan (2nd Edition). Chapman; Hall/CRC.",
    "crumbs": [
      "Week 8: **More on Non-Gaussian**"
    ]
  },
  {
    "objectID": "brief_module_05.html",
    "href": "brief_module_05.html",
    "title": "Module 5: Clusterphobia?",
    "section": "",
    "text": "Summary\nIn this module of the course, we explore our understanding of working with clustered data and how Bayesian modelling provides a guided approach for analysing such structures. Clustered, or hierarchical, data arise when observations are grouped, such as patients within hospitals or repeated measurements within individuals. Ignoring this structure can lead to biased estimates and misleading inferences, making it essential to account for clustering in our models.\nWe begin by exploring why clustering matters and use the example of C-reactive protein (CRP) levels to illustrate the impact of ignoring versus accounting for within-group correlation. This sets the stage for introducing Bayesian hierarchical models, also known as multilevel models, which naturally accommodate clustered structures by allowing parameters to vary across groups.\nTo develop an intuitive understanding of these models, we examine three modelling strategies: complete pooling, where all data are assumed to come from a single group; no pooling, where each group is treated entirely separately; and partial pooling, which provides a balance by borrowing strength across groups while still allowing for individual variation. Partial pooling is a core feature of hierarchical modelling and provides more stable and generalisable estimates, especially when group sizes vary or data are sparse.\nBuilding on this, we explore random (or varying) intercept and random slope models, which allow both the baseline level and the effect of predictors to vary across groups. These models introduce flexibility and nuance, helping us capture complex real-world patterns in clustered data. We also extend the discussion to partial pooling in the context of binary outcomes, where hierarchical logistic regression were used to model group-level variation in probabilities.\nA critical part of Bayesian hierarchical modelling is the thoughtful allocation of prior distributions. We discuss strategies for assigning priors at both the group and population levels, including weakly informative priors that regularise estimates while preserving flexibility.\nFinally, we apply these concepts in practice by implementing models in R, using brms packages. We learn how to structure the data, specify models, assign priors, and interpret the output, gaining hands-on experience in fitting and evaluating hierarchical models.\nBy the end of this section, we will have developed both theoretical insight and practical skills for handling clustered data using Bayesian multilevel models. These tools enable us to analyse structured data appropriately, produce more reliable inferences, and make better-informed decisions under uncertainty.",
    "crumbs": [
      "**Module 5:** Clusterphobia?"
    ]
  },
  {
    "objectID": "M05_1.html",
    "href": "M05_1.html",
    "title": "Week 9: Cluster Smart with Bayes",
    "section": "",
    "text": "Learnings\n– LO2: Demonstrate how to specify and fit simple Bayesian models with appropriate attention to the role of the prior distribution and the data model.\n– LO4: Demonstrate proficiency in using statistical software packages (R) to specify and fit models, assess model fit, detect and remediate non-convergence, and compare models.\n– LO5: Engage in specifying, checking and interpreting Bayesian statistical analyses in practical problems using effective communication with health and medical investigators.\nBy the end of this week you should be able to:\n– Learn the difference between clustered and non-clustered data.\n– Implement Bayesian hierarchical (or multilevel) models.\n– Interpret real-life clustered data problems in Bayesian context.",
    "crumbs": [
      "Week 9: **Cluster Smart with Bayes**"
    ]
  },
  {
    "objectID": "M05_1.html#learnings",
    "href": "M05_1.html#learnings",
    "title": "Week 9: Cluster Smart with Bayes",
    "section": "",
    "text": "Outcomes\n\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 9: **Cluster Smart with Bayes**"
    ]
  },
  {
    "objectID": "M05_1.html#clustered-data",
    "href": "M05_1.html#clustered-data",
    "title": "Week 9: Cluster Smart with Bayes",
    "section": "Clustered Data",
    "text": "Clustered Data\n\n\nClustered data refers to observations that are grouped into clusters, where data points within the same cluster tend to be more similar to each other than to those in other clusters. This structure often arises naturally in health and medical research, where measurements are taken on individuals who share a common setting, treatment provider, or geographic location. A common example, say in healthcare is data collected from multiple patients treated within the same hospital or clinic. Patients within a single hospital may receive similar types of care, be exposed to the same medical practitioner, and follow similar protocols. As a result, their outcomes, such as recovery time or treatment success often are more alike compared to patients from different hospitals. Another example occurs say in longitudinal studies, where repeated measurements are taken from the same individual over time. Here, each individual forms a cluster of observations. For instance, a study tracking blood pressure readings in hypertensive patients over several months will generate multiple observations per patient, and these readings are naturally correlated.\n\n\n\n\n\nExample of Clustered Data\n\n\n\n\nIn both cases, recognising and correctly handling clustered data ensures more accurate inferences and better decision-making in health and medical research. A Bayesian hierarchical model (also known as Bayesian multilevel model) can handle this types of complexity by introducing levels of variation (hierarchies) and latent processes that influence the observed data.\n\nWhy Consider Clustering?\nNot accounting for clustering effects in modelling often leads to issues that compromise the validity, reliability, and interpretability of the results. Bayesian hierarchical modelling is actually designed to handle clustering. That said, if clustering is not properly modelled even within a Bayesian framework, similar issues can still occur. Let’s go through some of the problems below:\nUnderestimated Posterior Uncertainty: When clusters are ignored (e.g., no latent effects or hierarchical structure), the model assumes full independence. This leads to overconfident posterior estimates (i.e., narrow credible intervals), similar to underestimated standard errors in frequentist models.\nBiased Posterior Estimates: Without modelling cluster-level effects, the posterior distribution may be pulled too strongly by certain clusters (e.g., large hospitals or outlier clinics). This skews inferences about population-level parameters.\nInvalid Posterior Inference: Posterior summaries (means, medians, credible intervals) assume the model structure is correct. If clustering is ignored, the posteriors may not reflect the true data-generating process, leading to misleading decisions or predictions.\nModel Misspecification: Ignoring clustering violates the conditional independence assumptions built into the likelihood. This leads to a mismatch between model assumptions and data structure, harming model fit and diagnostics.\nLoss of Information Sharing (No Partial Pooling): Without hierarchical modelling, there is no borrowing of strength across clusters. Hence, small clusters may have unstable estimates, while large clusters may dominate the inference.\nPoor Generalisability (Out-of-Sample): A model without clustering may overfit to the specific sampled clusters, especially when data is sparse or imbalanced across groups. Hence, predictions for new clusters (e.g., new hospitals) are unreliable.\n\n\n\nC-Reactive Protein Data\nBefore explaining the Bayesian hierarchical (or multilevel) model, let us explain a clustered data on C-reactive protein measured at the intensive care unit (ICU).\nIn this example, we examine data collected from patients admitted to the intensive care unit (ICU). Our primary focus is on the measurement of C-reactive protein (CRP) levels (in mg/L), which were recorded daily over the first six days following ICU admission. These repeated measurements allow us to study the progression of inflammation during the early phase of critical illness.\nWe interpret CRP levels based on commonly accepted reference ranges. A CRP level of less than 3 mg/L is generally considered normal for most healthy individuals. When CRP levels fall between 3 and 10 mg/L, they may still be within the normal range but can also indicate a mild elevation. This slight increase is sometimes observed in conditions such as chronic low-grade inflammation. When CRP levels rise to 10 mg/L or higher, we consider this indicative of a more significant inflammatory response. Such elevated levels may reflect the presence of infection, acute inflammation, or other underlying medical conditions requiring further investigation.\nNow, each patient in the dataset is identified by a unique ID, which enables us to link multiple CRP measurements to the same individual. In addition to CRP levels, we have several patient-level variables that provide important clinical and demographic context.\nSpecifically, we have data on the patient’s age and sex, as well as their sepsis status at admission, categorised as sepsis, severe sepsis, or septic shock. The more severe the systemic inflammatory response in sepsis, the higher and more prolonged the CRP levels tend to be. Septic shock, a severe form of sepsis characterised by dangerously low blood pressure, is often associated with persistently high CRP levels. For our data, we also know whether each patient was given antibiotics within the first hour of ICU admission, which may be an indicator of timely clinical intervention. Finally, we include the patient’s discharge status from the ICU, recorded as either alive or dead.\n\n\nCode\nlibrary(tidyverse)\nlibrary(skimr)\ncrp_data &lt;- read.csv(\"crp_data_complete.csv\")\ncrp_data &lt;- tibble(\n  ID = crp_data$ID,\n  CRP = crp_data$crp,\n  Day = crp_data$day,\n  Antibiotic = as.factor(crp_data$antib_1h),\n  Age = crp_data$age,\n  Sex = as.factor(crp_data$SEX),\n  Sepsis = as.factor(crp_data$SEPSIS),\n  Discharg = as.factor(crp_data$discharge_r)\n)\nskimr:::skim_without_charts(crp_data[,-1])\n\n\n\nData summary\n\n\nName\ncrp_data[, -1]\n\n\nNumber of rows\n432\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nAntibiotic\n0\n1\nFALSE\n2\nNo: 234, Yes: 198\n\n\nSex\n0\n1\nFALSE\n2\nMal: 276, Fem: 156\n\n\nSepsis\n0\n1\nFALSE\n3\nSep: 228, Sev: 180, Sep: 24\n\n\nDischarg\n0\n1\nFALSE\n2\nAli: 354, Dea: 78\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\n\n\n\n\nCRP\n0\n1\n14.40\n10.75\n0.2\n6.40\n11.5\n20.2\n68.6\n\n\nDay\n0\n1\n3.50\n1.71\n1.0\n2.00\n3.5\n5.0\n6.0\n\n\nAge\n0\n1\n63.92\n16.79\n18.0\n52.75\n69.0\n78.0\n91.0",
    "crumbs": [
      "Week 9: **Cluster Smart with Bayes**"
    ]
  },
  {
    "objectID": "M05_1.html#bayesian-hierarchical-multilevel-model",
    "href": "M05_1.html#bayesian-hierarchical-multilevel-model",
    "title": "Week 9: Cluster Smart with Bayes",
    "section": "Bayesian Hierarchical (Multilevel) Model",
    "text": "Bayesian Hierarchical (Multilevel) Model\n\n\n\nSimple Model for Clustering\nLet’s start developing a simple model, where we can address the clustering effect. To create such model, we can introducte dummy variable for the cluster variable. For the CRP data, ID variable is refering the patients and we convert ID variable into a dummay variable and we get ID (or subject) specific regression coefficients.\nThese types of models are sometimes referred to as ‘fixed effects’ models in frequentis approach, to distinguish them from ‘random effects’ models, though the terminology is not universally agreed upon. In particular, the terms ‘fixed’ and ‘random’ are used in various contexts and are often applied inconsistently or inappropriately.\nMore recently, this type of model in Bayesian context is often described as a ‘no pooling’ model, in contrast to ‘complete pooling’ and ‘partial pooling’ approaches. Now we will discuss these types of pooling approaches starting with ‘complete pooling’ and ‘no pooling’ models.\n\nComplete Pooling\nIn complete pooling, we ignore the clustering structure and fit a single set of parameters for all individuals/clusters. We assume everyone follows the same model. Thus, for a simple intercept only model, we can write:\n\\[\n\\text{CRP}_{ij} \\sim N(\\beta_0\\cdot1,\\sigma^2)\n\\]\nwhere, \\(i\\) represents the number of observations for each individual/cluster and \\(j\\) represents the number of indiviluals/clusters, i.e., we can write \\(i=1,\\cdots,n_j\\), \\(j=1,\\cdots,J\\), and total number of observations \\(n=\\sum_{j=1}^J\\sum_{i=1}^{n_j} n_{ij}\\). This model is nothing but a simple linear regression without any predictor variable. We can use a prior distribution for the intercept, such that \\(\\beta_0 \\sim N(0,\\sigma^2_0)\\), and add the weakly informative, non-informative or informative situations towards the variance hyper-parameter \\(\\sigma^2_0\\), as we have discussed in our previous lectures. Hence, we get the posterior distribution of \\(\\beta_0\\) from the completely pooled model.\n\n\n\n\n\n\n\n\nNo Pooling\nIn no pooling, we fully account for the clustering structure by estimating separate parameters for each individual or cluster. That is, we do not share any information across individuals or clusters. Each cluster gets its own independent model, and we assume the data for each cluster is generated from its own set of parameters.\nFor a simple intercept-only model under no pooling, we write:\n\\[\n\\text{CRP}_{ij} \\sim N(\\beta_{0j} \\cdot 1, \\sigma^2)\n\\]\nHere, \\(i = 1,\\cdots,n_j\\) denotes the observations within each individual or cluster \\(j = 1,\\cdots, J\\), and \\(\\beta_{0j}\\) is a unique intercept parameter for each cluster \\(j\\). This means that we estimate \\(J\\) separate intercepts, one for each group, without any sharing or pooling of information between groups.\nTo complete the Bayesian formulation, we specify a prior for each \\(\\beta_{0j}\\), such as:\n\\[\n\\beta_{0j} \\sim N(0, \\sigma_{0}^2)\n\\]\nThis prior can be weakly informative, non-informative, or informative, depending on the context and prior knowledge. Since there is no sharing of information across clusters, the posterior distribution of each \\(\\beta_{0j}\\) is informed only by the data from the corresponding group \\(j\\) and the prior.\nThis model structure is highly flexible but may suffer from overfitting, especially when the number of observations within some clusters is small. Unlike complete pooling, no pooling does not assume a common intercept across all clusters but instead treats each as an entirely separate entity.\n\n\n\n\n\n\n\n\nPartial Pooling\nNow for partial pooling, we strike a balance between complete pooling and no pooling by allowing the parameters for each cluster to vary, while also sharing information across clusters. This is typically done using a model, where individual cluster parameters are assumed to come from a common population-level distribution. As a result, the estimates for each cluster are “shrunk” toward the overall mean, with the degree of shrinkage depending on the amount of data available in each cluster and the variability across clusters.\nFor a simple intercept-only model under partial pooling, we write:\n\\[\n\\text{CRP}_{ij} \\sim N((\\beta_0 + \\beta_{0j}) \\cdot 1, \\sigma^2)\n\\]\nHere, \\(\\beta_0\\) is a global intercept shared across all clusters, and \\(\\beta_{0j}\\) is a group-specific deviation from the global intercept for cluster \\(j\\), where we assume that \\(\\beta_{0j}\\) follows a normal (or Gaussian) distribution, and write \\(\\beta_{0j} \\sim N(0, \\sigma_{0}^2)\\). Note that, unlike no pooling model, here in the partial pooling model we are treating \\(\\beta_{0j}\\) as randomly distributed with mean zero and an unknown group/cluster specific variance \\(\\sigma_0^2\\). Hence, we are interested to learn about the estimate of the group-wise or cluster variability \\(\\sigma_0^2\\).\nNow, as a Bayesian we need define the distributions for the model parameters:\n\\[\n\\beta_{0} \\sim N(0,\\sigma^2_{00}); \\quad \\beta_{0j}|\\sigma_{0} \\sim N(0, \\sigma_{0}^2);\n\\]\n\\[\n\\sigma_{0} \\sim \\text{Half-Cauchy}(0,\\tau_0); \\quad \\sigma \\sim \\text{Half-Cauchy}(0,\\tau);\n\\]\nThe term, \\(\\beta_{0j}\\) in this model are providing no pulling and mimicing a random process or effect, whereas, \\(\\beta_0\\) is the global intercept. Thus, using the prior distribution hierarchies, we can get the posteriors for \\(\\beta_0\\) and \\(\\beta_{0j}\\) accordingly.\nThis structure allows the model to pool information across clusters while still allowing each cluster to have its own intercept. The posterior estimates of the cluster effects \\(\\beta_{0j}\\) are ‘partially pooled’ toward the global mean \\(\\beta_0\\), with the amount of shrinkage determined by the variance parameter \\(\\sigma_{0j}^2\\): clusters with less data (or higher uncertainty) are pulled more toward the global mean, while clusters with more data retain more individualized estimates.\nPartial pooling provides a compromise: it avoids the rigidity of complete pooling (which assumes all clusters are the same) and the potential overfitting of no pooling (which treats all clusters as completely separate). The result is more stable and interpretable estimates, especially when the number of observations per cluster is highly variable.\n\n\n\n\n\n\n\n\nComparative Analysis\n\n\nHere’s a table summarising the completely pooled, no pooling, and partial pooling models, with the prior distributions and hyper parameters:\n\n\n\n\n\n\n\n\nCompletely Pooled\nNo Pooling\nPartial Pooling\n\n\n\n\n\\(\\text{CRP}_{ij} = \\beta_0 + \\epsilon_{ij}\\)\n\\(\\text{CRP}_{ij} = \\beta_{0j} + \\epsilon_{ij}\\)\n\\(\\text{CRP}_{ij} = \\beta_0 + \\beta_{0j} + \\epsilon_{ij}\\)\n\n\n\\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\)\n\\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\)\n\\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\)\n\n\n\\(\\beta_{0} \\sim N(0, \\sigma_0^2 = 10^2)\\)\n\\(\\beta_{0j} \\sim N(0, \\sigma_0^2 = 10^2)\\)\n\\(\\beta_0 \\sim N(0, \\sigma_{00}^2 = 10^2)\\)\n\n\n\\(\\sigma \\sim \\text{Half-Cauchy}(0, \\tau = 1)\\)\n\\(\\sigma \\sim \\text{Half-Cauchy}(0, \\tau = 1)\\)\n\\(\\beta_{0j}|\\sigma_{0} \\sim N(0, \\sigma_0^2)\\)\n\n\n—\n—\n\\(\\sigma_0 \\sim \\text{Half-Cauchy}(0, \\tau_0 = 1)\\)\n\n\n—\n—\n\\(\\sigma \\sim \\text{Half-Cauchy}(0, \\tau = 1)\\)\n\n\n\nFollowing this we get the posterior summaries as follows:\n\n\nCode\nlibrary(tidyverse)\nlibrary(skimr)\ncrp_data &lt;- read.csv(\"crp_data_complete.csv\")\ncrp_data &lt;- tibble(\n  ID = crp_data$ID,\n  CRP = crp_data$crp,\n  CRP_log = log(crp_data$crp),\n  Day = crp_data$day,\n  Antibiotic = as.factor(crp_data$antib_1h),\n  Age = crp_data$age,\n  Sex = as.factor(crp_data$SEX),\n  Sepsis = as.factor(crp_data$SEPSIS),\n  Discharg = as.factor(crp_data$discharge_r)\n)\ncrp_data_subset &lt;- subset(crp_data, ID &lt; 1500)\n## \nlibrary(brms)\nlibrary(tidyverse)\ncomplete_pooling_model &lt;- brm(\n  formula = CRP ~ 1,  # Single intercept, no ID-specific effects\n  data = crp_data_subset,\n  family = gaussian(),\n  prior = c(\n    prior(normal(0, 10), class = \"Intercept\"),   # Prior on the global intercept\n    prior(cauchy(0, 1), class = \"sigma\")         # Prior on residual SD\n  ),\n  cores = 3, chains = 3, iter = 2000\n)\ncat(\n  \"Complete Pooling Model Summary:\\n\",\n  paste(capture.output(summary(complete_pooling_model)), collapse = \"\\n\"),\n  \"\\n\"\n)\n\n\nComplete Pooling Model Summary:\n  Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: CRP ~ 1 \n   Data: crp_data_subset (Number of observations: 90) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    14.57      1.22    12.24    16.94 1.00     2190     1993\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    11.42      0.87     9.87    13.34 1.00     2156     1563\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1). \n\n\nCode\nno_pooling_model &lt;- brm(\n  formula = CRP ~ 0 + factor(ID),  # separate intercept for each ID\n  data = crp_data_subset,\n  family = gaussian(),\n  prior = c(\n    prior(normal(0, 10), class = \"b\"),   # N(mean, sd) \n    prior(cauchy(0, 1), class = \"sigma\")  # Half-Cauchy prior for sigma\n  ),\n  cores = 3, chains = 3, iter = 2000\n)\ncat(\n  \"No Pooling Model Summary:\\n\",\n  paste(capture.output(summary(no_pooling_model)), collapse = \"\\n\"),\n  \"\\n\"\n)\n\n\nNo Pooling Model Summary:\n  Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: CRP ~ 0 + factor(ID) \n   Data: crp_data_subset (Number of observations: 90) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nfactorID122     3.66      3.29    -2.90    10.15 1.00     5491     1986\nfactorID133    10.80      3.07     4.82    16.70 1.00     5643     2142\nfactorID246    14.47      3.21     7.96    20.68 1.00     7192     2272\nfactorID506    11.86      3.29     5.66    18.31 1.00     6508     2133\nfactorID603    16.85      3.22    10.32    23.42 1.00     5909     2090\nfactorID663     3.20      3.11    -2.93     9.15 1.00     7173     2194\nfactorID756     6.75      3.22     0.47    12.85 1.00     7169     2077\nfactorID826    12.84      3.26     6.43    19.29 1.01     5963     2249\nfactorID861    24.17      3.36    17.45    30.51 1.00     6669     2082\nfactorID878    26.86      3.29    20.41    33.27 1.00     6106     1972\nfactorID880    28.90      3.20    22.54    35.05 1.00     5497     2176\nfactorID926     9.99      2.99     4.24    16.03 1.00     7011     2213\nfactorID951     5.90      3.11    -0.28    12.03 1.00     5703     2160\nfactorID995     6.89      3.17     0.57    13.13 1.00     6868     2163\nfactorID998    16.71      3.28    10.28    23.00 1.01     6621     2256\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     8.12      0.71     6.89     9.67 1.00     3668     2181\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1). \n\n\nCode\n##\npartial_pooling_model &lt;- brm(\n  formula = CRP ~ 1 + (1 | ID),  # random intercept for each ID\n  data = crp_data_subset,\n  family = gaussian(),\n  prior = c(\n    prior(normal(0, 10), class = \"Intercept\"),   # N(mean, sd)\n    prior(cauchy(0, 1), class = \"sigma\"),  # Half-Cauchy prior for sigma\n    prior(cauchy(0, 1), class = \"sd\")  # prior for the variation across IDs\n  ),\n  cores = 3, chains = 3, iter = 2000\n)\ncat(\n  \"Partial Pooling Model Summary:\\n\",\n  paste(capture.output(summary(partial_pooling_model)), collapse = \"\\n\"),\n  \"\\n\"\n)\n\n\nPartial Pooling Model Summary:\n  Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: CRP ~ 1 + (1 | ID) \n   Data: crp_data_subset (Number of observations: 90) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nMultilevel Hyperparameters:\n~ID (Number of levels: 15) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     8.36      1.87     5.41    12.67 1.00      868     1303\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    14.27      2.30     9.57    18.74 1.00      748      953\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     8.02      0.66     6.87     9.39 1.00     2858     2180\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1). \n\n\nCode\n##\n\n\nWe can see that in the no pooling model, the posterior estimates are provided for each individual ID. In this model, the IDs are treated as completely separate, with each having its own intercept estimated independently from all others. As a result, we obtain distinct estimates for 15 IDs (since we have only considered the first 15 IDs from the CRP dataset for illustration).\nWhereas, the partial pooling model assumes that individual intercepts are drawn from a common distribution. This hierarchical structure allows the model to “borrow strength” from the group, which leads to more stable and regularized estimates—particularly for IDs with limited data.\nWe also observe that the posterior mean estimate for the complete pooling model is approximately the same as the group-level mean in the partial pooling model. This is because, in partial pooling, individual estimates are shrunk toward the group mean, which itself closely aligns with the complete pooling estimate that assumes no variation between IDs.\nHowever, we can see that the credible interval in the complete pooling model is narrower compared to that of the partial pooling model. This occurs because the complete pooling model assumes no between-ID variability, attributing all uncertainty to residual error. Here, the partial pooling model accounts for both residual error and variation across individuals, which increases overall uncertainty in individual-level estimates.\n\n\nCode\nlibrary(tidyverse)\n##\ncomplete_pooling_draws &lt;- as_draws_df(complete_pooling_model) %&gt;%\n  select(b_Intercept) %&gt;%\n  rename(estimate = b_Intercept)\nunique_ids &lt;- unique(crp_data_subset$ID)\nsummary_df_complete &lt;- tibble(\n  parameter = as.character(unique_ids),\n  median = median(complete_pooling_draws$estimate),\n  lower = quantile(complete_pooling_draws$estimate, 0.025),\n  upper = quantile(complete_pooling_draws$estimate, 0.975)\n)\n##\nno_pooling_draws &lt;- as_draws_df(no_pooling_model) %&gt;%\n  select(starts_with(\"b_factorID\")) %&gt;%\n  pivot_longer(cols = everything(),\n               names_to = \"parameter\",\n               values_to = \"estimate\") %&gt;%\n  mutate(model = \"No pooling\")\nsummary_df_no_pooling &lt;- no_pooling_draws %&gt;%\n  group_by(parameter) %&gt;%\n  summarise(\n    median = median(estimate),\n    lower = quantile(estimate, 0.025),\n    upper = quantile(estimate, 0.975),\n    .groups = \"drop\"\n  )\npartial_draws &lt;- as_draws_df(partial_pooling_model) %&gt;%\n  select(starts_with(\"r_ID\")) %&gt;%\n  pivot_longer(cols = everything(),\n               names_to = \"parameter\",\n               values_to = \"estimate\") %&gt;%\n  mutate(parameter = str_remove(parameter, \"r_ID\\\\[|,Intercept\\\\]\"),\n         model = \"Partial pooling\")\nsummary_df_partial &lt;- partial_draws %&gt;%\n  group_by(parameter) %&gt;%\n  summarise(\n    median = median(estimate),\n    lower = quantile(estimate, 0.025),\n    upper = quantile(estimate, 0.975),\n    .groups = \"drop\"\n  )\nlibrary(ggplot2)\n# Remove \"b_factorID\" prefix for no pooling model\nsummary_df_no_pooling &lt;- summary_df_no_pooling %&gt;%\n  mutate(parameter = str_remove(parameter, \"b_factorID\"))\n# Remove any prefix from partial pooling model if needed\nsummary_df_partial &lt;- summary_df_partial %&gt;%\n  mutate(parameter = str_remove(parameter, \"r_ID\\\\[|,Intercept\\\\]\"))\np1 &lt;- ggplot(summary_df_no_pooling, aes(x = parameter, y = median)) +\n  geom_point(color = \"steelblue\", position = position_dodge(width = 0.5), size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper),\n                color = \"steelblue\",\n                position = position_dodge(width = 0.5), width = 0.2) +\n  labs(title = \"ID: No Pooling\",\n       y = \"Posterior Estimate (No Pooling)\",\n       x = \"ID\") +\n  theme_minimal() + ylim(-20, 40) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\np2 &lt;- ggplot(summary_df_partial, aes(x = parameter, y = median)) +\n  geom_point(color = \"darkorange\", position = position_dodge(width = 0.5), size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper),\n                color = \"darkorange\",\n                position = position_dodge(width = 0.5), width = 0.2) +\n  labs(title = \"ID: Partial Pooling\",\n       y = \"Posterior Estimate (Partial Pooling)\",\n       x = \"ID\") +\n  theme_minimal() + ylim(-20, 40) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\np3 &lt;- ggplot(summary_df_complete, aes(x = parameter, y = median)) +\n  geom_point(color = \"darkgreen\", position = position_dodge(width = 0.5), size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper),\n                color = \"darkgreen\",\n                position = position_dodge(width = 0.5), width = 0.2) +\n  labs(title = \"ID: Complete Pooling\",\n       y = \"Posterior Estimate (Complete Pooling)\",\n       x = \"ID\") +\n  theme_minimal() + ylim(-20, 40) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\nlibrary(gridExtra)\ngrid.arrange(p3, p1, p2, ncol = 3)\n\n\n\n\n\n\n\n\n\nFrom the plot, we can see the complete pooling model assumes that all groups (IDs) are identical, and therefore estimates a single shared intercept for everyone. The no pooling model takes the opposite approach, treating each group as entirely independent. Whereas, as we have discussed earlier, the partial pooling model offers a middle ground, where it allows for individual differences in intercepts while also sharing information across groups through a hierarchical structure.\nOne of the interesting results we can see that the no pooling and partial pooling models are showing approximately similar 95% credible intervals. In usual scenarios, we would see a wider interval for the no pooling model compared to the partial pooling. For our data, this is not the case and we provide some possible explanations for this types of situation below:\n– If you have very few observations per group, both models may be under-informed, leading to similar uncertainty. The hierarchical model might not have enough data to meaningfully shrink group estimates toward a global mean. Check how many observations each group has. Shrinkage becomes more apparent with moderate to large group sizes.\n– If the variability between groups is very small, then there’s little for the hierarchical model to “shrink”. In this case, both models behave similarly because group means are already close to each other. Check the estimate of your group-level variance parameter, or the Intraclass Correlation Coefficient (ICC), which we will explin later in this lecture. If they are small then partial pooling won’t differ much from no pooling.\n– If your hierarchical model has weak pooling (i.e., the group-level variance is given a very wide prior), the model may behave similarly to no pooling. Conversely, if your no pooling model has strong informative priors, it could behave more like a pooled or partially pooled model.\n– Shrinkage in partial pooling doesn’t always translate to a narrower 95% CI for every group, especially if the group estimate is being pulled away from its data toward a grand mean, the CI might not narrow much but could shift position.\n– There might be sampling or convergence issues in your hierarchical model. If the group-level variance isn’t being estimated properly, or if the model isn’t fully exploring the posterior, shrinkage might not appear as expected. Check diagnostics like R-hat, effective sample size, and posterior trace plots.",
    "crumbs": [
      "Week 9: **Cluster Smart with Bayes**"
    ]
  },
  {
    "objectID": "M05_1.html#partial-pooling-for-c-reactive-protein-data",
    "href": "M05_1.html#partial-pooling-for-c-reactive-protein-data",
    "title": "Week 9: Cluster Smart with Bayes",
    "section": "Partial Pooling for C-Reactive Protein Data",
    "text": "Partial Pooling for C-Reactive Protein Data\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(skimr)\ncrp_data &lt;- read.csv(\"crp_data_complete.csv\")\ncrp_data &lt;- tibble(\n  ID = crp_data$ID,\n  CRP = crp_data$crp,\n  CRP_log = log(crp_data$crp),\n  Day = crp_data$day,\n  Antibiotic = as.factor(crp_data$antib_1h),\n  Age = crp_data$age,\n  Sex = as.factor(crp_data$SEX),\n  Sepsis = as.factor(crp_data$SEPSIS),\n  Discharg = as.factor(crp_data$discharge_r)\n)\n#p1 &lt;- ggplot(crp_data, aes(x = CRP)) +\n#  geom_density(fill = \"skyblue\", alpha = 0.6) +\n#  labs(title = \"Density of CRP\", x = \"CRP\", y = \"Density\") +\n#  theme_minimal()\n#p2 &lt;- ggplot(crp_data, aes(x = CRP_log)) +\n#  geom_density(fill = \"salmon\", alpha = 0.6) +\n#  labs(title = \"Density of log(CRP)\", x = \"log(CRP)\", y = \"Density\") +\n#  theme_minimal()\np1 &lt;- ggplot(data = crp_data, aes(x=Day, y=CRP, group=ID)) +\n  geom_line() +\n  stat_summary(fun = mean, geom = \"line\", lwd = 1.5, colour = \"purple\", aes(group=1)) +\n  scale_x_continuous(breaks=seq(0,6,1)) +\n  labs(title=\"CRP in sepsis patients in ICU\", x = \"Time (days)\",y = \"CRP\")\np2 &lt;- ggplot(data = crp_data, aes(x=Day, y=CRP_log, group=ID)) +\n  geom_line() +\n  stat_summary(fun = mean, geom = \"line\", lwd = 1.5, colour = \"purple\", aes(group=1)) +\n  scale_x_continuous(breaks=seq(0,6,1)) +\n  labs(title=\"CRP (log transformation)\", x = \"Time (days)\",y = \"Log CRP\")\nlibrary(gridExtra)\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\nThe C-reactive protein (CRP) values on the original scale show a right-skewed distribution. In typical modelling settings, a variable transformation is often used to address this. The most common transformation is the logarithm. The plots above display CRP (left) and log-transformed CRP (right) levels over the first six days in the ICU, with each individual’s trajectory shown, and this type of plot is sometimes referred to as a ‘spaghetti plot’. We’ve also added a line connecting the means at each time point to help visualise the overall trend. However, due to the large number of observations, it is difficult to see any specific patterns.\n\nModel Development - Random Intercept\nNow, we will explore the partial pooling model with random intercept and the contribution of fixed effects from a set of covariates \\(\\mathbf{x}_{ij}\\), such as the day since admission, patient age, sex, and whether antibiotics were administered early. The equation at this level is:\n\\[\n\\text{log(CRP)}_{ij} = \\beta_0 + \\beta_{0j} + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\epsilon_{ij}\n\\]\nwhere \\(\\epsilon_{ij}\\) represents normally distributed residual error with standard deviation \\(\\sigma\\).\nAs we have discussed earlier, at the second level (between-patient), the random intercept \\(\\beta_{0j}\\) for each patient is modelled as coming from a common normal distribution with mean 0 and variance \\(\\sigma_0^2\\).\n\nHence, we write the hierarchical structure of the model as follows:\n\\[\n\\begin{aligned}\n\\text{log(CRP)}_{ij} &\\sim N(\\beta_0 + \\beta_{0j} + \\mathbf{x}_{ij} \\boldsymbol{\\beta}, \\sigma^2) \\\\\n\\beta_0 &\\sim N(0, 10^2) \\\\\n\\boldsymbol{\\beta} &\\sim N(0, 10^2) \\\\\n\\beta_{0j}|\\sigma_0 &\\sim N(0, \\sigma_0^2) \\\\\n\\sigma, \\sigma_0 &\\sim \\text{Half-Cauchy}(0, 1)\n\\end{aligned}\n\\]\nWe can write the DAG for the model as:\n\n\n\n\n\n\n\n\nR Code\nNow, we model log(CRP) as the outcome, with, random intercepts for each patient (ID), and fixed effects for Day, Age, Sex, and Antibiotic (see below the R code related to this model).\n\n\nCode\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(brms)\ncrp_data &lt;- read.csv(\"crp_data_complete.csv\")\ncrp_data &lt;- tibble(\n  ID = crp_data$ID,\n  CRP = crp_data$crp,\n  CRP_log = log(crp_data$crp),\n  Day = crp_data$day,\n  Antibiotic = as.factor(crp_data$antib_1h),\n  Age = crp_data$age,\n  Sex = as.factor(crp_data$SEX),\n  Sepsis = as.factor(crp_data$SEPSIS),\n  Discharg = as.factor(crp_data$discharge_r)\n)\npriors &lt;- c(\n  prior(normal(0, 10), class = \"Intercept\"),         # prior for global intercept\n  prior(normal(0, 10), class = \"b\"),                 # prior for all fixed effects\n  prior(cauchy(0, 1), class = \"sd\"),   # prior for random intercept SD\n  prior(cauchy(0, 1), class = \"sigma\")               # prior for residual SD\n)\ncrp_model_random_intercept &lt;- brm(\n  formula = CRP_log ~ Day + Age + Sex + Antibiotic + (1 | ID),\n  data = crp_data,\n  family = gaussian(),\n  prior = priors,\n  chains = 3,\n  cores = 3,\n  iter = 2000,\n  warmup = 1000,\n  seed = 1234,\n  control = list(adapt_delta = 0.95)\n)\nprint(prior_summary(crp_model_random_intercept, all = FALSE), show_df = FALSE)\n\n\nb ~ normal(0, 10)\nIntercept ~ normal(0, 10)\n&lt;lower=0&gt; sd ~ cauchy(0, 1)\n&lt;lower=0&gt; sigma ~ cauchy(0, 1)\n\n\nNote that in brm code we used the argument control = list(adapt_delta = 0.95), which is to adjust the behaviour of the HMC’s No-U-Turn Sampler (NUTS). Here, to adjust the target acceptance probability for proposed steps in the NUTS algorithm by using adapt_delta (with range 0 to 1). The default value for this is 0.8, which is mostly useful and reasonable for Bayesian models that we developed in our previous modules (i.e., Bayesian linear and logistic regressions). However, in hierarchical models, such as with random effects, correlated predictors, and possibly imbalanced or sparse data per group, smaller value of adapt_delta might lead to challenges on posterior accuracy. Hence, it is recommended to increase adapt_delta (e.g., to 0.95 or 0.99) if, we see warnings about divergent transitions, or we are fitting a hierarchical or nonlinear model. This improves accuracy, however at the cost of speed, which is often a worthwhile trade-off.\nNow we get the following posterior summaries from the model:\n\n\nCode\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(brms)\nsummary(crp_model_random_intercept)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: CRP_log ~ Day + Age + Sex + Antibiotic + (1 | ID) \n   Data: crp_data (Number of observations: 432) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nMultilevel Hyperparameters:\n~ID (Number of levels: 72) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.67      0.07     0.55     0.80 1.01      805      988\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         3.37      0.36     2.67     4.08 1.01      694     1119\nDay              -0.11      0.02    -0.14    -0.08 1.00     5705     1843\nAge              -0.00      0.01    -0.01     0.01 1.01      745     1373\nSexMale          -0.39      0.17    -0.73    -0.05 1.00      489      702\nAntibioticYes    -0.29      0.17    -0.63     0.05 1.01      590      827\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.55      0.02     0.51     0.59 1.00     4164     2480\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nlibrary(bayesplot)\nposterior_samples &lt;- as.array(crp_model_random_intercept)\n#mcmc_trace(posterior_samples, pars = c(\"b_Intercept\", \"b_Day\", \"b_Age\", \"b_SexMale\", \"b_SepsisSepticShock\",\"b_SepsisSevereSepsis\",\"b_AntibioticYes\",\"b_DischargDead\"))\nmcmc_trace(posterior_samples, pars = c(\"b_Intercept\", \"b_Day\", \"b_Age\", \"b_SexMale\", \"b_AntibioticYes\"))\n\n\n\n\n\n\n\n\n\nCode\nmcmc_trace(posterior_samples, pars = c(\"sd_ID__Intercept\", \"sigma\"))\n\n\n\n\n\n\n\n\n\nCode\n#plot(crp_model_random_intercept)\npp_check(crp_model_random_intercept)\n\n\n\n\n\n\n\n\n\nWe can plot the random intercepts with 95% credible intervals as follows:\n\n\nCode\nlibrary(brms)\nlibrary(tidyverse)\nrandom_intercepts &lt;- ranef(crp_model_random_intercept)$ID[, , \"Intercept\"] %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"ID\") %&gt;%\n  rename(Estimate = Estimate, Q2.5 = Q2.5, Q97.5 = Q97.5)\nrandom_intercepts$ID &lt;- factor(random_intercepts$ID, levels = random_intercepts$ID[order(random_intercepts$Estimate)])\nggplot(random_intercepts, aes(x = ID, y = Estimate)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +\n  coord_flip() +\n  labs(\n    title = \"Estimated Random Intercepts by Patient\",\n    x = \"Patient ID\",\n    y = \"Random Intercept Estimate (log-CRP/day)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nTo get the fitted lines for each individual we use the following R code:\n\n\nCode\nlibrary(brms)\nlibrary(tidyverse)\nnewdata &lt;- crp_data %&gt;%\n  select(ID, Day, Age, Sex, Antibiotic) %&gt;%\n  distinct()  \nfitted_values &lt;- fitted(\n  crp_model_random_intercept,\n  newdata = newdata,\n  re_formula = NULL,  \n  summary = TRUE      \n)\nfitted_df_log &lt;- bind_cols(newdata, as_tibble(fitted_values))\np1 &lt;- ggplot(fitted_df_log, aes(x = Day, y = Estimate, group = ID, color = ID)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.2) +\n  labs(title = \"CRP Trajectories (log scale)\",\n       y = \"Fitted CRP (log scale)\", x = \"Day\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n#fitted_df_original &lt;- fitted_df_log %&gt;%\n#  mutate(CRP_fitted = exp(Estimate),\n#         CRP_lower = exp(Q2.5),\n#         CRP_upper = exp(Q97.5))\n#p2 &lt;- ggplot(fitted_df_original, aes(x = Day, y = CRP_fitted, group = ID, color = ID)) +\n#  geom_line() +\n#  geom_ribbon(aes(ymin = CRP_lower, ymax = CRP_upper), alpha = 0.2) +\n#  labs(title = \"\",\n#       y = \"Fitted CRP\", x = \"Day\") +\n#  theme_minimal()\n#library(gridExtra)\n#grid.arrange(p1,p2, ncol=2)\np1\n\n\n\n\n\n\n\n\n\nMCMC Diagnostics\nWe can perform MCMC diagnostics in a similar way to how we did for Gaussian and non-Gaussian models in our previous lectures. In particular, in the partial pooling model, we might want to focus on the variability of the \\(\\beta_{0j}\\) parameter, i.e., \\(\\sigma_0^2\\), which is the group or cluster variability. Additionally, we should also consider the effective sample size values and examine the posterior predictive plots. We will explain more on the MCMC diagnostics in our next lecture.\nIntraclass Correlation Coefficient (ICC)\nFor a Gaussian model, such as continuous outcome variable C-reactive protine (CRP) in logarithm scale, we can compute the Intraclass Correlation Coefficient (ICC). The ICC quantifies the proportion of the total variance that is attributable to grouping (e.g., clusters, schools, individuals). In the context of a random intercept model, it’s:\n\\[\n\\text{ICC}_{\\text{MCMC}} = \\frac{\\sigma_{0,\\text{MCMC}}^2}{\\sigma_{0,\\text{MCMC}}^2+\\sigma_{\\text{MCMC}}^2}\n\\]\nwhere, the subscript \\(\\text{MCMC}\\) refers to the posterior MCMC samples that we obtained from the model for \\(\\sigma_0^2\\) and \\(\\sigma^2\\) variance parameters.\nHence, for the simple partial pooling model that we explained in today’s lecture, we get the posterior mean ICC and 95% credible interval of the ICC as:\n\n\nCode\nvc &lt;- VarCorr(crp_model_random_intercept)\ngroup_var &lt;- as.numeric(vc$ID$sd[,c(1,3,4)])^2\nresid_var &lt;- as.numeric(vc$residual__$sd[,c(1,3,4)])^2\nicc &lt;- group_var / (group_var + resid_var)\nnames(icc) &lt;- names(vc$ID$sd[,c(1,3,4)])\nicc &lt;- round(icc, 2)\ncat(\n  \"ICC:\\n\",\n  paste(capture.output(icc), collapse = \"\\n\"),\n  \"\\n\"\n)\n\n\nICC:\n Estimate     Q2.5    Q97.5 \n    0.60     0.54     0.65",
    "crumbs": [
      "Week 9: **Cluster Smart with Bayes**"
    ]
  },
  {
    "objectID": "M05_1.html#summary",
    "href": "M05_1.html#summary",
    "title": "Week 9: Cluster Smart with Bayes",
    "section": "Summary",
    "text": "Summary\nIn today’s lecture, we explored Bayesian modelling in the context of clustered data. We began by discussing how accounting for the clustering structure is essential to avoid misleading inferences. To handle clustered data effectively, we introduced the Bayesian hierarchical model, which provides a principled way to model variability both within and across clusters. We examined three approaches to pooling information across clusters: complete pooling, where all clusters are assumed to share the same parameters; no pooling, where each cluster is modelled entirely independently; and partial pooling, where clusters share information through a common prior distribution. Partial pooling, as implemented via hierarchical models, allows for shrinkage of estimates toward a global mean, balancing between overfitting and underfitting.",
    "crumbs": [
      "Week 9: **Cluster Smart with Bayes**"
    ]
  },
  {
    "objectID": "M05_1.html#live-tutorial-and-discussion",
    "href": "M05_1.html#live-tutorial-and-discussion",
    "title": "Week 9: Cluster Smart with Bayes",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 9: **Cluster Smart with Bayes**"
    ]
  },
  {
    "objectID": "M05_1.html#tutorial-exercises",
    "href": "M05_1.html#tutorial-exercises",
    "title": "Week 9: Cluster Smart with Bayes",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.",
    "crumbs": [
      "Week 9: **Cluster Smart with Bayes**"
    ]
  },
  {
    "objectID": "M05_2.html",
    "href": "M05_2.html",
    "title": "Week 10: Goldilocks",
    "section": "",
    "text": "Learnings\n– LO2: Demonstrate how to specify and fit simple Bayesian models with appropriate attention to the role of the prior distribution and the data model.\n– LO4: Demonstrate proficiency in using statistical software packages (R) to specify and fit models, assess model fit, detect and remediate non-convergence, and compare models.\n– LO5: Engage in specifying, checking and interpreting Bayesian statistical analyses in practical problems using effective communication with health and medical investigators.\nBy the end of this week you should be able to:\n– Gain insight into different versions of the partial pooling model.\n– Construct Bayesian hierarchical models for binary outcome.\n– Interpret real-life clustered data problems in Bayesian context.",
    "crumbs": [
      "Week 10: **Goldilocks**"
    ]
  },
  {
    "objectID": "M05_2.html#learnings",
    "href": "M05_2.html#learnings",
    "title": "Week 10: Goldilocks",
    "section": "",
    "text": "Outcomes\n\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 10: **Goldilocks**"
    ]
  },
  {
    "objectID": "M05_2.html#partial-pooling-with-random-slope-intercept",
    "href": "M05_2.html#partial-pooling-with-random-slope-intercept",
    "title": "Week 10: Goldilocks",
    "section": "Partial Pooling with Random Slope & Intercept",
    "text": "Partial Pooling with Random Slope & Intercept\n\n\nRecall the C-reactive protein (CRP) data, where we are interested in modelling the daily progression of C-reactive protein levels in patients admitted to the intensive care unit during the first six days following admission. Since each patient has multiple CRP measurements over time, and we expect substantial variability between patients in their baseline inflammation levels, we use a Bayesian hierarchical (multilevel) model, such as in our last lecture we used the partial pooling model with a random intercept to account for individual differences while borrowing strength across the patient population.\n\nNow, what if we are interested to know different types of heterogeneous effects that may arise from the clustering. For example, we want to allow both varying baseline CRP levels and also its rate of change over time to differe among patients. To address this we can use a another version of partial pooling, where both intercepts and slopes vary by patient.\nIn the random intercept and slope model, we allow both (i) the intercept (baseline CRP level on day 0 or 1) and (ii) the slope (how CRP changes across days in the ICU) to vary by patient.\nThis accounts for the biological intuition that some patients start with higher or lower CRP levels and the trajectory of inflammation (rise or fall of CRP over time) can differ across individuals. This kind of model captures heterogeneous responses over time while still sharing information across patients through partial pooling.\n\nModel Specification\nIn this hierarchical Bayesian model, we aim to describe how C-reactive protein (CRP) levels (in log scale) evolve over time in critically ill patients admitted to the ICU. The model accounts for both population-level effects and patient-specific deviations, allowing us to flexibly capture heterogeneity in inflammation patterns across individuals. The observed CRP level (in log scale) for patient \\(j\\) on day \\(i\\), denoted \\(\\text{log(CRP)}_{ij}\\), is modelled as normally distributed around a mean, say \\(\\mu_{ij}\\), with constant residual variance \\(\\sigma^2\\). This forms the first level of the model, reflecting within-patient variation.\nThe mean\n\\[\n\\mu_{ij}=\\beta_0 + \\beta_{0j} + (\\beta_1 + \\beta_{1j}) \\cdot \\text{Day}_{ij} + \\mathbf{x}_{ij} \\boldsymbol{\\beta}\n\\]\nrepresents the expected log-CRP level and is composed of several components. It includes a global intercept \\(\\beta_0\\), a patient-specific random intercept \\(\\beta_{0j}\\), a global slope for time \\(\\beta_1\\) (reflecting the average effect of ICU day), and a patient-specific random slope \\(\\beta_{1j}\\), which captures how the rate of CRP change over time varies across patients. The model also includes a set of fixed effects \\(\\boldsymbol{\\beta}\\) corresponding to patient-level covariates such as age, sex, sepsis category, antibiotic timing, and ICU discharge status. These covariates are encoded in the vector \\(\\mathbf{x}_{ij}\\), and their contribution to the mean is given by the inner product \\(\\mathbf{x}_{ij}\\boldsymbol{\\beta}\\).\nAt the second level of the model, the random intercept \\(\\beta_{0j}\\) and random slope \\(\\beta_{1j}\\) for each patient are modeled jointly as a bivariate normal distribution centered at zero. This distribution is parameterised by a variance-covariance matrix that includes the variance of the random intercepts \\(\\sigma_0^2\\), the variance of the random slopes \\(\\sigma_1^2\\), and the correlation \\(\\rho\\) between them. This structure allows for partial pooling: each patient’s intercept and slope are informed not only by their own data but also by the overall distribution of intercepts and slopes across all patients. The correlation term \\(\\rho\\) enables us to capture whether patients with higher baseline CRP levels tend to experience faster or slower changes in inflammation over time.\nWe write the hierarchical structure of the random slope and intercept model as follows:\n\\[\n\\begin{aligned}\n\\text{log(CRP)}_{ij} &\\sim N(\\beta_0 + \\beta_{0j} + (\\beta_1 + \\beta_{1j}) \\cdot \\text{Day}_{ij} + \\mathbf{x}_{ij} \\boldsymbol{\\beta}, \\sigma^2) \\\\\n\\begin{bmatrix}\n\\beta_{0j} \\\\\n\\beta_{1j}\n\\end{bmatrix}\n&\\sim \\text{MVN}\\left(\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix},\n\\Sigma = \\begin{bmatrix}\n\\sigma_0^2 & \\rho \\sigma_0 \\sigma_1 \\\\\n\\rho \\sigma_0 \\sigma_1 & \\sigma_1^2\n\\end{bmatrix}\n\\right)\n\\end{aligned}\n\\]\nwhere,\n– \\(\\beta_0\\): global intercept. – \\(\\beta_1\\): global slope for time (Day). – \\(\\beta_{0j}\\), \\(\\beta_{1j}\\): patient-specific random deviations (random effects). – \\(\\boldsymbol{\\beta}\\): fixed effects for other covariates (e.g. Age, Sex, and Antibiotic). – \\(\\sigma\\): residual standard deviation. – The random effects for intercept and slope are correlated via \\(\\rho\\).\nHere, each patient has their own intercept and slope, but those are not estimated in isolation. Instead, patient-specific estimates are regularised (shrunken) toward the population average. This balances overfitting (too much flexibility) and underfitting (forcing everyone to follow the same trend).\nWe can see, a random intercept allows each patient to have a unique baseline CRP on ICU Day 1. Whereas, a random slope allows each patient to have their own CRP trajectory across the six ICU days, e.g., fast responders vs. slow responders vs. worsening patients. Furthermore, the fixed effects (age, sex, antibiotics, etc.) apply consistently across patients and help explain variability beyond time and patient ID.\nOne of the key benefits of using random slope over random intercept alone is to captures individual disease dynamics, i.e., in our example the different rates of inflammation. It also provides more accurate modelling of time trends without assuming identical change for all and provides better predictions, especially over time within individuals.\n\n\nPrior Distributions\nNow let us discuss the model and related prior distributions and explain the priors step-by-step: \\(\\beta_0, \\beta_1 \\sim \\mathcal{N}(0, 10^2)\\) are mainly the weakly informative priors. The prior \\(\\boldsymbol{\\beta} \\sim \\mathcal{N}(0, 10^2 I)\\) is placed on the coefficients of other fixed effects: age, sex, sepsis type, antibiotic timing, and discharge status. Again, it’s weakly informative and assumes we don’t have strong prior knowledge, but that extremely large effect sizes are unlikely.\nNow for the random effects (patient-level intercept and slope), i.e.,\n\\[\n\\begin{bmatrix} \\beta_{0j} \\\\ \\beta_{1j} \\end{bmatrix} \\sim \\text{MVN} \\left( \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\Sigma \\right)\n\\]\nThis is a bivariate normal distribution, modelling the patient-specific deviations in intercept (\\(\\beta_{0j}\\)) and slope (\\(\\beta_{1j}\\)) from the global values. It allows each patient to have their own baseline CRP and rate of change over time. The covariance matrix \\(\\Sigma\\) is:\n\\[\n  \\Sigma =\n  \\begin{bmatrix}\n  \\sigma_0^2 & \\rho \\sigma_0 \\sigma_1 \\\\\n  \\rho \\sigma_0 \\sigma_1 & \\sigma_1^2\n  \\end{bmatrix}\n\\]\nNote that options for \\(\\Sigma\\) includes the Inverse-Wishart distribution, which is the conjugate prior for the covariance matrix of a multivariate/bivariate normal model, and priors set not directly on the covariance matrix but on a decomposition of the matrix, in particular, the Cholesky decomposition can be used in this context. Hence, this structure allows us to estimate variability in baseline CRP (\\(\\sigma_0\\)), CRP trajectory over time (\\(\\sigma_1\\)) and to learn the correlation (\\(\\rho\\)) between baseline and change. Now, we use half-Cauchy distribution for \\(\\sigma_0\\), \\(\\sigma_1\\) and \\(\\sigma\\). As we have already discussed in our previous modules that half-Cauchy is commonly used as a weakly informative prior on scale parameters.\nNow, for the correlation (\\(\\rho\\)), LKJ (Lewandowski–Kurowicka–Joe) prior is a standard prior in Bayesian hierarchical models (Lewandowski, Kurowicka, and Joe (2009)). And a shape parameter of 2, gently favors correlations near 0 (i.e., assumes weak prior belief in strong correlation), but still allows the model to estimate a strong correlation if present in the data.\nHence, we can write these as:\n\\[\n\\begin{aligned}\n\\beta_0, \\beta_1 &\\sim N(0, 10^2); \\\\\n\\boldsymbol{\\beta} &\\sim N(0, 10^2 I) \\\\\n\\sigma, \\sigma_0, \\sigma_1 &\\sim \\text{Half-Cauchy}(0, 1); \\\\\n\\rho &\\sim \\text{LKJ}(2)\n\\end{aligned}\n\\]\nBased on the equations we can draw the DAG as:\n\n\n\n\n\n\n\n\nR-Stan (brm) Example\nNow we will explore R code to apply these types of model for our data on C-reactive protein (CRP). We will develop the R code based on the DAG we developed. This model estimates CRP (in log scale) using a mix of demographic and clinical predictors while accounting for individual-level variation over time. It includes both fixed effects (shared across the population) and random effects (individual-specific intercepts and slopes for time, i.e., Day). The outcome is modeled assuming a Gaussian distribution, with an identity link function. A total of 432 observations from 72 individuals were analysed. To represent the partial pooling model with random slop and intercept model, we use (1 + Day | ID) inside the formula argument of brm function.\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\ncrp_data &lt;- read.csv(\"crp_data_complete.csv\")\ncrp_data &lt;- tibble(\n  ID = crp_data$ID,\n  CRP = crp_data$crp,\n  CRP_log = log(crp_data$crp),\n  Day = crp_data$day,\n  Antibiotic = as.factor(crp_data$antib_1h),\n  Age = crp_data$age,\n  Sex = as.factor(crp_data$SEX),\n  Sepsis = as.factor(crp_data$SEPSIS),\n  Discharg = as.factor(crp_data$discharge_r)\n)\npriors &lt;- c(\n  prior(normal(0, 10), class = \"Intercept\"),\n  prior(normal(0, 10), class = \"b\"),\n  prior(cauchy(0, 1), class = \"sd\"),\n  prior(lkj(2), class = \"cor\"),\n  prior(cauchy(0, 1), class = \"sigma\")\n)\ncrp_model_random_slope &lt;- brm(\n  formula = CRP_log ~ Day + Age + Sex + Antibiotic + (1 + Day | ID),\n  data = crp_data,\n  family = gaussian(),\n  prior = priors,\n  chains = 3,\n  cores = 3,\n  iter = 2000,\n  warmup = 1000,\n  seed = 1234,\n  control = list(adapt_delta = 0.95)\n)\nprint(prior_summary(crp_model_random_slope, all = FALSE), show_df = FALSE)\n\n\nb ~ normal(0, 10)\nIntercept ~ normal(0, 10)\nL ~ lkj_corr_cholesky(2)\n&lt;lower=0&gt; sd ~ cauchy(0, 1)\n&lt;lower=0&gt; sigma ~ cauchy(0, 1)\n\n\nCode\nsummary(crp_model_random_slope)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: CRP_log ~ Day + Age + Sex + Antibiotic + (1 + Day | ID) \n   Data: crp_data (Number of observations: 432) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nMultilevel Hyperparameters:\n~ID (Number of levels: 72) \n                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)          0.62      0.07     0.48     0.78 1.00     1160     1756\nsd(Day)                0.20      0.02     0.16     0.24 1.01      474      483\ncor(Intercept,Day)    -0.42      0.12    -0.63    -0.15 1.01      256      497\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         3.49      0.33     2.87     4.15 1.00     1317     1660\nDay              -0.11      0.03    -0.16    -0.06 1.01      572      757\nAge              -0.01      0.00    -0.01     0.00 1.00     1316     1698\nSexMale          -0.50      0.16    -0.80    -0.20 1.00     1040     1664\nAntibioticYes    -0.20      0.15    -0.49     0.09 1.00      829     1193\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.41      0.02     0.38     0.44 1.00     1869     2348\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIn terms of population-level trends, CRP (in log scale) decreases significantly over time, with an average slope of -0.11 (95% credible interval: -0.16 to -0.06), indicating that inflammation, as measured by CRP, tends to reduce as patients progress through their hospital stay. Age has a small but consistent negative association with CRP, suggesting that older patients tend to have slightly lower CRP levels. Male patients also show significantly lower CRP compared to females, with an estimated difference of -0.5 (95% CI: -0.8 to -0.2). Antibiotic use is associated with a modest reduction in CRP, but the effect is not statistically conclusive.\nThe model includes random intercepts and slopes for each individual, capturing how baseline CRP and its trajectory over time differ from person to person. The estimated standard deviation of the intercepts is 0.62, and for the slopes (Day), it is 0.20, indicating meaningful between-subject variability. Importantly, the correlation between the intercept and the slope is -0.42, suggesting that individuals with higher baseline CRP levels tend to experience faster declines in CRP over time.\nLastly, the residual standard deviation (\\(\\sigma\\)) is estimated at 0.41, reflecting the variability in CRP that remains unexplained after accounting for both fixed and random effects. Model diagnostics (e.g., Rhat ≈ 1 and high effective sample sizes) indicate good convergence and reliable posterior estimation.\nNow we can plot both random intercepts and slops with their 95% credible interaval as follows:\n\n\nCode\nranef_data &lt;- ranef(crp_model_random_slope)$ID[, , \"Intercept\"] %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"ID\") %&gt;%\n  rename(Estimate = Estimate, Q2.5 = Q2.5, Q97.5 = Q97.5)\nranef_data$ID &lt;- factor(ranef_data$ID, levels = ranef_data$ID[order(ranef_data$Estimate)])\nggplot(ranef_data, aes(x = ID, y = Estimate)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +\n  coord_flip() +\n  labs(\n    title = \"Estimated Random Intercepts by Patient\",\n    x = \"Patient ID\",\n    y = \"Random Intercept Estimate (log-CRP scale)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n##\nslope_data &lt;- ranef(crp_model_random_slope)$ID[, , \"Day\"] %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"ID\") %&gt;%\n  rename(Estimate = Estimate, Q2.5 = Q2.5, Q97.5 = Q97.5)\nslope_data$ID &lt;- factor(slope_data$ID, levels = slope_data$ID[order(slope_data$Estimate)])\nggplot(slope_data, aes(x = ID, y = Estimate)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +\n  coord_flip() +\n  labs(\n    title = \"Estimated Random Slopes by Patient\",\n    x = \"Patient ID\",\n    y = \"Random Slope Estimate (log-CRP/day)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nTo get the fitted lines for each individual we use the following R code:\n\n\nCode\nnewdata &lt;- crp_data %&gt;%\n  select(ID, Day, Age, Sex, Sepsis, Antibiotic, Discharg) %&gt;%\n  distinct()\nfitted_values &lt;- fitted(\n  crp_model_random_slope,\n  newdata = newdata,\n  re_formula = NULL,\n  summary = TRUE\n)\nfitted_df_log &lt;- bind_cols(newdata, as_tibble(fitted_values))\np1 &lt;- ggplot(fitted_df_log, aes(x = Day, y = Estimate, group = ID, color = ID)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.2) +\n  labs(\n    title = \"CRP Trajectories (log scale)  - Random Slope\",\n    y = \"Fitted CRP (log scale)\",\n    x = \"Day\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\np1",
    "crumbs": [
      "Week 10: **Goldilocks**"
    ]
  },
  {
    "objectID": "M05_2.html#partial-pooling-with-only-random-slope",
    "href": "M05_2.html#partial-pooling-with-only-random-slope",
    "title": "Week 10: Goldilocks",
    "section": "Partial Pooling with Only Random Slope",
    "text": "Partial Pooling with Only Random Slope\n\n\nUsing partial pooling with only a random slope (and a fixed intercept) is a more targeted modelling choice. It means we’re assuming all groups start at the same baseline level, but respond differently to a predictor or covariate. This version of the partial pooling model removes the random intercept \\(\\beta_{0j}\\), leaving only a random slope \\(\\beta_{1j}\\) for Day. Since there’s only one random effect, no correlation structure (like \\(\\rho\\)) is required. The model assumes each patient has their own trajectory over time (slope), but shares the same intercept.\n\nModel Specification\nBased on the concept described above, we write the hierarchical model with prior specifications as follows:\n\\[\n\\begin{aligned}\n\\text{log(CRP)}_{ij} &\\sim N(\\beta_0 + (\\beta_1 + \\beta_{1j}) \\cdot \\text{Day}_{ij} + \\mathbf{x}_{ij} \\boldsymbol{\\beta}, \\sigma^2) \\\\\n\\beta_0, \\beta_1 &\\sim N(0, 10^2) \\\\\n\\boldsymbol{\\beta} &\\sim N(0, 10^2 I) \\\\\n\\beta_{1j}|\\sigma_1 &\\sim N(0, \\sigma_1^2) \\\\\n\\sigma_1, \\sigma &\\sim \\text{Half-Cauchy}(0, 1)\n\\end{aligned}\n\\]\nWhich can be drawn using the following DAG:\n\n\n\n\n\n\nFrom this DAG we can say, the outcome variable \\(\\text{log(CRP)}_{ij}\\) assumes a normal distribution. The mean structure includes a global intercept (\\(\\beta_0\\)) and global slope (\\(\\beta_1\\)), both of which have normal priors with large variance (\\(N(0, 10^2)\\)), reflecting weakly informative prior. In addition to these fixed effects, the model includes covariates \\(\\mathbf{x}_{ij}\\), which are associated with a coefficient vector \\(\\boldsymbol{\\beta}\\), also assumed to follow a multivariate normal prior with identity covariance scaled by \\(10^2\\).\nTo account for subject-level (or group-level) variation in the effect of time (e.g., “Day”), the model introduces a random slope \\(\\beta_{1j}\\), which allows the slope of “Day” to vary by group \\(j\\). This random slope is drawn from a normal distribution with standard deviation \\(\\sigma_1\\), which itself follows a Half-Cauchy(0,1) prior to constrain it to positive values and reflect uncertainty about its scale.\nThe model also includes a residual standard deviation parameter \\(\\sigma\\), representing unexplained variability in the outcome. Like \\(\\sigma_1\\), this parameter is given a Half-Cauchy(0,1) prior.\n\n\nR-Stan (brm) Example\nWe use following R code to run the partial pooling model with only random slope, where we consider writing (0 + Day | ID) formula argument using brm function.\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\ncrp_data &lt;- read.csv(\"crp_data_complete.csv\")\ncrp_data &lt;- tibble(\n  ID = crp_data$ID,\n  CRP = crp_data$crp,\n  CRP_log = log(crp_data$crp),\n  Day = crp_data$day,\n  Antibiotic = as.factor(crp_data$antib_1h),\n  Age = crp_data$age,\n  Sex = as.factor(crp_data$SEX),\n  Sepsis = as.factor(crp_data$SEPSIS),\n  Discharg = as.factor(crp_data$discharge_r)\n)\npriors &lt;- c(\n  prior(normal(0, 10), class = \"Intercept\"),\n  prior(normal(0, 10), class = \"b\"),\n  prior(cauchy(0, 1), class = \"sd\"),           # for random slope\n  prior(cauchy(0, 1), class = \"sigma\")         # residual SD\n)\ncrp_model_random_slope_only &lt;- brm(\n  formula = CRP_log ~ Day + Age + Sex + Antibiotic + (0 + Day | ID),\n  data = crp_data,\n  family = gaussian(),\n  prior = priors,\n  chains = 3,\n  cores = 3,\n  iter = 2000,\n  warmup = 1000,\n  seed = 1234,\n  control = list(adapt_delta = 0.95)\n)\nprint(prior_summary(crp_model_random_slope_only, all = FALSE), show_df = FALSE)\n\n\nb ~ normal(0, 10)\nIntercept ~ normal(0, 10)\n&lt;lower=0&gt; sd ~ cauchy(0, 1)\n&lt;lower=0&gt; sigma ~ cauchy(0, 1)\n\n\nCode\nsummary(crp_model_random_slope_only)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: CRP_log ~ Day + Age + Sex + Antibiotic + (0 + Day | ID) \n   Data: crp_data (Number of observations: 432) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nMultilevel Hyperparameters:\n~ID (Number of levels: 72) \n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Day)     0.18      0.02     0.15     0.21 1.00      653      760\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept         3.53      0.21     3.12     3.93 1.00     1722     2006\nDay              -0.11      0.02    -0.16    -0.06 1.00      685     1087\nAge              -0.01      0.00    -0.01    -0.00 1.00     1723     1803\nSexMale          -0.55      0.10    -0.74    -0.36 1.00     1676     1983\nAntibioticYes    -0.17      0.10    -0.36     0.01 1.00     1585     1992\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.50      0.02     0.47     0.54 1.00     3054     2380\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nslope_data &lt;- ranef(crp_model_random_slope_only)$ID[, , \"Day\"] %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"ID\") %&gt;%\n  rename(\n    Estimate = Estimate, \n    Q2.5 = Q2.5, \n    Q97.5 = Q97.5\n  )\nslope_data$ID &lt;- factor(slope_data$ID, levels = slope_data$ID[order(slope_data$Estimate)])\nggplot(slope_data, aes(x = ID, y = Estimate)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +\n  coord_flip() +\n  labs(\n    title = \"Estimated Random Slopes by Patient\",\n    x = \"Patient ID\",\n    y = \"Random Slope Estimate (log-CRP/day)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nnewdata &lt;- crp_data %&gt;%\n  select(ID, Day, Age, Sex, Sepsis, Antibiotic, Discharg) %&gt;%\n  distinct()\nfitted_values &lt;- fitted(\n  crp_model_random_slope_only,\n  newdata = newdata,\n  re_formula = NULL,\n  summary = TRUE\n)\nfitted_df_log &lt;- bind_cols(newdata, as_tibble(fitted_values))\nggplot(fitted_df_log, aes(x = Day, y = Estimate, group = ID, color = ID)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.2) +\n  labs(\n    title = \"CRP Trajectories (log scale) - Random Slope Only\",\n    y = \"Fitted CRP (log scale)\",\n    x = \"Day\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nIn our model, we estimate two main variance parameters that capture different sources of variability in the CRP measurements. First, the standard deviation of the random slope for Day across individuals is estimated to be about 0.18, with a 95% credible interval ranging from 0.15 to 0.21. This tells us that the effect of time (Day) on CRP levels varies somewhat from person to person. While there is a general trend captured by the global slope, individual trajectories differ around this average, reflecting meaningful heterogeneity in how CRP changes over time for different subjects.\nThe residual standard deviation, which represents variability in CRP that is not explained by either the fixed effects or the random slopes, is estimated to be approximately 0.5, with a 95% credible interval between 0.47 and 0.54. This residual variation captures other sources of noise or unexplained fluctuations in CRP measurements. Despite including several predictors and accounting for individual differences in the time effect, there remains substantial unexplained variation in CRP levels.\nBoth variance estimates are supported by high effective sample sizes and Rhat values equal to 1, indicating that our sampling and model convergence are reliable. Overall, these parameters highlight the importance of accounting for individual-level differences in the effect of time, as well as recognizing the inherent variability in CRP measurements that cannot be fully explained by our predictors.",
    "crumbs": [
      "Week 10: **Goldilocks**"
    ]
  },
  {
    "objectID": "M05_2.html#partial-pooling-with-binary-outcome",
    "href": "M05_2.html#partial-pooling-with-binary-outcome",
    "title": "Week 10: Goldilocks",
    "section": "Partial Pooling with Binary Outcome",
    "text": "Partial Pooling with Binary Outcome\n\n\nIn this section, we will explain the partial pooling model, when the outcome variable is binary. The hierarchy related to binary outcome variable model is same as we have already discussed earlier in this module. But the main difference is the use of logistic type hierarchical models. Let us now explain more using an example related to a trial related to toe nail infection.\n\nToe Nail Data\nThe toenail dataset originates from a randomised, double-blinded clinical trial investigating treatments for fungal toenail infection. A total of 378 subjects/patients were randomly assigned to one of two oral antifungal therapies: 250 mg/day of terbinafine or 200 mg/day of itraconazole. Patients were assessed at seven scheduled visits: weeks 0, 4, 8, 12, 24, 36, and 48. The primary outcome of interest is onycholysis, a condition characterised by the separation of the nail plate from the nail bed. For descriptive purposes and alignment with the study’s objectives, the visit weeks were converted to months.\n\n\nCode\nlibrary(tidyverse)\nlibrary(skimr)\ntoe_data &lt;- read.csv(\"toenail_data.csv\")\ntoe_data$treatment &lt;- factor(toe_data$treatment, levels = c(1, 0), labels = c(\"Terbinafine\", \"Itraconazole\"))\ntoe_data$outcome &lt;- factor(toe_data$outcome, levels = c(0, 1), labels = c(\"None or Mild\", \"Moderate or Severe\"))\ntoe_data &lt;- tibble(\n  Patient_ID = toe_data$patient,\n  Onycholysis = toe_data$outcome,\n  Treatment = toe_data$treatment,\n  Visit = toe_data$visit,\n  Month = toe_data$month\n)\nsummary_table &lt;- toe_data %&gt;%\n  group_by(Treatment, Onycholysis) %&gt;%\n  summarise(\n    Count = n(),\n    Avg_Month = mean(Month, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  group_by(Treatment) %&gt;%\n  mutate(Proportion = Count / sum(Count))\nlibrary(kableExtra)\nkable(summary_table, digits = 2, caption = \"Summary of Toe Data by Treatment and Onycholysis\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\nSummary of Toe Data by Treatment and Onycholysis\n\n\nTreatment\nOnycholysis\nCount\nAvg_Month\nProportion\n\n\n\n\nTerbinafine\nNone or Mild\n777\n5.28\n0.80\n\n\nTerbinafine\nModerate or Severe\n194\n2.21\n0.20\n\n\nItraconazole\nNone or Mild\n723\n5.30\n0.77\n\n\nItraconazole\nModerate or Severe\n214\n2.74\n0.23\n\n\n\n\n\n\n\nWe can summarised the severity of onycholysis (nail separation) by treatment group across all visits. For both itraconozole and terbinafine, the majority of observations fell into the mnne or mild category. In the itraconazole group, about 77% of visits showed none or mild onycholysis, with an average visit time of 5.3 months. The remaining 23% were classified as moderate or severe, and these cases tended to occur earlier, averaging around 2.7 months. Similarly, in the terbinafine group, about 80% of visits were none or mild, with a mean timing of 5.3 months. The moderate or severe cases made up 20%, occurring earlier on average, at around 2.2 months. These summary statistics suggest that more severe symptoms tend to be reported earlier in treatment, while milder outcomes are more common later on. This trend appears consistent across both treatment groups, though terbinafine shows a slightly higher proportion of mild cases.\nNow, we will develop a model that can provide us more insights on the relationships.\n\n\nModel Development\nWe have repeated measurements for each patient over time, which are likely to be correlated within individuals. To appropriately account for this intra-subject correlation, we employ a partial pooling model, especifically, a hierarchical model with random (varying) intercepts across patients. Here, the outcome variable is onycholysis, indicating the severity of nail separation. The exposure variable is the treatment group (itraconozole or terbinafine). We also include Month (time since baseline) as a covariate to capture temporal changes in outcome. The model can be written mathematically as:\n\\[\n\\text{Onycholysis}_{ij} \\sim \\text{Bernoulli}(p_{ij})\n\\]\n\\[\n\\text{logit}(p_{ij}) = \\beta_0 + \\beta_{0j} + \\beta_1 \\cdot \\text{Treatment}_{ij} + \\beta_2 \\cdot \\text{Month}_{ij}\n\\]\n\\[\n\\beta_{0j}|\\sigma_0 \\sim N(0, \\sigma_0^2); \\quad \\sigma_0 \\sim \\text{Half-Cauchy}(0,1)\n\\]\nwhere, \\(i\\) is the repeated measurements (visits), and \\(j\\) indexes patients. Following our learning in previous lecture, the term \\(\\beta_0\\) is the global intercept, and \\(\\beta_{0j}\\) is the random (or varying) intercept for patient \\(j\\). The term \\(\\beta_1\\) is the treatment effect, and \\(\\sigma_0^2\\) captures the between-patient variability. You can see, this model assumes that each patient has their own baseline risk (via \\(\\beta_{0j}\\)), while sharing information across the entire population through partial pooling.\n\n\n\n\n\n\n\n\nR-Stan (brm) Example\nBased on the model we developed for the toenail data and research aims, we will now impliment the hierarchical model using R code.\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\ntoe_data &lt;- read.csv(\"toenail_data.csv\")\ntoe_data$treatment &lt;- factor(toe_data$treatment, levels = c(0, 1), labels = c(\"Itraconozole\", \"Terbinafine\"))\ntoe_data$outcome &lt;- factor(toe_data$outcome, levels = c(0, 1), labels = c(\"None or Mild\", \"Moderate or Severe\"))\ntoe_data &lt;- tibble(\n  Patient_ID = toe_data$patient,\n  Onycholysis = toe_data$outcome,\n  Treatment = toe_data$treatment,\n  Visit = toe_data$visit,\n  Month = toe_data$month\n)\npriors &lt;- c(\n  prior(normal(0, 10), class = \"Intercept\"),  \n  prior(normal(0, 10), class = \"b\"),          \n  prior(cauchy(0, 1), class = \"sd\")           \n)\nonycholysis_model &lt;- brm(\n  formula = Onycholysis ~ Treatment + Month + (1 | Patient_ID),\n  data = toe_data,\n  family = bernoulli(link = \"logit\"),\n  prior = priors,\n  chains = 3,\n  cores = 3,\n  iter = 2000,\n  warmup = 1000,\n  seed = 1234,\n  control = list(adapt_delta = 0.95)\n)\nprint(prior_summary(onycholysis_model, all = FALSE), show_df = FALSE)\n\n\nb ~ normal(0, 10)\nIntercept ~ normal(0, 10)\n&lt;lower=0&gt; sd ~ cauchy(0, 1)\n\n\nCode\nsummary(onycholysis_model)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Onycholysis ~ Treatment + Month + (1 | Patient_ID) \n   Data: toe_data (Number of observations: 1908) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nMultilevel Hyperparameters:\n~Patient_ID (Number of levels: 294) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     4.09      0.39     3.38     4.94 1.00      820     1428\n\nRegression Coefficients:\n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept               -1.46      0.44    -2.34    -0.66 1.01      545\nTreatmentTerbinafine    -0.53      0.58    -1.67     0.60 1.01      531\nMonth                   -0.46      0.04    -0.53    -0.38 1.00     2807\n                     Tail_ESS\nIntercept                 943\nTreatmentTerbinafine      935\nMonth                    2606\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs we have discuss in our previous module, brm output provides the posterior estimates in log-odds scale. The summary results above indicates the posterior estimates in log-odds. Hence, we can use the following R code to convert it into odds ratio (i.e., \\(\\exp(.)\\)). The interpretation of the posterior estimates is similar to what we explained for a Bayesian logistic regression using odds ratios.\n\n\nCode\nlibrary(gtsummary)\nonycholysis_model %&gt;%\n  tbl_regression(exponentiate = TRUE) %&gt;%\n  bold_labels()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nexp(Beta)\n95% CI\n\n\n\n\nTreatment\n\n\n\n\n\n\n    Itraconozole\n—\n—\n\n\n    Terbinafine\n0.59\n0.19, 1.82\n\n\nMonth\n0.63\n0.59, 0.68\n\n\n\nAbbreviation: CI = Credible Interval\n\n\n\n\n\n\n\n\nCode\nintercept_data &lt;- ranef(onycholysis_model)$Patient_ID[, , \"Intercept\"] %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(\"Patient_ID\") %&gt;%\n  rename(\n    Estimate = Estimate,\n    Q2.5 = Q2.5,\n    Q97.5 = Q97.5\n  )\nintercept_data$Patient_ID &lt;- factor(intercept_data$Patient_ID, levels = intercept_data$Patient_ID[order(intercept_data$Estimate)])\n#p1 &lt;- ggplot(intercept_data, aes(x = Patient_ID, y = Estimate)) +\n#  geom_point() +\n#  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +\n#  coord_flip() +\n#  labs(\n#    title = \"\",\n#    x = \"Patient ID\",\n#    y = \"Estimate (Log-Odds)\"\n#  ) +\n#  theme_minimal()\n#intercept_data &lt;- intercept_data %&gt;%\n#  mutate(Significant = ifelse(Q2.5 &gt; 0 | Q97.5 &lt; 0, \"Yes\", \"No\"))\n#p2 &lt;- ggplot(intercept_data, aes(x = Patient_ID, y = Estimate, color = Significant)) +\n#  geom_point() +\n#  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +\n#  coord_flip() +\n#  labs(\n#    title = \"Significance Highlighted\",\n#    x = \"Patient ID\",\n#    y = \"Estimate (Log-Odds)\",\n#    color = \"Significant?\"\n#  ) +\n#  theme_minimal()\n#library(gridExtra)\n#grid.arrange(p1,p2,col=2)\n#library(patchwork)\n#p1 + p2 + \n#  plot_layout(ncol = 2) + \n#  plot_annotation(title = \"Random (Varying) Effects\")\n#ggplot(intercept_data, aes(y = Patient_ID, x = Estimate)) +\n#  geom_point() +\n#  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +\n#  coord_flip() +\n#  labs(\n#    title = \"\",\n#    y = \"Patient ID\",\n#    x = \"Estimate\"\n#  ) +\n#  theme_minimal()\nlibrary(tidyverse)\nlibrary(brms)\nnewdata &lt;- toe_data %&gt;%\n  select(Patient_ID, Month, Treatment) %&gt;%\n  distinct()\nfitted_values &lt;- fitted(\n  onycholysis_model,\n  newdata = newdata,\n  re_formula = NULL,  # Includes random effects\n  summary = TRUE\n)\nfitted_df &lt;- bind_cols(newdata, as_tibble(fitted_values))\nggplot(fitted_df, aes(x = Month, y = Estimate, group = Patient_ID, color = Patient_ID)) +\n  geom_line() +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.2) +\n  labs(\n    title = \"Predicted Probability of Moderate/Severe Onycholysis\",\n    y = \"Predicted Probability\",\n    x = \"Month\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Week 10: **Goldilocks**"
    ]
  },
  {
    "objectID": "M05_2.html#summary",
    "href": "M05_2.html#summary",
    "title": "Week 10: Goldilocks",
    "section": "Summary",
    "text": "Summary\nIn this lecture, we explored various hierarchical modelling approaches using partial pooling. We began with models that include both random (or varying) intercepts and slopes, followed by models with only random slopes, illustrating how these structures account for individual-level variation. We then extended the framework to binary outcomes, demonstrating how to implement Bayesian logistic hierarchical (or mixed-effects) models. Throughout, we provided practical examples and R code to help apply these methods to real-world data. These models allow us to make more accurate and generalisable inferences by borrowing strength across groups while accounting for within-group differences.",
    "crumbs": [
      "Week 10: **Goldilocks**"
    ]
  },
  {
    "objectID": "M05_2.html#live-tutorial-and-discussion",
    "href": "M05_2.html#live-tutorial-and-discussion",
    "title": "Week 10: Goldilocks",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 10: **Goldilocks**"
    ]
  },
  {
    "objectID": "M05_2.html#tutorial-exercises",
    "href": "M05_2.html#tutorial-exercises",
    "title": "Week 10: Goldilocks",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.\n\n\n\n\nLewandowski, Daniel, Dorota Kurowicka, and Harry Joe. 2009. “Generating Random Correlation Matrices Based on Vines and Extended Onion Method.” Journal of Multivariate Analysis 100 (9): 1989–2001. https://doi.org/https://doi.org/10.1016/j.jmva.2009.04.008.",
    "crumbs": [
      "Week 10: **Goldilocks**"
    ]
  },
  {
    "objectID": "brief_module_06.html",
    "href": "brief_module_06.html",
    "title": "Module 6: Wander into the Wonder!",
    "section": "",
    "text": "Summary\nIn this final module of the unit, we deepen our understanding of sample size determination through the lens of Bayesian inference, exploring how Bayesian methods offer a principled and flexible framework for designing studies under uncertainty. Traditional sample size calculations often rely on fixed assumptions and binary decision rules, but the Bayesian approach allows us to incorporate prior knowledge, account for uncertainty in key parameters, and make probabilistically coherent decisions.\nWe begin by examining the foundations of Bayesian sample size calculations, contrasting them with their frequentist counterparts. While frequentist methods typically target power for a specific effect size under a fixed hypothesis, Bayesian designs revolve around decision criteria, such as expected utility or posterior probabilities. This shift allows for richer, more context-aware design strategies that align with specific goals of a study. In this module we will only explore the posterior probability decision rule.\nTo ground these concepts, we compare frequentist and Bayesian sample size methods, highlighting differences in objectives, and flexibility. Through case studies, we see that Bayesian designs often allow for smaller sample sizes by borrowing prior information or adapting as data accumulate, features that can be particularly advantageous in settings with limited resources or ethical concerns.\nA critical consideration in Bayesian design is sensitivity analysis, assessing how sample size recommendations or decisions vary under different prior assumptions. We introduce the role of Bayesian type-I error, a probabilistic analogue to the frequentist error rate, in balancing false discoveries with detection power.\nWe then briefly explore the concept of Bayesian adaptive approaches for sample size determination, where interim data inform real-time decisions about continuing, stopping, or modifying a study. This adaptive framework supports more ethical and efficient designs, allowing studies to respond to accumulating evidence rather than relying on rigid pre-specified plans.\nFinally, we discuss the Bayesian model selection, using tools such as Bayes factor and information criteria like the Deviance Information Criterion (DIC), Watanabe-Akaike Information Criterion (WAIC), and Leave-One-Out Cross-Validation (LOO). These criteria help compare and choose models based on predictive accuracy and complexity, guiding us toward parsimonious yet effective designs.\nThroughout the module, we apply these concepts using hands-on exercises that guide us in specifying Bayesian designs, conducting sensitivity analyses, performing model comparisons, and interpreting decision outcomes.",
    "crumbs": [
      "**Module 6:** Wander into the Wonder!"
    ]
  },
  {
    "objectID": "M06_1.html",
    "href": "M06_1.html",
    "title": "Week 11: Size Matters!",
    "section": "",
    "text": "Learnings\n– LO1: Explain the difference between Bayesian and frequentist concepts of statistical inference.\n– LO5: Engage in specifying, checking and interpreting Bayesian statistical analyses in practical problems using effective communication with health and medical investigators.\nBy the end of this week you should be able to:\n– Understand the key concepts of Bayesian sample size calculations.\n– Explain the difference between Bayesian and classical methods.\n– Calculate sample size using Bayesian approach.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#learnings",
    "href": "M06_1.html#learnings",
    "title": "Week 11: Size Matters!",
    "section": "",
    "text": "Outcomes\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#bayesian-sample-size",
    "href": "M06_1.html#bayesian-sample-size",
    "title": "Week 11: Size Matters!",
    "section": "Bayesian Sample Size",
    "text": "Bayesian Sample Size\n\n\nBayesian sample size selection is a method used in the design of experiments or studies, especially useful in clinical trials that determines how many subjects or data points are needed, based on Bayesian statistical principles.\nRather than relying on traditional frequentist criteria (like fixed power and significance levels), Bayesian sample size selection incorporates prior knowledge (recall that we have already discussed the key concepts of Bayesian method in our previous modules) and focuses on decision-making under uncertainty.\nFor example, from frequentist view we can ask:\n\n“How many participants do I need to have an 80% chance of detecting a true effect if it exists?”\n\nWhereas, in Bayesian thoughts, we ask:\n\n“How many participants do I need so that, after seeing the data, I’m likely to have strong enough evidence to make a good decision?”\n\nIn Bayesian sample size determination, we choose a sample size that meets a predefined decision or utility criterion, based on the posterior distribution.\nFor example, let’s say we’re testing a new drug and we think there’s a fair chance it might work. To figure out how many people we need in the study, we simulate data using different sample sizes. For each one, we check how likely it is, after seeing the data, that the drug has a positive effect. We then choose the smallest sample size where, in 90% (say) of the simulations, that chance is greater than say 95%.\nThe Bayesian sample size seletion is very flexible in decision making and not tied to fixed p-values such as we see in frequentist method. It is also more natural for thinking adaptive ways of doing trials using interim analysis.\n\nThis type of sample size selection, while more computationally intensive due to its simulation-based nature, offers greater flexibility and is increasingly recognised for its potential in complex study designs. Regulatory agencies, such as the Therapeutic Goods Administration (TGA) in Australia, the Food and Drug Administration (FDA) in the USA, and the European Medicines Agency (EMA) in the EU, have traditionally relied on frequentist approaches, particularly in the context of clinical trials. However, Bayesian methods are steadily gaining acceptance, as they provide a richer framework for incorporating prior knowledge and interpreting results through posterior probabilities. While the design process, including prior specification and simulation-based sample size determination, is still evolving and less standardised, ongoing dialogue and guidance development are helping to bridge this familiarity gap and foster broader adoption in regulatory settings.\nNow, let us discuss this with specific terms starting with explaining randomised control trials (RCTs) briefly.\n\nRCTs: Where Size Really Matters\nAn RCT is a type of scientific study we use to find out whether a treatment, intervention, or program actually works. RCTs are considered one of the most reliable ways to prove cause and effect.\nIn an RCT, we start by selecting a group of participants, usualy with common baseline characteristics. We then randomly assign them into two or more groups. One group receives the treatment we are testing, while the other group receives either a placebo, which looks like the treatment but has no real effect, or the usual standard treatment. Randomisation helps make sure the groups are similar at the beginning of the study. This way, if there is a difference in outcomes at the end, we can be more confident that it is because of the treatment and not because of other factors.\nBefore the study begins, we decide what we want to measure. This is called the outcome. Depending on what we are testing, outcomes can include things like changes in symptoms, blood pressure, infection rates, or even school test scores. Measuring the right outcomes is key to understanding whether the treatment worked.\nFrom statistical perspective, outcome can be represented using distributions. For example, if outcome is continuous then we can use normal distribution and if it is binary then use bernoulli distribution.\nWhen we do an RCT, we are setting up a fair and careful design to see if something really works. Sample size determination plays a crucial role here by ensuring that the study has enough statistical power to detect a true effect if one exists. It helps to estimate the minimum number of participants needed to achieve reliable and valid results, while minimising the risk of Type I (false positive) and Type II (false negative) errors.\nA well calculated sample size allows for accurate comparisons between treatment groups, ensuring that the study can confidently evaluate the intervention’s efficacy. It also helps optimise resources by preventing over-recruitment, which can be costly and unnecessary, or under-recruitment, which can lead to inconclusive or biased results.\nIn this lecture, we will learn how we can utilise Bayesian methods to calculate sample size for an RCT design. We will also briefly discuss the flexibility of Bayesian methods in conducting adaptive trials. Additionally, we will see that the Bayesian approach provides a more nuanced understanding of uncertainty, potentially leading to more efficient trial designs and better resource allocation, making it an attractive option in situations where prior data or expert opinion is available.\nBefore discussing more on this, let us first explain the Bayesian decision rules, as they are a key part of determining the appropriate sample size.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#rcts-where-size-really-matters",
    "href": "M06_1.html#rcts-where-size-really-matters",
    "title": "Week 11: Size Matters!",
    "section": "RCTs: Where Size Really Matters",
    "text": "RCTs: Where Size Really Matters\nAn RCT is a type of scientific study we use to find out whether a treatment, intervention, or program actually works. RCTs are considered one of the most reliable ways to prove cause and effect.\nIn an RCT, we start by selecting a group of participants, usualy with common baseline characteristics. We then randomly assign them into two or more groups. One group receives the treatment we are testing, while the other group receives either a placebo, which looks like the treatment but has no real effect, or the usual standard treatment. Randomisation helps make sure the groups are similar at the beginning of the study. This way, if there is a difference in outcomes at the end, we can be more confident that it is because of the treatment and not because of other factors.\nBefore the study begins, we decide what we want to measure. This is called the outcome. Depending on what we are testing, outcomes can include things like changes in symptoms, blood pressure, infection rates, or even school test scores. Measuring the right outcomes is key to understanding whether the treatment worked.\nFrom statistical perspective, outcome can be represented using distributions. For example, if outcome is continuous then we can use normal distribution and if it is binary then use bernoulli distribution.\nWhen we do an RCT, we are setting up a fair and careful design to see if something really works. Sample size determination plays a crucial role here by ensuring that the study has enough statistical power to detect a true effect if one exists. It helps to estimate the minimum number of participants needed to achieve reliable and valid results, while minimising the risk of Type I (false positive) and Type II (false negative) errors.\nA well calculated sample size allows for accurate comparisons between treatment groups, ensuring that the study can confidently evaluate the intervention’s efficacy. It also helps optimise resources by preventing over-recruitment, which can be costly and unnecessary, or under-recruitment, which can lead to inconclusive or biased results.\nIn this lecture, we will learn how we can utilise Bayesian methods to calculate sample size for an RCT design. We will also briefly discuss the flexibility of Bayesian methods in conducting adaptive trials. Additionally, we will see that the Bayesian approach provides a more nuanced understanding of uncertainty, potentially leading to more efficient trial designs and better resource allocation, making it an attractive option in situations where prior data or expert opinion is available.\nBefore discussing more on this, let us first explain the Bayesian decision rules, as they are a key part of determining the appropriate sample size.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#bayesian-decision-criteria",
    "href": "M06_1.html#bayesian-decision-criteria",
    "title": "Week 11: Size Matters!",
    "section": "Bayesian Decision Criteria",
    "text": "Bayesian Decision Criteria\n\n\nBerry et al. (2010)\nDecision criteria can provide a flexible understanding of the choice of sample size. There are multiple options available to define the decision rules. For example, we can set a decision criterion to aim for a high probability threshold, based on the data that the treatment actually works. Sometimes, we might also try to ensure our decisions are worthwhile by considering factors like cost and benefit.\nThere are different Bayesian decision rules, below, we provide a brief discussion of some of the criteria.\nPosterior Probability Decision Rule:\nThis decision rule is based on the posterior probability that a treatment effect is either greater or less than a specified threshold (e.g., a clinically meaningful effect). In Bayesian terms, the posterior distribution of the treatment effect is updated as data accumulate.\nBayesian Predictive Probability:\nBayesian predictive probability calculates the probability of achieving a successful outcome (e.g., a significant treatment effect) based on the data already collected and the prior knowledge (prior distribution) about the treatment. This decision rule is similar to the posterior probability decision rule, however instead using posterior probability we use posterior predictive probability to make the decision.\nCredible Intervals for Treatment Effects:\nWe already know the credible interval of parameter calculated from the posterior distributions, and now we use this credible interval for treatment effects to define the decision rule. Here, we consider a credible interval threshold (say 95%), and check this range of values within which the true treatment effect lies with a given probability.\nUtility-Based Decision Rules:\nIn some cases, the decision to continue or stop a trial is based on a utility function that combines the benefits (e.g., treatment efficacy) and the costs (e.g., patient safety, cost of the trial), or any other decision functions. Bayesian methods allow for the specification of a utility function that can be used to guide decisions.\nIn this course, we will explore and discuss examples using decision criterion based on posterior probability decision rules only. We will also compare the Bayesian approach of the sample size calculations with the approach based on frequentist methods.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#bayesian-power",
    "href": "M06_1.html#bayesian-power",
    "title": "Week 11: Size Matters!",
    "section": "Bayesian Power",
    "text": "Bayesian Power\n\n\nIn frequentist statistics, power is the probability of correctly rejecting the null hypothesis when the alternative is true, i.e., detecting an effect if there truly is one.\nBut in Bayesian statistics, we think a bit differently. A Bayesian analogue of power is the probability that, under the assumed true effect, our decision rule leads us to the correct conclusion, such as declaring the treatment effective when it actually is.\nLet’s say we define success as the posterior probability that the treatment effect is greater than a particular value (e.g., 0) being at least 95%. In this case, if we take that value to be zero, then a successful outcome would indicate a positive treatment effect. So our decision rule is to declare treatment effective if\n\\[\nPr(\\text{effect} &gt; 0 \\mid \\text{data}) \\geq 0.95\n\\]\nThen, we can simulate many trials assuming the treatment actually works, and compute how often our posterior crosses the 0.95 threshold. This result, how often the rule leads to the right conclusion is our Bayesian power.\nNow, if under the assumed true effect, 80% of simulated trials lead us to declare the treatment effective using our decision rule, then we say the design has 80% power. This is conceptually similar to the frequentist idea, where we aim for 80% power as a common benchmark to ensure a reasonably high chance of detecting a true effect. Note that consideration of 80% power is seen as a good trade-off in many health science. However, it’s not a rule, just a convention, and different fields or high-stakes studies might set higher bars for the power.\nIn more advanced Bayesian decision theory, power is just one part of the equation. We can optimise sample size based on expected utility (e.g., benefit of correct decision, cost of study), where power feeds into the expected gain of making the right call. In this course, however, we will only discuss Bayesian power related to the posterior probability decision rule.\nNow, let’s explain this more with an example below, which reflects the power using posterior probability decision rule.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#frequentist-comparison",
    "href": "M06_1.html#frequentist-comparison",
    "title": "Week 11: Size Matters!",
    "section": "Frequentist Comparison",
    "text": "Frequentist Comparison\n\n\nHere, we want to determine the minimum sample size per group required to detect a treatment effect (from 60% to 75%) with 80% power, using a frequentist approach. Thus, we consider null hypothesis as: no improvement or the treatment is not better than the control, i.e., \\(H_0: \\theta_\\text{trt}\\leq \\theta_\\text{crt}\\). The alternative hypothesis we consider as: the treatment is better than the control (i.e., higher success rate), i.e., \\(H_1: \\theta_\\text{trt}&gt; \\theta_\\text{crt}\\).\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\np1 &lt;- ggplot(power_results, aes(x = SampleSize, y = Power)) +\n  geom_line(color = \"#0072B2\", size = 1.2) +\n  geom_point(color = \"#D55E00\", size = 2) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = min_n$SampleSize, linetype = \"dashed\", color = \"darkgreen\") +\n  labs(\n    title = \"Bayesian\",\n    x = \"Sample Size per Group\",\n    y = \"Power\"\n  ) +\n  ylim(0,1) +\n  theme_minimal(base_size = 14)\n\n# frequentist way\n\ntrue_control &lt;- 0.60  \ntrue_treatment &lt;- 0.75  \nalpha &lt;- 0.05  \nn_range &lt;- seq(50, 150, by = 5)  \npower_values &lt;- numeric(length(n_range))\nfor (i in seq_along(n_range)) {\n  result &lt;- power.prop.test(n = n_range[i],\n                            p1 = true_control,\n                            p2 = true_treatment,\n                            sig.level = alpha,\n                            alternative = \"one.sided\")\n  power_values[i] &lt;- result$power\n}\ndf &lt;- data.frame(SampleSize = n_range, Power = power_values)\nmin_n &lt;- df %&gt;% filter(Power &gt;= 0.80) %&gt;% slice(1)\nprint(paste(\"Frequentist: Sample size/group for 80% power: \", min_n$SampleSize))\n\n\n[1] \"Frequentist: Sample size/group for 80% power:  120\"\n\n\nCode\np2 &lt;- ggplot(df, aes(x = SampleSize, y = Power)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = min_n$SampleSize, linetype = \"dashed\", color = \"darkgreen\") +  \n  labs(\n    title = \"Frequentist\",\n    x = \"Sample Size per Group\",\n    y = \"Power\"\n  ) +\n  ylim(0,1) +  \n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  )\n\ngridExtra::grid.arrange(p1,p2,ncol=2)\n\n\n\n\n\n\n\n\n\nThe above plots provide power curves for both Bayesian and frequentist methods for sample sizes 50 to 150. We can see both plots show the power increases with the sample size and we can achieve 80% power at sample size 120 for both methods. This reflects under \\(\\text{Beta}(2,2)\\) prior we get same results for both Bayesian and frequentist methods based on the Bayesian decision rule we considered.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#when-they-are-different",
    "href": "M06_1.html#when-they-are-different",
    "title": "Week 11: Size Matters!",
    "section": "When they are different?",
    "text": "When they are different?\n\n\nNow, we will explore Bayesian sample size calculations by implementing sensitivity analyses. First, we will examine how the sample size changes with variations in the prior distribution. We will also explore sensitivity related to the choice of the cutoff threshold, ranging from 90% to 99%, which reflects the sensitivity related to the Bayesian decision rule.\n\nPrior Sensitivity\nIn real-world situations, we often don’t have much prior knowledge about how well a new treatment will work, so we can’t build a strong prior belief for it. However, for the standard treatment (SOC), there may be existing data or studies that provide useful information. In a Bayesian framework, this existing information about the SOC can help us design the study more efficiently, potentially reducing the number of participants needed.\nLet’s explain this using the Pediatric Antibiotic Trial example. Suppose, we find from a study that the minimum safety (or success) rate for the SOC, i.e., the control group is about 40% (i.e., this is an informative prior). So, we can write this prior using Beta distribution with shape parameters 4 and 6, i.e., \\(\\text{Beta}(4,6)\\).\nRecall that, in the example we considered the true success rate for the SOC is about 60%. This information is an assumption related to the observe data, which is different compared to the prior assumption for the SOC group.\nNow we might want to ask: if we already believe safety is around 40% (prior), why assume it’s 60%? Is there any need to use the 60% safety rate to design the trial?\nThe short answer for this question is yes, both the prior (40%) and the assumed “true” rate (60%) can be useful, but for different purposes in trial design. When we design a trial, especially using simulations, we need to ask if the true success rate is 60%, how likely is our trial to detect it? Here, the 60% value is a design assumption, a scenario we’re testing. We might say, if the true SOC rate is actually better than we thought, will our trial design pick that up? And the prior (40% safety) is our starting belief about the SOC success rate before the trial. When we get data, we will update this prior with new evidence to get the posterior distribution.\nWe can definitely see a key challenge in interpreting trial results in these types of situations. Here, the same data can lead to vastly different conclusions depending on the prior beliefs brought into the analysis. Therefore, the use of informative priors should be chosen very carefully, ideally motivated by strong prior clinical trial data and agreed upon in consultation with regulatory bodies (e.g., TGA, FDA and others).\nTo learn and explore more regarding this we refer to JAMA article by Quintana, Viele, and Lewis (2017).\nNow, let us assume that we have strong prior knowledge, and for the above example it is 40%, and hence, we plot the prior density function using \\(\\text{Beta}(4,6)\\) and true safety as:\n\n\nCode\nlibrary(ggplot2)\nx_vals &lt;- seq(0, 1, length.out = 1000)\nbeta_df &lt;- data.frame(\n  x = x_vals,\n  density = dbeta(x_vals, shape1 = 4, shape2 = 6)\n)\nggplot(beta_df, aes(x = x, y = density)) +\n  geom_area(fill = \"skyblue\", alpha = 0.5) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_vline(xintercept = 0.6, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = 0.61, y = max(beta_df$density)*0.8, label = \"True Safety = 0.6\", \n           vjust = -0.5, hjust = 0, color = \"red\", size = 4) +\n  labs(\n    title = \"Beta(4, 6) Prior with True Safety\",\n    x = expression(theta),\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nConsidering this informative prior we get the minimum sample size per group for 80% power as:\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nset.seed(0001)\nestimate_safety_power_pr &lt;- function(n_per_group, n_sim = 20000) {\n  mean(replicate(n_sim, simulate_safety_trial(n_per_group,\n                                              prior_control = c(4, 6),\n                                              prior_treatment = c(2, 2))))\n}\nsample_sizes &lt;- seq(50, 150, by = 5)\npower_results_pr &lt;- tibble(\n  SampleSize = sample_sizes,\n  Power = map_dbl(.x = sample_sizes, .f = estimate_safety_power_pr, .progress = TRUE)\n)\nmin_n &lt;- power_results_pr %&gt;% filter(Power &gt;= 0.80) %&gt;% slice(1)\nprint(paste(\"Bayesian: Sample size/group for 80% power: \", min_n$SampleSize))\n\n\n[1] \"Bayesian: Sample size/group for 80% power:  100\"\n\n\nCode\nggplot(power_results_pr, aes(x = SampleSize, y = Power)) +\n  geom_line(color = \"#0072B2\", size = 1.2) +\n  geom_point(color = \"#D55E00\", size = 2) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = min_n$SampleSize, linetype = \"dashed\", color = \"darkgreen\") +\n  labs(\n    title = \"Bayesian - Beta(4,6) Prior\",\n    x = \"Sample Size per Group\",\n    y = \"Power\"\n  ) +\n  ylim(0,1) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\n\n\nCutoff Sensitivity\nIn this section, we conduct a sensitivity analysis to examine how varying posterior probability cutoffs (0.90 vs 0.99) influence the estimated power across a range of sample sizes per group. We use data repeated across multiple simulations to estimate power for each scenario.\n\n\nCode\nlibrary(tidyverse)\nestimate_safety_power_cutoff &lt;- function(n_per_group, prob_cutoff, n_sim = 20000) {\n  mean(replicate(n_sim, simulate_safety_trial(n_per_group, prob_cutoff = prob_cutoff)))\n}\nsample_sizes &lt;- seq(50, 150, by = 5)\ncutoffs &lt;- c(0.90, 0.99)\nset.seed(0001)\npower_results_all &lt;- expand_grid(\n  SampleSize = sample_sizes,\n  Cutoff = cutoffs\n) %&gt;%\n  mutate(\n    Power = map2_dbl(SampleSize, Cutoff, ~estimate_safety_power_cutoff(.x, .y), .progress = TRUE)\n  )\nggplot(power_results_all, aes(x = SampleSize, y = Power, color = factor(Cutoff))) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"gray40\") +\n  labs(\n    title = \"Different Posterior Probability Cutoffs\",\n    x = \"Sample Size per Group\",\n    y = \"Power\",\n    color = \"Cutoff\"\n  ) +\n  ylim(0, 1) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nThe plot illustrates the relationship between posterior probability cutoffs and the statistical power of a safety trial, as the sample size increases. Specifically, it shows that using a lower cutoff value (e.g., 0.90) results in higher power more quickly compared to a higher cutoff (e.g., 0.99).\nThis means that when we set a cutoff of 0.90, we are requiring less evidence from the data to conclude that the treatment is safe. As a result, it’s easier to meet this criterion, and we reach acceptable power levels (e.g., 80%) with fewer participants.\nOn the other hand, with a stricter cutoff of 0.99, we are demanding more certainty before declaring safety. This makes it harder to reach the decision threshold, so the power of the test increases more slowly. To achieve the same level of power as with the 0.90 cutoff, we need a larger sample size to accumulate enough evidence.\nIn practical terms, this reflects a trade-off between statistical rigor and resource requirements:\n\nA lower cutoff may be more efficient in terms of sample size and cost but might increase the risk of falsely declaring a treatment safe.\nA higher cutoff increases the level of confidence in the safety conclusion but demands more data, which could be more expensive or time-consuming to collect.\n\nThus, this sensitivity analysis helps guide decision-making around trial design, balancing efficiency in safety evaluations.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#bayesian-type-i-error",
    "href": "M06_1.html#bayesian-type-i-error",
    "title": "Week 11: Size Matters!",
    "section": "Bayesian Type-I Error",
    "text": "Bayesian Type-I Error\n\n\nBayesian Type-I error is a concept that mirrors the classical (frequentist) Type-I error. In a frequentist hypothesis test, type-I error refers to the probability of rejecting the null hypothesis when it is actually true. For example, with statistical level of significance \\(\\alpha=0.05\\), we are accepting a 5% chance of falsely declaring a treatment effective when it’s not.\nIn Bayesian analysis, we don’t really reject null in the traditional way. Instead, we compute the posterior probability that the treatment is better than control, given the data.\nSo, say in a Bayesian superiority trial, as we have already explain earlier, we might want to declare a treatment superior, if the posterior probability that treatment is better than control exceeds some threshold (e.g., 95%). Here, Bayesian Type-I Error is the probability that we incorrectly declare superiority, when the treatment is not actually better (e.g., when there’s no difference, or control is better).\nNow, how do we estimate Bayesian Type-I Error (in practice)? We simulate many trials under the null hypothesis, and see how often our Bayesian decision rule declares a false positive.\n\nAssume null is true: i.e., treatment = control = 60% success.\nSimulate N trials (e.g., 10,000).\nFor each trial:\n\nCompute posterior probability that treatment is better than control.\nIf it exceeds your decision threshold (e.g., 95%), declare “superior.”\n\nCount how many of those trials wrongly declare superiority.\nThat proportion is our Bayesian Type-I Error rate.\n\nNow, even in a Bayesian framework, especially in regulated environments (e.g., clinical trials), controlling type-I error is critical for ethical, financial, and regulatory reasons.\nWhile Bayesian methods don’t inherently control \\(\\alpha\\) like frequentist ones do, regulators often require simulations to show that the Type-I error is controlled.\nExample:\nIf our decision rule is to declare a treatment superior if posterior P(treatment &gt; control) &gt; 0.95; then we might find:\n\nUnder the null (treatment = control), this rule gives a false positive 5% of the time, i.e, Type-I error is approximately 0.05\nIf it gives greater than 5% of the time, then Type-I error is too high and we may need to adjust the cutoff, sample size, or prior.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nsimulate_safety_trial &lt;- function(n_per_group, \n                                  true_control = 0.60, \n                                  true_treatment = 0.60,\n                                  prior_control = c(1, 1), \n                                  prior_treatment = c(1, 1),\n                                  prob_cutoff = 0.95,\n                                  threshold = 0) {\n  \n  success_c &lt;- rbinom(1, n_per_group, true_control)\n  success_t &lt;- rbinom(1, n_per_group, true_treatment)\n  \n  post_c &lt;- prior_control + c(success_c, n_per_group - success_c)\n  post_t &lt;- prior_treatment + c(success_t, n_per_group - success_t)\n  \n  n_samples &lt;- 500\n  p_c_post &lt;- rbeta(n_samples, post_c[1], post_c[2])\n  p_t_post &lt;- rbeta(n_samples, post_t[1], post_t[2])\n  \n  prob_superior &lt;- mean((p_t_post - p_c_post) &gt; threshold)\n  \n  return(prob_superior &gt;= prob_cutoff)\n}\nestimate_safety_power &lt;- function(n_per_group, prob_cutoff_val, n_sim = 10000) {\n  mean(replicate(n_sim, simulate_safety_trial(n_per_group, \n                                               prob_cutoff = prob_cutoff_val)))\n}\nsample_sizes &lt;- seq(50, 150, by = 5)\ncutoff_values &lt;- c(0.95) #c(0.95, 0.99)\npower_results_type1 &lt;- expand_grid(SampleSize = sample_sizes,\n                             ProbCutoff = cutoff_values) %&gt;%\n  mutate(Type1_Error = map2_dbl(SampleSize, ProbCutoff, estimate_safety_power, .progress = TRUE))\nggplot(power_results_type1, aes(x = SampleSize, y = Type1_Error, color = factor(ProbCutoff))) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Bayesian Type-I Error\",\n    x = \"Sample Size per Group\",\n    y = \"Type-I Error Rate\",\n    color = \"Cutoff\"\n  ) +\n  ylim(0, 0.20) +\n  theme_minimal(base_size = 14)",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#adaptivity-and-bayesian-sample-size",
    "href": "M06_1.html#adaptivity-and-bayesian-sample-size",
    "title": "Week 11: Size Matters!",
    "section": "Adaptivity and Bayesian Sample Size",
    "text": "Adaptivity and Bayesian Sample Size\n\n\nIn this course, we will introduce the fundamental conpects of Bayesian adaptive design. For interested readers to learn more, we refer to the book by Berry et al. (2010).\nAdaptive designs are an increasingly popular approach in clinical trials, offering flexibility to modify trial procedures based on accumulating data without undermining the study’s integrity or validity. One powerful framework for implementing adaptive designs is through Bayesian methods.\n\n\n\n\n\n\nAdaptive trials are designed with flexibility built in from the start. This means we can pre-plan adjustments to key elements like treatment types (dose, frequency, combinations), how patients are assigned to different treatments, which patient groups to include, and even how many participants are needed.\nWhat makes this approach powerful is its ability to learn as it goes. As data rolls in, the trial can start to focus more on the treatment arms that are showing promise. That allows us to begin with a broader range of options, maybe testing 8 doses instead of just 3, and still use fewer participants overall.\nA well-designed adaptive trial doesn’t just benefit researchers. Patients may get better treatments, regulators get more informative results, and the entire development process becomes more streamlined.\nThat said, the flip side is that adaptive designs take more effort to build. They require careful planning and trial simulations to make sure they perform well and stand up to regulatory standards. This means close collaboration of statisticians, clinicians, regulatory teams, operations, supply chain, and more all need to be in sync. When done right, though, the payoff is a trial design that’s not only smarter, but truly better for everyone involved.\n\n\n\nAntibiotic Safety Trial\nLet’s expain the adaptivity and realted sample size calculation based on the antibiotic safety trila we discussed earlier, where we want to evaluate if a new antibiotic is safer than the standard treatment for bacterial pneumonia in children aged 6 months to 5 years.\nDesign Features\n\nRandomisation: 1:1 (New Antibiotic vs. Standard of Care)\nPrior Beliefs: Non-informative priors for both arms (e.g., Beta(1,1) for safety rates)\nControl Safety Rate: Estimated at 60%\nTreatment Safety Rate: Estimated at 70% (say, about 10% point difference)\nPlanned Sample Size: Let’s assume 200 total (100 per group)\nInterim Analyses: After 50, 100, and 150 participants have been followed\n\nDecision Rule\n\nIf \\(Pr(\\theta_{trt} &gt; \\theta_{crt}) &gt; 0.95\\) then consider early success\nIf \\(Pr(\\theta_{trt} &lt; \\theta_{crt}) &gt; 0.95\\) then consider futility (stop for harm)\nElse continue to next interim or if in the final interim then either it is inconclusive or conclude as no added benefit.\n\n\n\nInterim Analyses\nAt the first interim, conducted after enrolling 50 participants (25 in each group), suppose we observed that 17 children in the treatment group (i.e., \\(\\approx\\) 70%) and 15 in the control group experienced no adverse events (i.e., 60%). Starting with weakly-informative \\(\\text{Beta}(2,2)\\) priors, we updated the posterior distributions based on this data. For the treatment group, this yielded a \\(\\text{Beta}(19, 10)\\) distribution, and for the control group, a \\(\\text{Beta}(17, 12)\\). We then calculated the posterior probability that the new antibiotic is safer than the standard treatment, especifically, \\(Pr(\\theta_{\\text{trt}} &gt; \\theta_{\\text{crt}})\\) (which is 0.71, see R code and results below). According to our decision rules, if this probability exceeds 0.95, we would consider stopping the trial early for efficacy. Conversely, if the probability that the treatment is worse exceeds 0.95, we would stop for futility. Since neither condition was met (i.e., this probability is 0.71), we proceeded to the next interim analysis.\nDuring the second interim analysis, conducted after 100 participants had been followed (50 per group), our cumulative data showed that 36 out of 50 participants in the treatment group and 29 out of 50 in the control group had no adverse events. We updated the posterior distributions accordingly: \\(\\text{Beta}(38, 16)\\) for the treatment group and \\(\\text{Beta}(31, 23)\\) for the control group. Once again, we computed the posterior probability of treatment superiority (which is as 0.93 in our case, see R code and results below). If this probability had exceeded our predefined threshold (greater than 0.95), we would have stopped the trial early for success. Since the threshold was not reached (i.e., this probability is 0.93), we decided to continue to the final planned interim.\nThe third interim analysis took place after 150 participants had been enrolled and followed (75 per group). At this point, 57 participants in the treatment arm and 45 in the control arm experienced no adverse events. These data produced updated posterior distributions: \\(\\text{Beta}(59, 20)\\) for the treatment and \\(\\text{Beta}(47, 32)\\) for the control. We calculated the final posterior probability that the treatment is safer than the control (which is 0.98, see R code and results below). If the final probability exceeded 0.95, we planned to declare the trial a success and conclude that the new antibiotic is statistically superior in terms of safety. Otherwise, we would interpret the result as either inconclusive or indicative of no added benefit.\nWe write R code for this analysis as follows that also provides us the power and decision for each interim steps.\n\n\nCode\nlibrary(ggplot2)\nposterior_prob_superiority &lt;- function(success_treat, total_treat, success_control, total_control, alpha_prior = 2, beta_prior = 2, n_sim = 5000) {\n  alpha_treat &lt;- alpha_prior + success_treat\n  beta_treat  &lt;- beta_prior + (total_treat - success_treat)\n  alpha_control &lt;- alpha_prior + success_control\n  beta_control  &lt;- beta_prior + (total_control - success_control)\n  p_treat_samples &lt;- rbeta(n_sim, alpha_treat, beta_treat)\n  p_control_samples &lt;- rbeta(n_sim, alpha_control, beta_control)\n  prob_superior &lt;- mean(p_treat_samples &gt; p_control_samples)\n  list(\n    prob_superior = prob_superior,\n    treat_posterior = c(alpha = alpha_treat, beta = beta_treat),\n    control_posterior = c(alpha = alpha_control, beta = beta_control),\n    samples = data.frame(p_treat = p_treat_samples, p_control = p_control_samples)\n  )\n}\ninterims &lt;- list(\n  interim1 = list(treat = c(success = 17, total = 25), control = c(success = 15, total = 25)),\n  interim2 = list(treat = c(success = 36, total = 50), control = c(success = 29, total = 50)),\n  interim3 = list(treat = c(success = 57, total = 75), control = c(success = 45, total = 75))\n)\nefficacy_thresh &lt;- 0.95\nfutility_thresh &lt;- 0.05\nfinal_success_thresh &lt;- 0.95\nfor (i in seq_along(interims)) {\n  \n  data &lt;- interims[[i]]\n  cat(paste(\"Interim: \", i,\" -- Sample Size: \", data$treat[2],\"\\n\"))\n\n  result &lt;- posterior_prob_superiority(\n    success_treat = data$treat[\"success\"],\n    total_treat = data$treat[\"total\"],\n    success_control = data$control[\"success\"],\n    total_control = data$control[\"total\"]\n  )\n  \n  cat(\"Posterior Pr(treat &gt; control):\", round(result$prob_superior, 4), \"\\n\")\n  \n  if (result$prob_superior &gt; efficacy_thresh) {\n    cat(\"Stop: Evidence for treatment superiority\\n\")\n    break\n  } else if (result$prob_superior &lt; (1 - efficacy_thresh)) {\n    cat(\"Stop: Futility (treatment worse)\\n\")\n    break\n  } else {\n    cat(\"Continue to next interim\\n\")\n  }\n}\n\n\nInterim:  1  -- Sample Size:  25 \nPosterior Pr(treat &gt; control): 0.7154 \nContinue to next interim\nInterim:  2  -- Sample Size:  50 \nPosterior Pr(treat &gt; control): 0.9304 \nContinue to next interim\nInterim:  3  -- Sample Size:  75 \nPosterior Pr(treat &gt; control): 0.9816 \nStop: Evidence for treatment superiority\n\n\nCode\nfinal_samples &lt;- result$samples\nggplot(final_samples, aes(x = p_treat - p_control)) +\n  geom_density(fill = \"steelblue\", alpha = 0.5) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Posterior Distribution of Difference: Pr(Treatment) - Pr(Control)\",\n    x = \"Difference in Safety Probabilities\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\nIt is also possible to do Bayesian power analyses for each intrem steps at the desing stage to identify the sample size required based on the decision rule.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#why-to-use-bayesian-sample-size",
    "href": "M06_1.html#why-to-use-bayesian-sample-size",
    "title": "Week 11: Size Matters!",
    "section": "Why to use Bayesian Sample Size",
    "text": "Why to use Bayesian Sample Size\nWith non-informative priors, Bayesian and frequentist designs often converge on similar results, especially for things like power and sample size. So then, why bother with Bayesian design at all?\nEven when power and sample sizes look similar, Bayesian methods offer several unique advantages that go beyond what frequentist designs provide.\nFor example,\nInterpretability of Results\nIn frequentist approach, we use p-value, which tells us the probability of observing data as extreme as ours, assuming the null hypothesis is true. Whereas, in Bayesian, we can directly compute the probability that a treatment works, given the data, which is often what clinicians and patients care about. For example, we might find that there is a 92% probability that the treatment improves outcomes.\nFlexible Decision-Making\nBayesian designs inherently support adaptive trials, where sample sizes can change mid-trial, arms can be dropped early, and interim decisions are based on posterior probabilities. On the other hand, frequentist designs often require pre-specified stopping rules and can’t adapt as naturally to new information during the trial.\nIncorporation of Prior Knowledge\nEven when we start with a non-informative prior, we can incorporate informative priors in future trials to borrow strength from earlier studies, reduce sample size for rare diseases or pediatric populations, and leverage previous data to guide our current study design.\nBetter Handling of Uncertainty\nBayesian models quantify uncertainty more naturally. We obtain a full posterior distribution, rather than just a point estimate and confidence interval. This is especially useful in early-phase trials or small-sample settings, where uncertainty is higher.\nA More Holistic View of Evidence\nFrequentist results are dichotomous, i.e., significant or not? Whereas, Bayesian results provide a more gradual understanding, showing degrees of belief supported by the data.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#summary",
    "href": "M06_1.html#summary",
    "title": "Week 11: Size Matters!",
    "section": "Summary",
    "text": "Summary\nIn today’s lecture, we examined key concepts and methods related to Bayesian sample size determination. We began by introducing Bayesian sample size calculations and the role of Bayesian decision rules in defining success criteria, such as posterior probability thresholds. We then compared Bayesian and frequentist approaches to sample size determination, highlighting differences in methodology and interpretation. To ensure robustness, we discussed the importance of sensitivity analyses. The concept of Bayesian priors was explored in depth, emphasising how prior information influences inference and sample size requirements. We also introduced the notion of Bayesian type-I error and how it can be controlled within a Bayesian framework.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#live-tutorial-and-discussion",
    "href": "M06_1.html#live-tutorial-and-discussion",
    "title": "Week 11: Size Matters!",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#tutorial-exercises",
    "href": "M06_1.html#tutorial-exercises",
    "title": "Week 11: Size Matters!",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.\n\n\n\n\nBerry, Scott M, Bradley P Carlin, J Jack Lee, and Peter Muller. 2010. Bayesian Adaptive Methods for Clinical Trials. CRC press.\n\n\nQuintana, Melanie, Kert Viele, and Roger J Lewis. 2017. “Bayesian Analysis: Using Prior Information to Interpret the Results of Clinical Trials.” JAMA 318 (16): 1605–6.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_2.html",
    "href": "M06_2.html",
    "title": "Week 12: Discover the Perfect Spell!",
    "section": "",
    "text": "Learnings\nNOT READY YET\n– LO4: Demonstrate proficiency in using statistical software packages (R) to specify and fit models, assess model fit, detect and remediate non-convergence, and compare models.\n– LO5: Engage in specifying, checking and interpreting Bayesian statistical analyses in practical problems using effective communication with health and medical investigators.\nBy the end of this week you should be able to:\n– Understand adaptive sample size calculation using the Bayesian approach.\n– Implement Bayesian model selection methods.\n– Compare different Bayesian models to identify the best-performing one.",
    "crumbs": [
      "Week 12: **Discover the Perfect Spell!**"
    ]
  },
  {
    "objectID": "M06_2.html#learnings",
    "href": "M06_2.html#learnings",
    "title": "Week 12: Discover the Perfect Spell!",
    "section": "",
    "text": "Outcomes\n\n\n\n\nObjectives",
    "crumbs": [
      "Week 12: **Discover the Perfect Spell!**"
    ]
  },
  {
    "objectID": "M06_2.html#more-priors",
    "href": "M06_2.html#more-priors",
    "title": "Week 12: Wander into the Wonder!",
    "section": "More Priors",
    "text": "More Priors\n\n\nLet’s now explore a few other prior scenarios for \\(\\beta_1\\), and what they imply about your beliefs or assumptions.\n\nRegularising Prior (stronger shrinkage)\n\nPrior: \\(\\beta_1 \\sim \\mathcal{N}(0, 1^2)\\)\nInterpretation: You assume most effects are small or near zero. Useful in high-dimensional or noisy settings.\nWhy use it?: Helps with model stability, especially with correlated predictors.\n\n\n\nHorseshoe Prior (for sparse signals)\n\nParticularly helpful if you think most predictors have near-zero effect but a few may be strong.\nPrior: \\(\\beta_1 \\sim \\text{Horseshoe}(0, \\tau)\\)\nInterpretation: Shrinks small effects to zero, but allows strong signals to remain.\nUse case: More common in high-dimensional models, but can still be useful if model includes many covariates.\n\n\n\nCode\n1 + 1\n\n\n[1] 2",
    "crumbs": [
      "Week 12: **Wander into the Wonder!**"
    ]
  },
  {
    "objectID": "M06_2.html#exercises",
    "href": "M06_2.html#exercises",
    "title": "Week 12: Wander into the Wonder!",
    "section": "Exercises",
    "text": "Exercises\nsdfads",
    "crumbs": [
      "Week 12: **Wander into the Wonder!**"
    ]
  },
  {
    "objectID": "M06_2.html#live-tutorial-and-discussion",
    "href": "M06_2.html#live-tutorial-and-discussion",
    "title": "Week 12: Discover the Perfect Spell!",
    "section": "Live tutorial and discussion",
    "text": "Live tutorial and discussion\nThe final learning activity for this week is the live tutorial and discussion. This tutorial is an opportunity for you to to interact with your teachers, ask questions about the course, and learn about biostatistics in practice. You are expected to attend these tutorials when possible for you to do so. For those that cannot attend, the tutorial will be recorded and made available on Canvas. We hope to see you there!",
    "crumbs": [
      "Week 12: **Discover the Perfect Spell!**"
    ]
  },
  {
    "objectID": "M06_2.html#summary",
    "href": "M06_2.html#summary",
    "title": "Week 12: Discover the Perfect Spell!",
    "section": "Summary",
    "text": "Summary\nIn today’s lecture, we covered briefly the key methods in Bayesian adaptive designs and model selection. We began with Bayesian adaptive sample size calculation, highlighting how interim analyses and design modifications based on accumulating data enhance flexibility and efficiency. Next, we introduced Bayesian model choice using Bayes factors to compare models based on evidence. We also discussed information criteria such as DIC, WAIC, and LOO, which help assess model fit and complexity for better predictive performance.",
    "crumbs": [
      "Week 12: **Discover the Perfect Spell!**"
    ]
  },
  {
    "objectID": "M06_2.html#preparation-for-week-2",
    "href": "M06_2.html#preparation-for-week-2",
    "title": "Week 12: Wander into the Wonder!",
    "section": "Preparation for Week 2",
    "text": "Preparation for Week 2\nIn week 2 you will be required to collaboratively complete some exercises. To do this, in week 1 you will be allocated into groups of 3-4 and you are encouraged to meet with your group in week 2 by zoom at a mutually beneficial time. Each group has their own discussion board, which you can use to help organise a meet up time. Interacting, discussing, and working through problems with your peers is an important skill for any biostatistician. This is also nice activity to get to know your peers in this online course.",
    "crumbs": [
      "Week 12: **Wander into the Wonder!**"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n\nCode\n1 + 1\n\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bernardo, JM, MJ Bayarri, JO Berger, AP Dawid, D Heckerman, AFM Smith,\nand M West. 2007. “Generative or Discriminative? Getting the Best\nof Both Worlds.” Bayesian Statistics 8 (3): 3–24.\n\n\nBerry, Scott M, Bradley P Carlin, J Jack Lee, and Peter Muller. 2010.\nBayesian Adaptive Methods for Clinical Trials. CRC press.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari,\nand Donald B Rubin. 2013. Bayesian Data Analysis (3rd Edition).\nChapman; Hall/CRC.\n\n\nGelman, Andrew, Aleks Jakulin, Maria Grazia Pittau, and Yu-Sung Su.\n2008. “A Weakly Informative Default Prior Distribution for\nLogistic and Other Regression Models.” The Annals of Applied\nStatistics 2 (4): 1360–83.\n\n\nHernán, M, and J Robins. 2025. Causal Inference: What If. Boca\nRaton: Chapman & Hall/CRC.\n\n\nKruschke, J. 2014. Doing Bayesian Data Analysis: A Tutorial with r,\nJAGS, and Stan. Academic Press.\n\n\nLambert, Ben. 2018. A Student’s Guide to Bayesian Statistics.\nSAGE Publications Ltd.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in r and Stan (2nd Edition). Chapman; Hall/CRC.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P Jewell. 2016. Causal\nInference in Statistics: A Primer. John Wiley & Sons.\n\n\nQuintana, Melanie, Kert Viele, and Roger J Lewis. 2017. “Bayesian\nAnalysis: Using Prior Information to Interpret the Results of Clinical\nTrials.” JAMA 318 (16): 1605–6.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "M06_1.html#pediatric-antibiotic-trial",
    "href": "M06_1.html#pediatric-antibiotic-trial",
    "title": "Week 11: Size Matters!",
    "section": "Pediatric Antibiotic Trial",
    "text": "Pediatric Antibiotic Trial\n\n\nSuppose, we aim to evaluate whether a new antibiotic (treatment group) is safer than the current standard of care (control group) for treating bacterial respiratory infections in children under 5 years of age.\nOur target population includes children aged 6 months to 5 years who are diagnosed with bacterial pneumonia.\nThe primary endpoint (or the outcome variable) is binary: defined as the absence of drug-related adverse events (e.g., rash, diarrhea, allergic reaction) within 10 days of starting treatment.\nWe are interested to conduct 1:1 allocations for the treatment and control groups.\nSuppose, the standard of care has an estimated safety rate of approximately 60%. As the safety data for the new antibiotic in this age group is still limited, we assume equal prior beliefs for both groups. Hence, we can model the safety probabilities using Beta prior distributions as: \\(\\text{Beta}(2, 2)\\).\n\nDecision Rule - Antibiotic Trial\nAs indicated earlier, we use the Bayesian posterior probability decision rule to test whether the treatment is superior in safety compared to the control. Let us denote \\(\\theta\\) be the safety or success rate and we declare the new antibiotic superior if:\n\\[\nPr(\\theta_{\\text{trt}} - \\theta_{\\text{crt}} &gt; 0 \\mid \\text{data}) \\geq 0.95\n\\]\nThat is, we require at least 95% posterior probability that the new antibiotic has a higher safety rate than the control group (even if the improvement is small). This approach points out any level of safety improvement, provided we are confident in the result.\n\n\nSample Size - Antibiotic Trial\nLet’s say we want to plan/design the sample size such that, assuming the expected data for the safety (or success) rates are 75% for treatment (i.e., \\(\\theta_{\\text{trt}}=0.75\\)) and 60% for control (i.e., \\(\\theta_{\\text{crt}}=0.6\\)), and the posterior probability that the treatment is better than control (superiority trial), in at least 95% of simulated trials. Changes between treatment and control success rates can be also indicated as 15% point differece, i.e., from 60% to 75%.\nNote that in this example we are assuming equal prior beliefs for both treatment and control groups, which follows a \\(\\text{Beta}(1,1)\\) distribution. This prior reflects a non-informative situation, where the safety rates can take any values from 0 to 100%.\nBased on the information, we run simulations to generate data. Then, using simulated data from, say, 50 to 150 samples per group, we calculate the Bayesian power. Next, we identify the minimum sample size per group that achieves 80% power, i.e., 80% of the simulated trials lead us to declare \\(\\theta_{\\text{trt}} - \\theta_{\\text{crt}} &gt; 0\\) using our decision rule considered earlier.\nNow, from the simulation we can see the minimum sample size per group for 80% power as:\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nsimulate_safety_trial &lt;- function(n_per_group, \n                                  true_control = 0.60, \n                                  true_treatment = 0.75,\n                                  prior_control = c(1, 1), \n                                  prior_treatment = c(1, 1),\n                                  prob_cutoff = 0.95,\n                                  threshold = 0) {\n  \n  success_c &lt;- rbinom(1, n_per_group, true_control)\n  success_t &lt;- rbinom(1, n_per_group, true_treatment)\n  post_c &lt;- prior_control + c(success_c, n_per_group - success_c)\n  post_t &lt;- prior_treatment + c(success_t, n_per_group - success_t)\n  n_samples &lt;- 10\n  p_c_post &lt;- rbeta(n_samples, post_c[1], post_c[2])\n  p_t_post &lt;- rbeta(n_samples, post_t[1], post_t[2])\n  prob_superior &lt;- mean((p_t_post - p_c_post) &gt; threshold)\n  return(prob_superior &gt;= prob_cutoff)\n}\nestimate_safety_power &lt;- function(n_per_group, n_sim = 20000) {\n  mean(replicate(n_sim, simulate_safety_trial(n_per_group)))\n}\nsample_sizes &lt;- seq(50, 150, by = 5)\nset.seed(0001)\npower_results &lt;- tibble(\n  SampleSize = sample_sizes,\n  Power = map_dbl(sample_sizes, estimate_safety_power)\n)\nmin_n &lt;- round(power_results,2) %&gt;% filter(Power &gt;= 0.80) %&gt;% slice(1)\nprint(paste(\"Bayesian: Sample size/group for 80% power: \", min_n$SampleSize))\n\n\n[1] \"Bayesian: Sample size/group for 80% power:  120\"\n\n\nNow, let us compare this with frequentist sample size calculation.\n\n\nFrequentist Comparison\nHere, we want to determine the minimum sample size per group required to detect a treatment effect (from 60% to 75%) with 80% power, using a frequentist approach. Thus, we consider null hypothesis as: no improvement or the treatment is not better than the control, i.e., \\(H_0: \\theta_\\text{trt}\\leq \\theta_\\text{crt}\\). The alternative hypothesis we consider as: the treatment is better than the control (i.e., higher success rate), i.e., \\(H_1: \\theta_\\text{trt}&gt; \\theta_\\text{crt}\\).\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\np1 &lt;- ggplot(power_results, aes(x = SampleSize, y = Power)) +\n  geom_line(color = \"#0072B2\", size = 1.2) +\n  geom_point(color = \"#D55E00\", size = 2) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = min_n$SampleSize, linetype = \"dashed\", color = \"darkgreen\") +\n  labs(\n    title = \"Bayesian\",\n    x = \"Sample Size per Group\",\n    y = \"Power\"\n  ) +\n  ylim(0,1) +\n  theme_minimal(base_size = 14)\n\n# frequentist way\n\ntrue_control &lt;- 0.60  \ntrue_treatment &lt;- 0.75  \nalpha &lt;- 0.05  \nn_range &lt;- seq(50, 150, by = 5)  \npower_values &lt;- numeric(length(n_range))\nfor (i in seq_along(n_range)) {\n  result &lt;- power.prop.test(n = n_range[i],\n                            p1 = true_control,\n                            p2 = true_treatment,\n                            sig.level = alpha,\n                            alternative = \"one.sided\")\n  power_values[i] &lt;- result$power\n}\ndf &lt;- data.frame(SampleSize = n_range, Power = power_values)\nmin_n &lt;- df %&gt;% filter(Power &gt;= 0.80) %&gt;% slice(1)\nprint(paste(\"Frequentist: Sample size/group for 80% power: \", min_n$SampleSize))\n\n\n[1] \"Frequentist: Sample size/group for 80% power:  120\"\n\n\nCode\np2 &lt;- ggplot(df, aes(x = SampleSize, y = Power)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = min_n$SampleSize, linetype = \"dashed\", color = \"darkgreen\") +  \n  labs(\n    title = \"Frequentist\",\n    x = \"Sample Size per Group\",\n    y = \"Power\"\n  ) +\n  ylim(0,1) +  \n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  )\n\ngridExtra::grid.arrange(p1,p2,ncol=2)\n\n\n\n\n\n\n\n\n\nThe above plots provide power curves for both Bayesian and frequentist methods for sample sizes 50 to 150. We can see both plots show the power increases with the sample size and we can achieve 80% power at sample size 120 for both methods. This reflects under \\(\\text{Beta}(2,2)\\) prior we get same results for both Bayesian and frequentist methods based on the Bayesian decision rule we considered.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_1.html#bayesian-version-of-type-i-error",
    "href": "M06_1.html#bayesian-version-of-type-i-error",
    "title": "Week 11: Size Matters!",
    "section": "Bayesian Version of Type-I Error!",
    "text": "Bayesian Version of Type-I Error!\n\n\nBayesian version of Type-I error is a concept that mirrors the classical (frequentist) Type-I error. In a frequentist hypothesis test, type-I error refers to the probability of rejecting the null hypothesis when it is actually true. For example, with statistical level of significance \\(\\alpha=0.05\\), we are accepting a 5% chance of falsely declaring a treatment effective when it’s not.\nIn Bayesian analysis, we don’t really reject null in the traditional way. Instead, we compute the posterior probability that the treatment is better than control, given the data.\nSo, say in a Bayesian superiority trial, as we have already explain earlier, we might want to declare a treatment superior, if the posterior probability that treatment is better than control exceeds some threshold (e.g., 95%). Here, Bayesian Type-I Error is the probability that we incorrectly declare superiority, when the treatment is not actually better (e.g., when there’s no difference, or control is better).\nNow, how do we estimate Bayesian Type-I Error (in practice)? We simulate many trials under the null hypothesis, and see how often our Bayesian decision rule declares a false positive.\n\nAssume null is true: i.e., treatment = control = 60% success.\nSimulate N trials (e.g., 10,000).\nFor each trial:\n\nCompute posterior probability that treatment is better than control.\nIf it exceeds your decision threshold (e.g., 95%), declare “superior.”\n\nCount how many of those trials wrongly declare superiority.\nThat proportion is our Bayesian Type-I Error rate.\n\nNow, even in a Bayesian framework, especially in regulated environments (e.g., clinical trials), controlling type-I error is critical for ethical, financial, and regulatory reasons.\nWhile Bayesian methods don’t inherently control \\(\\alpha\\) like frequentist ones do, regulators often require simulations to show that the Type-I error is controlled.\nExample:\nIf our decision rule is to declare a treatment superior if posterior P(treatment &gt; control) &gt; 0.95; then we might find:\n\nUnder the null (treatment = control), this rule gives a false positive 5% of the time, i.e, Type-I error is approximately 0.05\nIf it gives greater than 5% of the time, then Type-I error is too high and we may need to adjust the cutoff, sample size, or prior.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nsimulate_safety_trial &lt;- function(n_per_group, \n                                  true_control = 0.60, \n                                  true_treatment = 0.60,\n                                  prior_control = c(1, 1), \n                                  prior_treatment = c(1, 1),\n                                  prob_cutoff = 0.95,\n                                  threshold = 0) {\n  \n  success_c &lt;- rbinom(1, n_per_group, true_control)\n  success_t &lt;- rbinom(1, n_per_group, true_treatment)\n  \n  post_c &lt;- prior_control + c(success_c, n_per_group - success_c)\n  post_t &lt;- prior_treatment + c(success_t, n_per_group - success_t)\n  \n  n_samples &lt;- 500\n  p_c_post &lt;- rbeta(n_samples, post_c[1], post_c[2])\n  p_t_post &lt;- rbeta(n_samples, post_t[1], post_t[2])\n  \n  prob_superior &lt;- mean((p_t_post - p_c_post) &gt; threshold)\n  \n  return(prob_superior &gt;= prob_cutoff)\n}\nestimate_safety_power &lt;- function(n_per_group, prob_cutoff_val, n_sim = 10000) {\n  mean(replicate(n_sim, simulate_safety_trial(n_per_group, \n                                               prob_cutoff = prob_cutoff_val)))\n}\nsample_sizes &lt;- seq(50, 150, by = 5)\ncutoff_values &lt;- c(0.95) #c(0.95, 0.99)\npower_results_type1 &lt;- expand_grid(SampleSize = sample_sizes,\n                             ProbCutoff = cutoff_values) %&gt;%\n  mutate(Type1_Error = map2_dbl(SampleSize, ProbCutoff, estimate_safety_power, .progress = TRUE))\nggplot(power_results_type1, aes(x = SampleSize, y = Type1_Error, color = factor(ProbCutoff))) +\n  geom_line(size = 1.2) +\n  geom_point(size = 2) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Bayesian Type-I Error\",\n    x = \"Sample Size per Group\",\n    y = \"Type-I Error Rate\",\n    color = \"Cutoff\"\n  ) +\n  ylim(0, 0.20) +\n  theme_minimal(base_size = 14)",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  },
  {
    "objectID": "M06_2.html#bayesian-model-choice",
    "href": "M06_2.html#bayesian-model-choice",
    "title": "Week 12: Discover the Perfect Spell!",
    "section": "Bayesian Model Choice",
    "text": "Bayesian Model Choice\nRecall that in one of the previous modules, we focused on evaluating the performance of Bayesian models, especially after running Markov Chain Monte Carlo (MCMC) simulations. Where we used, trace plots, R-hat statistic (also called the Gelman-Rubin diagnostic), effective sample size (ESS), autocorrelation checks and posterior predictive checks.\nIn today’s lecture, we move from model evaluation to model selection, deciding which of several Bayesian models best explains your data, given prior knowledge and model structure. This process is called Bayesian model choice. In this unit, we will mainly discussed few of the methods, such as Bayes Factor and model choice using information criteria.",
    "crumbs": [
      "Week 12: **Discover the Perfect Spell!**"
    ]
  },
  {
    "objectID": "M06_2.html#bayes-factor",
    "href": "M06_2.html#bayes-factor",
    "title": "Week 12: Discover the Perfect Spell!",
    "section": "Bayes Factor",
    "text": "Bayes Factor\n\n\nGelman et al. (2013) page:182\n\nProbabilistic View Point\nThe Bayes Factor is a tool used in Bayesian statistics, say to compare two hypotheses. It tells you how much more likely the data is under one hypothesis than another.\nThink of it like a ratio:\n\\[\n\\text{Bayes Factor (BF)} = \\frac{\\text{Probability of data if Hypothesis 1 is true}}{\\text{Probability of data if Hypothesis 2 is true}}\n\\]\nWe can write it as:\n\\[\n\\text{BF}_{10} = \\frac{Pr(\\text{Data} \\mid H_1)}{Pr(\\text{Data} \\mid H_0)}\n\\]\nwhere, \\(H_1\\): Hypothesis one (or alternative hypothesis) e.g., “something is going on” and \\(H_0\\): Hypothesis two (or null hypothesis ), e.g., “nothing is going on”. Now, if \\(BF &gt; 1\\), data favors \\(H_1\\) and if \\(BF &lt; 1\\), data favors \\(H_0\\).\nLet’s go back to the medical practitioner example we have discussed earlier in this unit, where the medical practitioner was trying to determine whether a patient has type-2 diabetes (T2D). Here, now we want to compare two hypotheses:\n\n\\(H_0\\): The patient does not have type-2 diabetes\n\\(H_1\\): The patient does have type-2 diabetes\n\nThe goal is to use the Bayes Factor to compare how likely the observed test result is under each hypothesis.\nLet’s assume, the test returns positive if it detects likely diabetes. But, the test is not perfect, it has known sensitivity and specificity.\nLet us further assume, the sensitivity (true positive rate) is 90%, i.e., \\(Pr(\\text{Positive test} \\mid H_1) = 0.9\\). And the specificity (true negative rate) is 80%, i.e., \\(Pr(\\text{Negative test} \\mid H_0) = 0.8\\).\nFrom this, we get:\n\\[\n\\begin{aligned}\nPr(\\text{Positive test} \\mid H_0) &= 1 - 0.8 = 0.2 \\\\\nPr(\\text{Positive test} \\mid H_1) &= 0.9\n\\end{aligned}\n\\]\nHence, we get the Bayes Factor as:\n\\[\n\\text{BF}_{10} = \\frac{Pr(\\text{Positive test} \\mid H_1)}{Pr(\\text{Positive test} \\mid H_0)} = \\frac{0.9}{0.2} = 4.5\n\\] Here, a Bayes Factor of 4.5 means, the observed positive test result is 4.5 times more likely if the patient has type-2 diabetes than if they do not. This is considered a moderate evidence in favor of \\(H_1\\) (i.e., the patient likely has diabetes).\nNow, we could combine this with prior belief (say, based on risk factors) using Bayes’ Theorem to get a posterior probability (please see more in the following section), but the Bayes Factor alone tells us how the data shifts the odds between the two hypotheses.\nA commonly used guideline for interpreting the Bayes Factor (BF) is as follows::\n\n\n\nBayes Factor \\(\\text{BF}_{10}\\)\nStrength of Evidence for \\(H_1\\)\n\n\n\n\n1 to 3\nWeak evidence\n\n\n3 to 10\nModerate evidence\n\n\n10 to 30\nStrong evidence\n\n\n&gt; 100\nDecisive evidence\n\n\n\nNote that we can also define Bayes Factor (BF) changing the hypothesis that we are interested, i.e., if we write \\(BF_{01}\\) instead of \\(BF_{10}\\), then we are interested in \\(H_0\\).\nCompared to Frequentist p-values\n\n\n\n\n\n\n\n\n\nBayes Factor\np-value\n\n\n\n\nCompares hypotheses?\nYes (directly compares \\(H_1\\) and \\(H_0\\))\nNo (tests \\(H_0\\) only)\n\n\nProbability statement?\nYes (about models)\nNo (about data given \\(H_0\\))\n\n\nDepends on prior?\nYes\nNo\n\n\n\n\n\nBF for Bayesian Models\nNow let us discuss how we can use this concept of Bayes Factor in Bayesian model selection. Here, Bayes factor is the ratio of two marginal likelihoods obtained from the two models.\nIn Bayesian statistics, model selection is based on the posterior probabilities of models, given the observed data. Suppose we have two competing models, \\(\\mathcal{M}_1\\) and \\(\\mathcal{M}_2\\). Given observed data \\(D\\), the Bayesian approach computes the posterior probability of each model using Bayes’ theorem:\n\\[\np(\\mathcal{M}_i \\mid D) = \\frac{p(D \\mid \\mathcal{M}_i) p(\\mathcal{M}_i)}{p(D)}; \\quad i \\in \\{1,2\\}\n\\]\nHere, \\(p(D \\mid \\mathcal{M}_i)\\) is the marginal likelihood or evidence for model \\(\\mathcal{M}_i\\), \\(p(\\mathcal{M}_i)\\) is the prior probability of model \\(\\mathcal{M}_i\\) and \\(p(D)\\) is the marginal probability of the data across all models.\nThe marginal likelihood is computed by integrating the likelihood over the model’s parameter space:\n\\[\np(D \\mid \\mathcal{M}_i) = \\int p(D \\mid \\theta_i, \\mathcal{M}_i) p(\\theta_i \\mid \\mathcal{M}_i) \\, d\\theta_i \\quad i \\in \\{1,2\\}\n\\]\nThis marginal likelihood balances model fit (via the likelihood) with model complexity (via the prior).\nNow, as we have mentioned earlier, the Bayes Factor is the ratio of the marginal likelihoods of these two models:\n\\[\n\\text{BF}_{12} = \\frac{p(D \\mid \\mathcal{M}_1)}{p(D \\mid \\mathcal{M}_2)}\n\\]\n\n\\(\\text{BF}_{12} &gt; 1\\): Evidence favors model \\(\\mathcal{M}_1\\)\n\\(\\text{BF}_{12} &lt; 1\\): Evidence favors model \\(\\mathcal{M}_2\\)\n\\(\\text{BF}_{12} = 1\\): Both models are equally supported by the data\n\nThis ratio quantifies how much more likely the data is under one model than another, independent of prior beliefs about the models.\nIf we include model priors, the posterior odds in favor of \\(\\mathcal{M}_1\\) over \\(\\mathcal{M}_2\\) become:\n\\[\n\\begin{aligned}\n\\frac{p(\\mathcal{M}_1 \\mid D)}{p(\\mathcal{M}_2 \\mid D)} &= \\text{BF}_{12} \\cdot \\frac{p(\\mathcal{M}_1)}{p(\\mathcal{M}_2)} \\\\\n\\implies \\text{BF}_{12} &= \\frac{p(\\mathcal{M}_1 \\mid D)}{p(\\mathcal{M}_2 \\mid D)}\\cdot \\frac{p(\\mathcal{M}_2)}{p(\\mathcal{M}_1)}\n\\end{aligned}\n\\]\nThus, Bayes Factor serves as the updating factor for the prior odds between two models, given the prior distributions of the models \\(p(\\mathcal{M}_i)\\), \\(i\\in \\{1,2\\}\\).\n\n\nExample in Practice\nLet us recall the C-reactive protein (CRP) data example we discussed earlier in this unit. In that example, we used a Bayesian hierarchical model with age, sex, and antibiotic status as predictor variables. Now, we might want to investigate whether adding more variables to the model improves the results. For instance, we may consider including the sepsis and discharge status variables. The idea is to fit two models, one without sepsis and discharge variables (say crp_model1), and another with them (say crp_model2), and then use the Bayes factor to compare the models.\nWe write R for this as follows:\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\ncrp_data &lt;- read.csv(\"crp_data_complete.csv\")\ncrp_data &lt;- tibble(\n  ID = crp_data$ID,\n  CRP = crp_data$crp,\n  CRP_log = log(crp_data$crp),\n  Day = crp_data$day,\n  Antibiotic = as.factor(crp_data$antib_1h),\n  Age = crp_data$age,\n  Sex = as.factor(crp_data$SEX),\n  Sepsis = as.factor(crp_data$SEPSIS),\n  Discharge = as.factor(crp_data$discharge_r)\n)\npriors &lt;- c(\n  prior(normal(0, 10), class = \"Intercept\"), # prior for global intercept\n  prior(normal(0, 10), class = \"b\"), # prior for all fixed effects\n  prior(cauchy(0, 1), class = \"sd\"), # prior for random intercept SD\n  prior(cauchy(0, 1), class = \"sigma\") # prior for residual SD\n)\ncrp_model1 &lt;- brm(\n  formula = CRP_log ~ Day + Age + Sex + Antibiotic + (1 | ID),\n  data = crp_data,\n  family = gaussian(),\n  prior = priors,\n  chains = 2,\n  cores = 2,\n  iter = 2000,\n  warmup = 1000,\n  seed = 1234,\n  control = list(adapt_delta = 0.95),\n  save_all_pars = TRUE\n)\ncrp_model2 &lt;- brm(\n  formula = CRP_log ~ Day + Age + Sex + Antibiotic + Sepsis + Discharge + (1 | ID),\n  data = crp_data,\n  family = gaussian(),\n  prior = priors,\n  chains = 2,\n  cores = 2,\n  iter = 2000,\n  warmup = 1000,\n  seed = 1234,\n  control = list(adapt_delta = 0.95),\n  save_pars = save_pars(all=TRUE)\n)\n# compute Bayes factors \nset.seed(1234)\nbayes_factor(crp_model2, crp_model1)\n\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\nIteration: 6\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\nIteration: 6\nIteration: 7\nIteration: 8\n\n\nEstimated Bayes factor in favor of crp_model2 over crp_model1: 4.36971\n\n\nThe result shows that the CRP model with sepsis and discharge variables (i.e., crp_model2) provide more insight compared to the model without these variables (i.e., crp_model1).\nNow, we can also get the posterior model probabilities from marginal likelihoods. If you compare multiple Bayesian models using Bayes factors, you can use them to compute posterior model probabilities, i.e., the probability that each model is true, given the observed data.\nFor these two models crp_model2 and crp_model1, we compute the Bayes Factor:\n\\[\n\\text{BF}_{21} = \\text{BF}_{\\text{model2},\\text{model1}}= \\frac{p(\\text{data} \\mid \\text{model2})}{p(\\text{data} \\mid \\text{model1})} = 3.5\n\\]\nThen, assuming equal prior model probabilities, i.e., \\(p(\\text{model2}) = p(\\text{model1}) = 0.5\\), we get the posterior probability of each model:\n\\[\np(\\text{model2} \\mid \\text{data}) = \\frac{\\text{BF}_{21} \\cdot p(\\text{model2})}{\\text{BF}_{21} \\cdot p(\\text{model2}) + p(\\text{model1})}\n\\]\nWhich simplifies (with equal priors) to:\n\\[\n\\begin{aligned}\np(\\text{model2} \\mid \\text{data}) &= \\frac{BF_{21}}{BF_{21} + 1}\\\\\np(\\text{model1} \\mid \\text{data}) &= \\frac{1}{BF_{21} + 1}\n\\end{aligned}\n\\]\nThere asre the posterior model probabilities for each models from their marginal likelihoods. Hence, we can get the marginal posterior probabilities as:\n\n\nCode\n# compute the posterior model probabilities from Marginal Likelihoods\nset.seed(1234)\npost_prob(crp_model2, crp_model1)\n\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\nIteration: 6\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\nIteration: 6\nIteration: 7\nIteration: 8\n\n\ncrp_model2 crp_model1 \n 0.8137703  0.1862297 \n\n\nNow, suppose the prior model probabilities are not same, i.e., \\(p(\\text{model2}) = 0.25\\) and \\(p(\\text{model1}) = 0.75\\), then we get the marginal posterior probabilities as:\n\n\nCode\n# specify prior model probabilities\nset.seed(1234)\npost_prob(crp_model2, crp_model1, prior_prob = c(0.1, 0.9))\n\n\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\nIteration: 6\nIteration: 1\nIteration: 2\nIteration: 3\nIteration: 4\nIteration: 5\nIteration: 6\nIteration: 7\nIteration: 8\n\n\ncrp_model2 crp_model1 \n 0.3268367  0.6731633 \n\n\nWe can see from the results that how the prior preference for models can lead to a shift in the marginal posterior probabilities for the models and hence the selection choice.",
    "crumbs": [
      "Week 12: **Discover the Perfect Spell!**"
    ]
  },
  {
    "objectID": "M06_2.html#information-criteria",
    "href": "M06_2.html#information-criteria",
    "title": "Week 12: Discover the Perfect Spell!",
    "section": "Information Criteria",
    "text": "Information Criteria\n\n\nIn Bayesian statistics, information criteria are also used to compare models. Some common information criteria include the Deviance Information Criterion (DIC), the Watanabe–Akaike Information Criterion or also known as the Widely Applicable Information Criterion (WAIC), and Leave-One-Out Cross-Validation (LOO). Below, we provide the details of these criteria.\n\nDIC\nThe Deviance Information Criterion (DIC) is particularly useful when we’re working with models estimated via Markov Chain Monte Carlo (MCMC) methods. It balances model fit and complexity, giving us a way to choose between competing Bayesian models. Like AIC (Akaike Information Criterion) in the frequentist setting, DIC aims to prevent overfitting by penalising models that are too complex.\nTo calculate DIC, we first compute the deviance, which is defined as \\(D(\\theta) = -2 \\log p(y \\mid \\theta)\\), where \\(y\\) is the observed data and \\(\\theta\\) represents the model parameters. We then take the posterior mean of the deviance (denoted \\(\\overline{D(\\theta)}\\)) across MCMC samples. The DIC itself is given by the formula:\n\\[\n\\text{DIC} = \\overline{D(\\theta)} + p_D\n\\]\nHere, \\(p_D = \\overline{D(\\theta)} - D(\\overline{\\theta})\\) is the effective number of parameters, which serves as a complexity penalty. \\(D(\\overline{\\theta})\\) is the deviance evaluated at the posterior mean of the parameters. A lower DIC indicates a better trade-off between model fit and complexity, so we generally prefer models with lower DIC values.\nThere are several advantages to using DIC. First, it’s relatively easy to compute from MCMC output, making it convenient when we’re already using Bayesian estimation methods. DIC also provides a straightforward way to assess the fit of models that are not nested, and it helps account for overfitting by penalizing model complexity through the effective number of parameters.\nHowever, DIC also has limitations. It may not perform well for hierarchical or latent variable models, especially when the posterior distributions are highly skewed or multimodal. Since DIC relies on the posterior mean of the parameters, it can give misleading results if that mean doesn’t accurately reflect the shape of the full posterior distribution. In such cases, it might underestimate the model complexity or produce overly optimistic estimates of model fit.\nBecause of these issues, we might consider using more robust alternatives like WAIC or LOO-CV , especially when dealing with more complex models. Nevertheless, DIC remains a useful and accessible tool for many Bayesian model comparison tasks, particularly when the models and posteriors are relatively simple and well-behaved.\n\n\nWAIC\nWhen comparing Bayesian models, we often look for a criterion that not only rewards good fit but also guards against overfitting. The WAIC serves this purpose and is considered an improvement over earlier methods like the DIC, especially for complex or hierarchical models. Unlike DIC, which relies on point estimates, WAIC fully integrates over the posterior distribution, making it more aligned with the Bayesian philosophy.\nTo compute WAIC, we begin by evaluating the log-likelihood of each individual data point at every iteration of our MCMC samples. This allows us to measure how well the model predicts the data, on average, across the posterior. The sum of these log-likelihoods, averaged across samples, gives us the log pointwise predictive density (lppd). To adjust for model complexity, we estimate the effective number of parameters, \\(p_{\\text{WAIC}}\\), based on how much the log-likelihood varies across the samples. The WAIC formula is:\n\\[\n\\text{WAIC} = -2 \\left( \\text{lppd} - p_{\\text{WAIC}} \\right)\n\\]\nIn practice, WAIC allows us to assess how well a model would perform on new, unseen data. The smaller the WAIC value, the better the model’s predictive performance. One of the key strengths of WAIC is that it operates on a pointwise level, meaning it evaluates model fit for each observation independently. This makes it more flexible and reliable, especially when we are working with models that include latent variables, non-linear structures, or non-i.i.d. observations.\nWhile WAIC offers many advantages, it is not without limitations. It requires us to retain and process the full array of log-likelihood values from our MCMC samples, which can be computationally demanding for large datasets. Additionally, the reliability of WAIC depends heavily on the convergence and quality of the posterior samples. Poor MCMC mixing or insufficient sampling can result in inaccurate estimates of model performance.\nStill, WAIC is one of the most general-purpose and robust information criteria available for Bayesian model comparison. It provides a principled way to choose among models based on their predictive accuracy, rather than just fit alone, and is especially useful when our models go beyond simple parametric forms.\n\n\nLOO\nWhen we want to evaluate how well a Bayesian model predicts new data, Leave-One-Out (LOO) Cross-Validation provides one of the most intuitive and accurate approaches. In essence, LOO estimates the model’s predictive performance by systematically leaving out each observation in the dataset, fitting the model to the remaining data, and then measuring how well it predicts the left-out point. This process is repeated for every data point, giving us a robust estimate of out-of-sample predictive accuracy.\nIn Bayesian settings, we rarely re-fit the model \\(n\\) times for \\(n\\) observations because it would be too computationally expensive. Instead, we use methods like Pareto-smoothed importance sampling (PSIS-LOO), which approximate LOO using existing MCMC samples. This makes LOO practical while still maintaining high accuracy in most cases.\nThe result of LOO is typically expressed as:\n\\[\n\\text{LOO} = -2 \\sum_{i=1}^n \\log p(y_i \\mid y_{-i})\n\\]\nHere, \\(p(y_i \\mid y_{-i})\\) is the posterior predictive density for observation \\(i\\), given all other data points. A lower LOO score indicates better predictive performance. Since LOO directly evaluates generalization to unseen data, it is often considered the gold standard for model comparison in Bayesian analysis.\nLOO has several key strengths. First, it is fully Bayesian and does not rely on point estimates or asymptotic approximations. It is also more robust than DIC and, in some cases, WAIC, particularly when the model is complex or the likelihood is sensitive to small changes in the data. Moreover, LOO provides diagnostic tools that help us detect when certain data points are too influential or when the approximation might be unreliable.\nHowever, LOO is not without its drawbacks. While PSIS-LOO makes the method computationally feasible, it still relies on the assumption that importance sampling provides a good approximation. If the approximation breaks down, often due to high variance in the importance weights, LOO estimates can become unstable. In such cases, we might need to fall back on exact cross-validation or reconsider the model specification.\nOverall, we find that LOO offers one of the most reliable and interpretable assessments of predictive performance for Bayesian models. It directly measures how well our model generalizes, which is usually the main goal in statistical modeling. When used alongside WAIC or as a standalone metric, LOO helps us make informed decisions about which model is likely to perform best on future data.\n\n\nExample in Practice\nNow we implement these information criteria to select the best Bayesian model related to the C-reactive protein models discussed earlier. The crp_model2 includes all predictor variables (i.e., age, sex, antibiotic use, sepsis, and discharge), while crp_model1 excludes the sepsis and discharge variables.\nDIC\n\n\nCode\nlibrary(brms)\nlibrary(loo)\ndic_brm &lt;- function(fit){\n   log_lik_matrix &lt;- log_lik(fit)\n   deviance_pointwise &lt;- -2 * log_lik_matrix\n   D_bar &lt;- mean(rowSums(deviance_pointwise))\n   posterior_mean &lt;- posterior_summary(fit)[, \"Estimate\"]\n   D_hat &lt;- -2 * sum(log(colMeans(exp(log_lik_matrix))))\n   p_D &lt;- D_bar - D_hat\n   DIC &lt;- D_bar + p_D\n   df &lt;- data.frame(DIC=DIC, D_bar=D_bar, p_D=p_D)\n   row.names(df) &lt;- \"Estimate\"\n   return(df)\n}\nprint(\"CRP Model 1: \"); dic_brm(crp_model1)\n\n\n[1] \"CRP Model 1: \"\n\n\n              DIC    D_bar      p_D\nEstimate 757.9466 703.6482 54.29845\n\n\nCode\nprint(\"CRP Model 2: \"); dic_brm(crp_model2)\n\n\n[1] \"CRP Model 2: \"\n\n\n             DIC    D_bar      p_D\nEstimate 755.808 702.7549 53.05307\n\n\nThe output provides a comparison between two Bayesian hierarchical models, CRP Model 1 and CRP Model 2, using the DIC. As we have already discussed, a lower DIC value indicates a better model, assuming all else is equal.\nLooking at the results, model 1 has a DIC of 757.95, a posterior mean deviance \\((\\bar{D})\\) of 703.65, and an effective number of parameters \\((p_D)\\) of 54.30. In comparison, model 2 has a slightly lower DIC of 755.81, a \\(\\bar{D}\\) of 702.75, and a \\(p_D\\) of 53.05. These values suggest that model 2 fits the data slightly better (as shown by the lower \\(\\bar{D}\\)) and is marginally less complex (as shown by the lower \\(p_D\\)).\nOverall, while the difference in DIC between the two models is relatively small, approximately 2.14 points, the CRP Model 2 performs marginally better in terms of both model fit and simplicity. Although a DIC difference greater than 5 to 10 is typically considered more substantial, the results still suggest that CRP Model 2 is the preferred model in this comparison.\nWAIC\nNow for the same Bayesian hierarchical models, we calculate the WAIC as follows:\n\n\nCode\nlibrary(loo)\nwaic1 &lt;- waic(crp_model1)\nwaic2 &lt;- waic(crp_model2)\nprint(\"CRP Model 1: \"); waic1\n\n\n[1] \"CRP Model 1: \"\n\n\n\nComputed from 2000 by 432 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -387.1 17.7\np_waic        62.5  4.8\nwaic         774.3 35.5\n\n34 (7.9%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\nCode\nprint(\"CRP Model 2: \"); waic2\n\n\n[1] \"CRP Model 2: \"\n\n\n\nComputed from 2000 by 432 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -385.7 17.8\np_waic        60.9  4.8\nwaic         771.5 35.6\n\n34 (7.9%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\nThe output presents WAIC results, and as we know like DIC, lower WAIC values generally indicate better model performance. We can see the WAIC for model 2 is smaller compared to the model 1.\nBoth models were evaluated using a log-likelihood matrix with 2000 posterior samples across 432 observations. A key diagnostic mentioned in the output is that 34 (7.9%) of the p_waic estimates (pointwise effective number of parameters) are greater than 0.4 for each model.\nThis suggests some instability or potential problems with the WAIC approximation for these Bayesian hierarchical models. When a noticeable proportion of p_waic values are this high, it indicates that the model may not be well-behaved in some data regions. As a result, the tool recommends using LOO (Leave-One-Out cross-validation) instead, which is often more robust.\nNow we can also use the function loo_compare(), which gives difference in expected log predictive density (elpd). In this case, as we have use WAIC, hence use of loo_compare() function will provide difference in elpd_waic values. We can see that model 2 has an elpd_diff (of WAIC) of 0.0, while model 1 has an elpd_diff (of WAIC) of -1.4 with a standard error of 1.8. This means CRP model 2 performs slightly better in terms of predictive accuracy, but the difference is not statistically significant, since the difference is small and well within the range of uncertainty. In practical terms, we can say, both models perform similarly, and the choice might come down to factors like interpretability or theoretical justification.\nLOO\nNow, we get the results using LOO as:\n\n\nCode\nlibrary(loo)\nloo1 &lt;- loo(crp_model1, moment_match = TRUE)\nloo2 &lt;- loo(crp_model2, moment_match = TRUE)\nprint(\"CRP Model 1: \"); loo1\n\n\n[1] \"CRP Model 1: \"\n\n\n\nComputed from 2000 by 432 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -388.5 17.9\np_loo        63.9  4.9\nlooic       777.1 35.8\n------\nMCSE of elpd_loo is 0.2.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.3, 2.7]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nCode\nprint(\"CRP Model 2: \"); loo2\n\n\n[1] \"CRP Model 2: \"\n\n\n\nComputed from 2000 by 432 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -386.8 17.9\np_loo        61.9  4.8\nlooic       773.6 35.7\n------\nMCSE of elpd_loo is 0.2.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 2.1]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nCode\nloo_compare(loo1, loo2)\n\n\n           elpd_diff se_diff\ncrp_model2  0.0       0.0   \ncrp_model1 -1.7       1.8   \n\n\nThe output presents a comparison between CRP model 1 and model 2 with LOO, and we already know LOO is a robust method for estimating out-of-sample predictive accuracy in Bayesian models. This method evaluates how well a model is expected to predict new data by systematically leaving out each observation and computing the model’s performance.\nFor model 1, the estimated expected log predictive density (elpd_loo) is -388.5, with a standard error of 17.9. The effective number of parameters (p_loo) is 63.9, and the LOO Information Criterion (looic), which is simply -2 * elpd_loo is 777.1. All diagnostic values, including Pareto k estimates, are in the safe range (all k &lt; 0.7), which indicates reliable LOO estimates. The Monte Carlo standard error (MCSE) for elpd_loo is just 0.2, suggesting stable estimation.\nFor model 2, the elpd_loo is -386.8, with the same standard error of 17.9, a slightly lower p_loo of 61.9, and a looic of 773.6. Like model 1, all diagnostic measures are within acceptable bounds, and the MCSE of elpd_loo is also 0.2, indicating reliable MCMC sampling.\nThe loo_compare() output shows that model 2 has a slightly better elpd_loo than model 1, with a difference (elpd_diff) of 1.7 in favor of model 2. However, the standard error of this difference is 1.8, meaning that the difference is not statistically significant, it’s well within the margin of uncertainty. This suggests that, while model 2 is marginally preferred, both models perform nearly identically.",
    "crumbs": [
      "Week 12: **Discover the Perfect Spell!**"
    ]
  },
  {
    "objectID": "M06_2.html#bayesian-prediction",
    "href": "M06_2.html#bayesian-prediction",
    "title": "Week 12: Discover the Perfect Spell!",
    "section": "Bayesian Prediction",
    "text": "Bayesian Prediction\n\n\nBayesian prediction is the process of forecasting future or unseen data by integrating over the uncertainty in model parameters using the posterior distribution. Unlike classical prediction (which often plugs in point estimates of parameters), Bayesian prediction integrates over the entire posterior distribution of the model parameters. This means predictions are not just a single value but a distribution that reflects both uncertainty in the data (randomness, noise) and uncertainty in the model parameters (posterior uncertainty)\nSuppose you’ve observed data \\(D\\), and you’re interested in predicting a new observation \\(\\tilde{y}\\), possibly at a new covariate \\(\\tilde{x}\\).\nRecall that in Bayesian inference, the posterior predictive distribution is:\n\\[\np(\\tilde{y} \\mid D) = \\int p(\\tilde{y} \\mid \\theta) \\, p(\\theta \\mid D) \\, d\\theta\n\\]\n\n\\(p(\\tilde{y} \\mid \\theta)\\): likelihood of new data given parameters\n\\(p(\\theta \\mid D)\\): posterior distribution of parameters\n\\(p(\\tilde{y} \\mid D)\\): posterior predictive distribution\n\nSo you integrate over the posterior to get predictions, not just plug in a point estimate.\nImagine a Bayesian linear regression:\n\\[\ny_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\n\\]\nAfter fitting the model, the Bayesian prediction for a new \\(x^*\\) is not just \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x^*\\), but rather a distribution over possible \\(y^*\\), incorporating the full posterior over \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\).",
    "crumbs": [
      "Week 12: **Discover the Perfect Spell!**"
    ]
  },
  {
    "objectID": "M06_2.html#tutorial-exercises",
    "href": "M06_2.html#tutorial-exercises",
    "title": "Week 12: Discover the Perfect Spell!",
    "section": "Tutorial Exercises",
    "text": "Tutorial Exercises\nSolutions will be provided later after the tutorial.\n\n\n\n\nBerry, Scott M, Bradley P Carlin, J Jack Lee, and Peter Muller. 2010. Bayesian Adaptive Methods for Clinical Trials. CRC press.\n\n\nGelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis (3rd Edition). Chapman; Hall/CRC.",
    "crumbs": [
      "Week 12: **Discover the Perfect Spell!**"
    ]
  },
  {
    "objectID": "M06_2.html#adaptivity",
    "href": "M06_2.html#adaptivity",
    "title": "Week 12: Discover the Perfect Spell!",
    "section": "Adaptivity",
    "text": "Adaptivity\n\n\nIn this course, we will introduce the fundamental conpects of Bayesian adaptive design. For interested readers to learn more, we refer to the book by Berry et al. (2010).\nAdaptive designs are an increasingly popular approach in clinical trials, offering flexibility to modify trial procedures based on accumulating data without undermining the study’s integrity or validity. One powerful framework for implementing adaptive designs is through Bayesian methods.\n\n\n\n\n\n\nAdaptive trials are designed with flexibility built in from the start. This means we can pre-plan adjustments to key elements like treatment types (dose, frequency, combinations), how patients are assigned to different treatments, which patient groups to include, and even how many participants are needed.\nWhat makes this approach powerful is its ability to learn as it goes. As data rolls in, the trial can start to focus more on the treatment arms that are showing promise. That allows us to begin with a broader range of options, maybe testing 8 doses instead of just 3, and still use fewer participants overall.\nA well-designed adaptive trial doesn’t just benefit researchers. Patients may get better treatments, regulators get more informative results, and the entire development process becomes more streamlined.\nThat said, the flip side is that adaptive designs take more effort to build. They require careful planning and trial simulations to make sure they perform well and stand up to regulatory standards. This means close collaboration of statisticians, clinicians, regulatory teams, operations, supply chain, and more all need to be in sync. When done right, though, the payoff is a trial design that’s not only smarter, but truly better for everyone involved.\n\n\n\nAntibiotic Safety Trial\nLet’s expain the adaptivity and realted sample size calculation based on the antibiotic safety trila we discussed earlier, where we want to evaluate if a new antibiotic is safer than the standard treatment for bacterial pneumonia in children aged 6 months to 5 years.\nDesign Features\n\nRandomisation: 1:1 (New Antibiotic vs. Standard of Care)\nPrior Beliefs: Non-informative priors for both arms (e.g., Beta(1,1) for safety rates)\nControl Safety Rate: Estimated at 60%\nTreatment Safety Rate: Estimated at 70% (say, about 10% point difference)\nPlanned Sample Size: Let’s assume 200 total (100 per group)\nInterim Analyses: After 50, 100, and 150 participants have been followed\n\nDecision Rule\n\nIf \\(Pr(\\theta_{trt} &gt; \\theta_{crt}) &gt; 0.95\\) then consider early success\nIf \\(Pr(\\theta_{trt} &lt; \\theta_{crt}) &gt; 0.95\\) then consider futility (stop for harm)\nElse continue to next interim or if in the final interim then either it is inconclusive or conclude as no added benefit.\n\n\n\nInterim Analyses\nAt the first interim, conducted after enrolling 50 participants (25 in each group), suppose we observed that 17 children in the treatment group (i.e., \\(\\approx\\) 70%) and 15 in the control group experienced no adverse events (i.e., 60%). Starting with weakly-informative \\(\\text{Beta}(2,2)\\) priors, we updated the posterior distributions based on this data. For the treatment group, this yielded a \\(\\text{Beta}(19, 10)\\) distribution, and for the control group, a \\(\\text{Beta}(17, 12)\\). We then calculated the posterior probability that the new antibiotic is safer than the standard treatment, especifically, \\(Pr(\\theta_{\\text{trt}} &gt; \\theta_{\\text{crt}})\\) (which is 0.71, see R code and results below). According to our decision rules, if this probability exceeds 0.95, we would consider stopping the trial early for efficacy. Conversely, if the probability that the treatment is worse exceeds 0.95, we would stop for futility. Since neither condition was met (i.e., this probability is 0.71), we proceeded to the next interim analysis.\nDuring the second interim analysis, conducted after 100 participants had been followed (50 per group), our cumulative data showed that 36 out of 50 participants in the treatment group and 29 out of 50 in the control group had no adverse events. We updated the posterior distributions accordingly: \\(\\text{Beta}(38, 16)\\) for the treatment group and \\(\\text{Beta}(31, 23)\\) for the control group. Once again, we computed the posterior probability of treatment superiority (which is as 0.93 in our case, see R code and results below). If this probability had exceeded our predefined threshold (greater than 0.95), we would have stopped the trial early for success. Since the threshold was not reached (i.e., this probability is 0.93), we decided to continue to the final planned interim.\nThe third interim analysis took place after 150 participants had been enrolled and followed (75 per group). At this point, 57 participants in the treatment arm and 45 in the control arm experienced no adverse events. These data produced updated posterior distributions: \\(\\text{Beta}(59, 20)\\) for the treatment and \\(\\text{Beta}(47, 32)\\) for the control. We calculated the final posterior probability that the treatment is safer than the control (which is 0.98, see R code and results below). If the final probability exceeded 0.95, we planned to declare the trial a success and conclude that the new antibiotic is statistically superior in terms of safety. Otherwise, we would interpret the result as either inconclusive or indicative of no added benefit.\nWe write R code for this analysis as follows that also provides us the power and decision for each interim steps.\n\n\nCode\nlibrary(ggplot2)\nposterior_prob_superiority &lt;- function(success_treat, total_treat, success_control, total_control, alpha_prior = 2, beta_prior = 2, n_sim = 5000) {\n  alpha_treat &lt;- alpha_prior + success_treat\n  beta_treat  &lt;- beta_prior + (total_treat - success_treat)\n  alpha_control &lt;- alpha_prior + success_control\n  beta_control  &lt;- beta_prior + (total_control - success_control)\n  p_treat_samples &lt;- rbeta(n_sim, alpha_treat, beta_treat)\n  p_control_samples &lt;- rbeta(n_sim, alpha_control, beta_control)\n  prob_superior &lt;- mean(p_treat_samples &gt; p_control_samples)\n  list(\n    prob_superior = prob_superior,\n    treat_posterior = c(alpha = alpha_treat, beta = beta_treat),\n    control_posterior = c(alpha = alpha_control, beta = beta_control),\n    samples = data.frame(p_treat = p_treat_samples, p_control = p_control_samples)\n  )\n}\ninterims &lt;- list(\n  interim1 = list(treat = c(success = 17, total = 25), control = c(success = 15, total = 25)),\n  interim2 = list(treat = c(success = 36, total = 50), control = c(success = 29, total = 50)),\n  interim3 = list(treat = c(success = 57, total = 75), control = c(success = 45, total = 75))\n)\nefficacy_thresh &lt;- 0.95\nfutility_thresh &lt;- 0.05\nfinal_success_thresh &lt;- 0.95\nfor (i in seq_along(interims)) {\n  \n  data &lt;- interims[[i]]\n  cat(paste(\"Interim: \", i,\" -- Sample Size: \", data$treat[2],\"\\n\"))\n\n  result &lt;- posterior_prob_superiority(\n    success_treat = data$treat[\"success\"],\n    total_treat = data$treat[\"total\"],\n    success_control = data$control[\"success\"],\n    total_control = data$control[\"total\"]\n  )\n  \n  cat(\"Posterior Pr(treat &gt; control):\", round(result$prob_superior, 4), \"\\n\")\n  \n  if (result$prob_superior &gt; efficacy_thresh) {\n    cat(\"Stop: Evidence for treatment superiority\\n\")\n    break\n  } else if (result$prob_superior &lt; (1 - efficacy_thresh)) {\n    cat(\"Stop: Futility (treatment worse)\\n\")\n    break\n  } else {\n    cat(\"Continue to next interim\\n\")\n  }\n}\n\n\nInterim:  1  -- Sample Size:  25 \nPosterior Pr(treat &gt; control): 0.7196 \nContinue to next interim\nInterim:  2  -- Sample Size:  50 \nPosterior Pr(treat &gt; control): 0.9246 \nContinue to next interim\nInterim:  3  -- Sample Size:  75 \nPosterior Pr(treat &gt; control): 0.985 \nStop: Evidence for treatment superiority\n\n\nCode\nfinal_samples &lt;- result$samples\nggplot(final_samples, aes(x = p_treat - p_control)) +\n  geom_density(fill = \"steelblue\", alpha = 0.5) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(\n    title = \"Posterior Distribution of Difference: Pr(Treatment) - Pr(Control)\",\n    x = \"Difference in Safety Probabilities\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\nIt is also possible to do Bayesian power analyses for each intrem steps at the desing stage to identify the sample size required based on the decision rule.",
    "crumbs": [
      "Week 12: **Discover the Perfect Spell!**"
    ]
  },
  {
    "objectID": "M06_1.html#paediatric-antibiotic-trial",
    "href": "M06_1.html#paediatric-antibiotic-trial",
    "title": "Week 11: Size Matters!",
    "section": "Paediatric Antibiotic Trial",
    "text": "Paediatric Antibiotic Trial\n\n\nSuppose, we aim to evaluate whether a new antibiotic (treatment group) is safer than the current standard of care (control group) for treating bacterial respiratory infections in children under 5 years of age.\nOur target population includes children aged 6 months to 5 years who are diagnosed with bacterial pneumonia.\nThe primary endpoint (or the outcome variable) is binary: defined as the absence of drug-related adverse events (e.g., rash, diarrhea, allergic reaction) within 10 days of starting treatment.\nWe are interested to conduct 1:1 allocations for the treatment and control groups.\nSuppose, the standard of care has an estimated safety rate of approximately 60%. As the safety data for the new antibiotic in this age group is still limited, we assume equal prior beliefs for both groups. Hence, we can model the safety probabilities using Beta prior distributions as: \\(\\text{Beta}(1, 1)\\).\n\nDecision Rule - Antibiotic Trial\nAs indicated earlier, we use the Bayesian posterior probability decision rule to test whether the treatment is superior in safety compared to the control. Let us denote \\(\\theta\\) be the safety or success rate and we declare the new antibiotic superior if:\n\\[\nPr(\\theta_{\\text{trt}} - \\theta_{\\text{crt}} &gt; 0 \\mid \\text{data}) \\geq 0.95\n\\]\nThat is, we require at least 95% posterior probability that the new antibiotic has a higher safety rate than the control group (even if the improvement is small). This approach points out any level of safety improvement, provided we are confident in the result.\n\n\nSample Size - Antibiotic Trial\nLet’s say we want to plan/design the sample size such that, assuming the expected data for the safety (or success) rates are 75% for treatment (i.e., \\(\\theta_{\\text{trt}}=0.75\\)) and 60% for control (i.e., \\(\\theta_{\\text{crt}}=0.6\\)), and the posterior probability that the treatment is better than control (superiority trial), in at least 95% of simulated trials. Changes between treatment and control success rates can be also indicated as 15% point differece, i.e., from 60% to 75%.\nNote that in this example we are assuming equal prior beliefs for both treatment and control groups, which follows a \\(\\text{Beta}(1,1)\\) distribution. This prior reflects a non-informative situation, where the safety rates can take any values from 0 to 100%.\nBased on the information, we run simulations to generate data. Then, using simulated data from, say, 50 to 150 samples per group, we calculate the Bayesian power. Next, we identify the minimum sample size per group that achieves 80% power, i.e., 80% of the simulated trials lead us to declare \\(\\theta_{\\text{trt}} - \\theta_{\\text{crt}} &gt; 0\\) using our decision rule considered earlier.\nNow, from the simulation we can see the minimum sample size per group for 80% power as:\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nsimulate_safety_trial &lt;- function(n_per_group, \n                                  true_control = 0.60, \n                                  true_treatment = 0.75,\n                                  prior_control = c(1, 1), \n                                  prior_treatment = c(1, 1),\n                                  prob_cutoff = 0.95,\n                                  threshold = 0) {\n  \n  success_c &lt;- rbinom(1, n_per_group, true_control)\n  success_t &lt;- rbinom(1, n_per_group, true_treatment)\n  post_c &lt;- prior_control + c(success_c, n_per_group - success_c)\n  post_t &lt;- prior_treatment + c(success_t, n_per_group - success_t)\n  n_samples &lt;- 10\n  p_c_post &lt;- rbeta(n_samples, post_c[1], post_c[2])\n  p_t_post &lt;- rbeta(n_samples, post_t[1], post_t[2])\n  prob_superior &lt;- mean((p_t_post - p_c_post) &gt; threshold)\n  return(prob_superior &gt;= prob_cutoff)\n}\nestimate_safety_power &lt;- function(n_per_group, n_sim = 20000) {\n  mean(replicate(n_sim, simulate_safety_trial(n_per_group)))\n}\nsample_sizes &lt;- seq(50, 150, by = 5)\nset.seed(0001)\npower_results &lt;- tibble(\n  SampleSize = sample_sizes,\n  Power = map_dbl(sample_sizes, estimate_safety_power)\n)\nmin_n &lt;- round(power_results,2) %&gt;% filter(Power &gt;= 0.80) %&gt;% slice(1)\nprint(paste(\"Bayesian: Sample size/group for 80% power: \", min_n$SampleSize))\n\n\n[1] \"Bayesian: Sample size/group for 80% power:  120\"\n\n\nNow, let us compare this with frequentist sample size calculation.\n\n\nFrequentist Comparison\nHere, we want to determine the minimum sample size per group required to detect a treatment effect (from 60% to 75%) with 80% power, using a frequentist approach. Thus, we consider null hypothesis as: no improvement or the treatment is not better than the control, i.e., \\(H_0: \\theta_\\text{trt}\\leq \\theta_\\text{crt}\\). The alternative hypothesis we consider as: the treatment is better than the control (i.e., higher success rate), i.e., \\(H_1: \\theta_\\text{trt}&gt; \\theta_\\text{crt}\\).\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\np1 &lt;- ggplot(power_results, aes(x = SampleSize, y = Power)) +\n  geom_line(color = \"#0072B2\", size = 1.2) +\n  geom_point(color = \"#D55E00\", size = 2) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = min_n$SampleSize, linetype = \"dashed\", color = \"darkgreen\") +\n  labs(\n    title = \"Bayesian\",\n    x = \"Sample Size per Group\",\n    y = \"Power\"\n  ) +\n  ylim(0,1) +\n  theme_minimal(base_size = 14)\n\n# frequentist way\n\ntrue_control &lt;- 0.60  \ntrue_treatment &lt;- 0.75  \nalpha &lt;- 0.05  \nn_range &lt;- seq(50, 150, by = 5)  \npower_values &lt;- numeric(length(n_range))\nfor (i in seq_along(n_range)) {\n  result &lt;- power.prop.test(n = n_range[i],\n                            p1 = true_control,\n                            p2 = true_treatment,\n                            sig.level = alpha,\n                            alternative = \"one.sided\")\n  power_values[i] &lt;- result$power\n}\ndf &lt;- data.frame(SampleSize = n_range, Power = power_values)\nmin_n &lt;- df %&gt;% filter(Power &gt;= 0.80) %&gt;% slice(1)\nprint(paste(\"Frequentist: Sample size/group for 80% power: \", min_n$SampleSize))\n\n\n[1] \"Frequentist: Sample size/group for 80% power:  120\"\n\n\nCode\np2 &lt;- ggplot(df, aes(x = SampleSize, y = Power)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = min_n$SampleSize, linetype = \"dashed\", color = \"darkgreen\") +  \n  labs(\n    title = \"Frequentist\",\n    x = \"Sample Size per Group\",\n    y = \"Power\"\n  ) +\n  ylim(0,1) +  \n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12)\n  )\n\ngridExtra::grid.arrange(p1,p2,ncol=2)\n\n\n\n\n\n\n\n\n\nThe above plots provide power curves for both Bayesian and frequentist methods for sample sizes 50 to 150. We can see both plots show the power increases with the sample size and we can achieve 80% power at sample size 120 for both methods. This reflects under \\(\\text{Beta}(1,1)\\) prior we get same results for both Bayesian and frequentist methods based on the Bayesian decision rule we considered.",
    "crumbs": [
      "Week 11: **Size Matters!**"
    ]
  }
]